## Diffusion Models 扩散模型
核心思想是逐步的数据生成过程。
前向过程（Forward Process / Diffusion Process）： 逐步向数据中添加噪声，直到数据完全变成纯高斯噪声。
反向过程（Reverse Process）： 学习如何从纯噪声中一步步地去除噪声，最终恢复出原始数据。
**1. 前向过程（加噪）**
这是一个固定的（非学习的）、线性的过程。它被定义为一个马尔可夫链（Markov Chain），每一步都向数据 $\mathbf{x}_t$ 中添加一小步高斯噪声。

公式： $\mathbf{x}t = \sqrt{\alpha_t} \mathbf{x}{t-1} + \sqrt{1 - \alpha_t} \epsilon_t$， 其中 $\epsilon_t \sim \mathcal{N}(0, \mathbf{I})$
- $\mathbf{x}_0$ 是原始图像。
- $t$ 从 1 到 T（总步数，通常是1000步）。
- $\alpha_t$ 是预先定义好的噪声调度（Noise Schedule），控制每一步添加的噪声量。它随着 $t$ 增大而减小，意味着越往后加的噪声相对越多。

目的： 经过足够多的步数 $T$ 后，$\mathbf{x}_T$ 就完全变成了一个各向同性的高斯噪声（Isotropic Gaussian Noise），不再包含任何原始数据的信息。

**2. 反向过程（去噪）**
这是一个学习的过程，是模型的核心。我们需要训练一个神经网络来学习如何逆转前向过程。

目标： 给定第 $t$ 步的带噪图像 $\mathbf{x}t$ 和时间步 $t$，神经网络需要预测出添加到 $\mathbf{x}{t-1}$ 上的噪声 $\epsilon$，或者直接预测出去噪后的图像 $\mathbf{x}_0$。目前主流是预测噪声。
**神经网络（U-Net）**： 通常使用 U-Net 架构，并加入注意力机制（Attention） 和时间步嵌入（Timestep Embedding）。

- U-Net： 具有编码器-解码器结构，带有跳跃连接，非常适合捕捉图像的多尺度特征并进行像素级的预测。
- 时间步嵌入： 将当前的时间步 $t$ 编码成一个向量，并注入到U-Net的每一层中，让网络知道当前处于去噪的哪个阶段（是早期粗粒度去噪还是后期细粒度修复）。
- 注意力机制： 帮助模型处理图像不同部分之间的长程依赖关系，对于生成全局一致的图像至关重要。
-
**训练过程：**
1. 从训练集中随机取一张图片 $\mathbf{x}_0$。
2. 随机采样一个时间步 $t$ （从 1 到 T）。
3. 采样一个随机噪声 $\epsilon \sim \mathcal{N}(0, \mathbf{I})$。
4. 将噪声按前向过程公式加到图片上，得到 $\mathbf{x}_t$。
5. 将 $\mathbf{x}t$ 和 $t$ 输入神经网络，让网络预测添加的噪声 $\epsilon\theta(\mathbf{x}_t, t)$。
6. 计算预测噪声和真实噪声之间的**均方误差（MSE Loss）**： $L = ||\epsilon - \epsilon_\theta(\mathbf{x}_t, t)||^2$。

**采样/推理过程（生成新图像）：**

1. 从纯高斯噪声 $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$ 开始。
2. 从 $t = T$ 一步步循环到 $t = 1$：
    - 将当前的 $\mathbf{x}t$ 和 $t$ 输入训练好的网络，得到预测的噪声 $\epsilon\theta(\mathbf{x}_t, t)$。
    - 使用公式计算出 $\mathbf{x}_{t-1}$（这个过程会额外加入一点随机噪声，除非是最后一步）。
3. 最终得到生成的高清图像 $\mathbf{x}_0$。

### 与vae/GAN区别
Diffusion Model 相对于 VAE 的最大优势在于生成质量。它牺牲了生成速度。

## DDPM 与 DDIM 的区别
DDPM (Denoising Diffusion Probabilistic Models)：
采样随机而且慢。反向过程的每一步都遵循马尔可夫性（当前状态只依赖于前一步），并且会加入随机噪声。
DDIM (Denoising Diffusion Implicit Models)：
采样确定而且快，支持跳步采样。揭示了扩散模型的反向过程其实是一个_微分方程求解器_。它建立了扩散模型与神经ODE（Neural ODE） 的联系

## Classifier Guidance 与 Classifier-Free Guidance
### Classifier Guidance（分类器引导）
**原理**：
1. 需要训练一个额外的分类器。这个分类器接收带噪图像 x_t 和时间步 t，预测其条件 y（如类别标签）的概率。
2. 在反向去噪过程中，不仅使用扩散模型预测的噪声 ε_θ，还计算这个分类器关于输入 x_t 的梯度 ∇_{x_t} log p(y | x_t)。
3. 用这个梯度来“引导”下一步的生成方向，使其朝着最大化分类器置信度（即更符合条件 y）的方向移动。

**公式（简化）**： ε_guided = ε_θ(x_t, t) - s * σ_t * ∇_{x_t} log p(y | x_t)
s 是引导尺度（guidance scale），控制引导的强度。s 越大，生成结果与条件 y 的对齐越好，但可能会降低多样性。
### Classifier-Free Guidance（无分类器引
**原理：**
1. 彻底省去了分类器。它通过在训练时随机地丢弃条件（例如，有 10% 的概率将文本条件置空），同时训练一个条件扩散模型 ε_θ(x_t, t, y) 和一个无条件扩散模型 ε_θ(x_t, t, ∅)。
2. 在推理采样时，将条件预测和无条件预测进行向量组合，指向更符合条件的方向。

- 公式： ε_guided = ε_θ(x_t, t, ∅) + s * [ε_θ(x_t, t, y) - ε_θ(x_t, t, ∅)]
  -   ε_θ(x_t, t, y)：有条件噪声预测（我们想要的）。
  -   ε_θ(x_t, t, ∅)：无条件噪声预测（随意的，不关心条件的）。
  -   [ε_θ(x_t, t, y) - ε_θ(x_t, t, ∅)]：这个向量指向了“使得生成图像更符合条件 y”的方向。
  -   s 同样是引导尺度，控制方向上的步长。
 总结：Classifier-Free Guidance 通过巧妙的训练方式和采样时的向量运算，实现了更强大、更简单的控制引导，是目前文生图等技术的基石。

## Stable Diffusion
原理
LDM 的架构包含三个核心组件：

1. 自编码器 (VAE)：
- 注意：这个VAE是预先训练好的，在扩散训练和推理过程中是冻结（freeze）的。

2. 扩散模型 (U-Net)：
- 核心创新：扩散过程（加噪/去噪）不是在原始图像 x 上进行，而是在VAE编码后的潜在空间 z 上进行的。
- U-Net 的结构也进行了调整，以适应在潜在空间 z 上操作。

3.条件机制 (Conditioning Mechanism)：
  - 通常使用 CLIP 文本编码器，将输入文本提示词（Prompt）编码成向量表示。
  - 通过 Cross-Attention（交叉注意力） 机制注入到 U-Net 的中间层，实现精准的文本控制生成。

**工作流程：**
总结流程：
**随机噪声（潜空间） → U-Net多步去噪（受文本引导） → 干净的潜表示 → VAE解码器 → 最终高清图片**

_训练：_
1. 图像 x 通过编码器 E 得到潜在表示 z = E(x)。
2. 对 z 进行常规的DDPM扩散过程（加噪）。
3. U-Net 学习在潜在空间中去噪，并且通过交叉注意力接受文本条件。

_推理（生成）_：
1. 在潜在空间中随机采样一个噪声 z_T。
2. 用U-Net和DDIM等采样器，在文本条件的引导下，逐步去噪得到 z_0。
3. 将去噪后的潜在表示 z_0 送入VAE解码器 D，得到高清图像 x = D(z_0)。

为什么更高效？
1. 计算复杂度大幅降低：
  - 计算复杂度与数据维度呈指数关系。在 64x64x4 的潜在空间中进行扩散，相比在 512x512x3 的像素空间中进行，计算量和内存占用减少了数十倍。
  - 这是性能提升的最主要原因。
2. 语义集中：
  - 潜在空间 z 是原始图像经过VAE编码压缩后的“精华”，它过滤掉了图像中的高频细节（纹理、噪声）等难以学习又耗费算力的信息，保留了高级的语义和概念信息。
  - 在这个更抽象、更紧凑的空间里进行扩散，模型可以更专注于语义内容的学习和生成，效率更高，效果也更好。
3. 更适合与其它模态对齐：
文本、深度图等控制条件本身也是抽象表示。在潜在空间中进行融合（通过Cross-Attention）比在像素空间更自然、更高效。

结论：Stable Diffusion (LDM) 通过将扩散过程从像素空间迁移到计算成本低得多的潜在空间，在不显著牺牲质量的前提下，极大地降低了计算需求。
SD与VAE区别：
一句话：SD 的 VAE 只是“压缩-解压”工具，图像生成质量与多样性由潜空间扩散模型负责，而非传统 VAE 的采样随机性 