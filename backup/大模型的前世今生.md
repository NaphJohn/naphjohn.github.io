# Encoder-only模型

bert模型：
BERT（Bidirectional Encoder Representations from Transformers）= “只有 Transformer 编码器” 的预训练语言模型。
一句话：把 12/24 层 Transformer Encoder 叠起来，先在大规模无标签文本上做 Masked LM + Next Sentence Prediction 预训练，再微调就能横扫 11 项 NLP 任务。

# Decoder-Only模型

GPT模型

# Encoder-Decoder模型

T5模型