# LLM的推理/transformer架构/vllm

## LLM 推理的推理过程
目前主流LLM都是基于transformer的Decoder-only架构，因为 Decoder-only 架构**在“大规模无监督预训练”这个特定范式下，展现出了更好的可扩展性（Scalability）和任务通用性。**

总体过程如下：
输入Token -> 嵌入 -> 加上位置编码 -> 通过N个Transformer层 -> 最终层归一化 -> 语言模型头 -> 概率分布 -> 输出Token

> [!TIP]
>  
让我们以 Llama2-7B（4096 序列长度，float16精度）为例，计算一下 batch_size = 1的理想推理速度。

1. **prefill（预填充，并行处理输入的 tokens）**：假设 prompt 的长度是 350 token，那么预填充所需要的时间 = number of tokens * ( number of parameters / accelerator compute bandwidth) = 350 * (2 * 7B) FLOP / 125 TFLOP/s = 39 ms（A10)。这个阶段主要是**计算瓶颈。**
2. **decoding（解码，逐个生成下一个 token。）**：time/token = total number of bytes moved (the model weights) / accelerator memory bandwidth = (2 * 7B) bytes / (600 GB/s) = 23 ms/token（A10)。这个阶段的瓶颈是**带宽。**

**kvcache**
原理：自回归生成时，把之前所有 token 已经算过的 K/V 矩阵缓存到 HBM，下次只算新 token 的 Q/K/V，然后拼接历史 K/V 做注意力，避免 O(n²) 重复计算。
数学公式
KVCache(字节) = B × L × n_kv_heads × head_dim × 2 × sizeof(dtype) × 层数
B：batch 内同时服务的序列条数
L：当前已生成 token 数（=seq_len）
n_kv_heads：GQA 压缩后的 KV 头数（若无压缩 = 原头数） n_kv_heads = n_h // G
head_dim：每个头的维度
2：K 与 V 各一份
sizeof(dtype)：FP16/BF16→2字节；INT8/FP8→1字节；FP32→4字节


相关问题： kvcache节省了那部分计算？以及transformer中每一层信息
<details><summary>Details</summary>
<p>
1. FFN阶段本质上是两个矩阵乘计算，token之前互不影响，因此使用kv cache可以将计算量节省为原来的1/S
2. RMSNorm本质是对每一个token的隐状态做归一化，token之间是互相独立的，因此计算量为原来的1/S
3. Attention阶段由于Q和K做gemm，会得到一个shape为[S，S]的向量，在seqlen比较大的情况下，计算复杂度为O(S^2)，使用kv cache之后，Attention计算复杂度变为O(S)，计算Q、K、V的GEMM运算降为GEMV运算，整体计算量为原来的1/S;
4. 其他的Embedding等计算量比较小，大约也为原来计算量的1/S；

###  Transformer 中每一层对应细节
**Transformer block**（一个transformer块）
一个 Transformer Block = Attention（全局）+ MLP（局部）+ LN/残差，交替堆叠 L 次，就是现在大模型（GPT、ViT、Qwen 等）的核心骨架。

- Attention 负责全局找关系（谁重要）
- MLP 负责局部深加工（把重要信息揉进非线性空间）


Transformer 中典型的 Token-wise （“对每个 token 单独做同样的计算，token 之间不交换信息”）操作：
对于 Token-wise 操作（如FFN、Norm、新token的QKV投影），它们天然地只需要处理新 token 即可。计算量是固定的，与生成长度 S 无关


| 操作名称 | 功能 | 为什么是 Token-wise？ |
| :--- | :--- | :--- |
| **词嵌入 (Embedding)** | 将离散的 token ID 转换为高维向量。 | 查表操作，每个 token ID 独立地映射为一个向量。 |
| **线性投影/层 (Linear Layer)** | 如计算 Q, K, V 的投影层或 FFN 中的两个大矩阵乘。 | 矩阵乘法 `X * W` 可以看作是对 `X` 的每一行（即每个 token 的向量）进行独立的线性变换。 |
| **前馈网络 (FFN)** | 对每个 token 的特征进行非线性变换和增强。 | 本质上是两个连续的线性投影层加上激活函数，所以也是 token-wise 的。 |
| **层归一化 (LayerNorm)** | 对每个 token 的特征向量进行标准化。 | 计算一个 token 向量的均值和方差，然后用其来标准化这个**同一token**的向量。 |
| **残差连接 (Add)** | 将一层的输入和输出相加。 | 直接将对应位置的 token 向量相加。 |
| **激活函数 (e.g., ReLU, GELU, SiLU)** | 引入非线性。 | 对向量中的每个元素独立进行操作。 |

</p>
</details> 


QKV计算：
Score = Q * K^T
缩放：为了防止点积结果过大导致 Softmax 梯度消失，将得分除以 Key 向量维度的平方根。Scaled_Score = Score / √d_k
应用 Softmax：Attention_Weights = Softmax(Scaled_Score)
加权求和：用注意力权重对 Value 向量进行加权求和，得到最终的输出。Output = Attention_Weights * V

前馈神经网络 (Feed-Forward Network, FFN)

- 标准的Transformer FFN层由两个线性变换和一个ReLU激活函数组成： FFN_ReLU(x) = ReLU(xW1 + b1) W2 + b2 其中： 1. x 是输入，维度为 [batch_size, seq_len, d_model] 。
- 对自注意力层的输出进行非线性变换和空间映射，增强模型的表达能力。

残差连接 (Add) 和层归一化 (Norm)
**残差连接 (Residual Connection)**：将子层的输入直接加到其输出上（Output = Sublayer(input) + input）。这有效地缓解了深层网络中的梯度消失问题，使得模型可以堆叠得很深。
**层归一化 (Layer Normalization)**：对残差相加后的结果进行归一化，稳定训练过程，加快收敛
公式表示一个 Encoder Layer：
EncoderOutput = LayerNorm( SelfAttention(Input) + Input )
FinalOutput = LayerNorm( FFN(EncoderOutput) + EncoderOutput )

**位置编码positional encoding**
提供单词在序列中的绝对或相对位置信息，增加语言的有序性，使 Transformer 能够理解顺序。

**ROPE公式计算**
<details><summary>Details</summary>
<p>

| 收益        | 一句话解释                                                    |
| --------- | -------------------------------------------------------- |
| **外推性强**  | 训练 4k，推理 128k 不微调也能用，LLaMA-2 32k、ChatGLM-6B 32k/64k 全靠它。 |
| **无额外参数** | 旋转矩阵是固定的，0 可学习权重，过拟合风险低。                                 |
| **计算高效**  | 仅需对 Q/K 做一次旋转，矩阵乘法可融合进注意力，GPU 友好。                        |
| **长程衰减**  | 角度随距离增大而“转得远”，远距离点积自然变小，抑制噪声。                            |
| **缓存友好**  | KV-cache 里每个位置只存一次旋转后的向量，增量推理省内存。                        |


ROPE公式计算
对位置 m 的向量 x，把它每两个分量当成一个复数，乘以旋转因子
  f(x,m)=R_Θ^m·x，其中
  Θ_i=10000^(−2i/d)，i=0,1,…,d/2−1
结果：q_m、k_n 分别带上“相对距离”m−n 的旋转角，Attention 内积自然只与相对位置有关，而不再依赖绝对位置

</p>
</details> 
<details><summary>Details</summary>
<p>
为什么要位置编码？
Self-Attention 机制本身是置换不变 (Permutation Invariant) 的。即打乱输入序列的顺序，输出的结果（不考虑位置编码）是一样的，因为它计算的是两两向量之间的相似度。这显然不符合语言的有序性。

常用方式 ：

1. 正弦余弦编码 (Sinusoidal Positional Encoding) - 原版 Transformer
使用不同频率的正弦和余弦函数来生成每个位置独一无二的编码向量。
2. 学习式位置编码 (Learned Positional Embedding) - 如 BERT
将位置也视为一个可学习的参数，随机初始化一个位置嵌入矩阵（例如，最大长度 512 x 模型维度 768），随模型一起训练。
3. 相对位置编码 (Relative Positional Encoding) - 如 T5, DeBERTa
</p>
</details>

**MHA/GQA)原理**
Multi-Head Attention多头注意力原理：
- 每个注意力头拥有独立的 Q/K/V 投影矩阵，并行计算后再拼接输出，可捕捉不同子空间信息。
- 优点：模型容量最大，效果通常最好；缺点：KV 缓存随头数线性增长，长序列推理时显存与访存压力最大。
- 因此 MHA 常被当作“精度基准”，但在大模型长上下文部署里最先遇到内存墙
GQA（Grouped-Query Attention）
本质：把“ Query 头”分组，让同组内的 Query 共享同一对 K/V，用“分组共享”在精度与内存之间取甜点。
展开：
- 若总头数为 A，分成 G 组，则只需 G 套 K/V 参数（G=1 时退化为 MQA，G=A 时就是 MHA）。
- KV 缓存量从 MHA 的 A·n·d 降到 G·n·d，推理速度明显加快，而实验表明模型质量损失可忽略 。
- Llama 2、GPT-4、Mistral 等主流大模型因此把 GQA 作为“新默认”，兼顾长序列与高吞吐需求  

| 机制      | 实现要点          | KV 缓存大小     | 质量-速度权衡                              |
| ------- | ------------- | ----------- | ------------------------------------ |
| **MHA** | 每头独享 K/V      | H·n·d       | 最高质量，最慢、最费显存                         |
| **MQA** | 全部头共享 1 组 K/V | 1·n·d       | 速度最快，显存最小，精度掉最多                      |
| **GQA** | 头分成 G 组，组内共享  | G·n·d       | 质量≈MHA，显存-速度居中；GPT-4、Llama2 采用       |
| **MLA** | 低秩投影后缓存潜向量    | R·n·d (R≪H) | 缓存比 MQA 还小，质量反超 MHA；DeepSeek-V2 已上线  |



分组查询注意力 (Grouped-Query Attention, GQA)/原理：
将 n_heads 个查询头 (Query Heads) 分成 g 个组。
<details><summary>Details</summary>
<p>
1. 分组：将 n_heads 个查询头 (Query Heads) 分成 g 个组。
2. 共享：每组内的所有查询头共享同一套 K 和 V。
3. 操作：
Q 的个数不变（仍是 n_heads）。
K 和 V 的个数减少到 g 个（n_heads >= g）。
如过 g = n_heads，GQA 退化为 MHA。
如果 g = 1，GQA 退化为 MQA（所有头共享一套 K, V），也就是多头查询注意力 (Multi-Query Attention, MQA)
</p>
</details> 


**MOE架构：**
共享专家，若干路由专家组成，由门控网络选择top-k个专家
<details><summary>Details</summary>
<p>
moe中一个重要的点是控制负载均衡损失 (Load Balancing Loss)：为了解决这个问题，会在损失函数中加入一个辅助损失项，鼓励所有专家都能被均衡地使用。

优势：
每次只计算少数专家，减少了内存，加快了计算

参考链接：
[[https://fancyerii.github.io/2019/03/09/transformer-illustrated/#残差连接](https://fancyerii.github.io/2019/03/09/transformer-illustrated/#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5)](https://fancyerii.github.io/2019/03/09/transformer-illustrated/#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5)
</p>
</details>

**Mask原理** 
mask 就是布尔矩阵，True 处 logits→−∞，softmax 后概率=0，实现 pad 屏蔽 + 自回归 + 任意可见性控制，形状随场景从 (B,L) 到 (L,L) 不等。
比如因果掩码：防止标签泄漏，应用在Decoder的自注意力层（在训练和推理中均使用）。
填充掩码 (Padding Mask)：忽略填充符号，应用在Encoder和Decoder的自注意力层（主要用于处理批量数据）。

<details><summary>Details</summary>
<p>
在编码器（Encoder）中：通常只需要 Padding Mask，因为Encoder是一次性看到整个输入序列的。

在解码器（Decoder）中：需要同时使用 Padding Mask 和 Causal Mask（两者相加），既不能看Pad token，也不能看未来的token。
</p>
</details>

## LLM 推理的核心指标
<details><summary>Details</summary>
<p>
- Time To First Token (TTFT): 首 Token 延迟，即从输入到输出第一个 token 的延迟。在在线的流式应用中，TTFT 是最重要的指标，因为它决定了用户体验。
- Time Per Output Token (TPOT)： 每个输出 token 的延迟（不含首个Token）。在离线的批处理应用中，TPOT 是最重要的指标，因为它决定了整个推理过程的时间。
- Latency：延迟，即从输入到输出最后一个 token 的延迟。 Latency = (TTFT) + (TPOT) * (the number of tokens to be generated). Latency 可以转换为 Tokens Per Second (TPS)：TPS = (the number of tokens to be generated) / Latency。
- Throughput：吞吐量，即每秒针对所有请求生成的 token 数。以上三个指标都针对单个请求，而吞吐量是针对所有并发请求的。

我们将 LLM 应用分为两种：
  - 在线流式应用：对 TTFT、TPOT、Latency 敏感，需要尽可能快的生成 token。
  - 离线批量应用：对 Throughput 敏感，需要在单位时间内尽可能多的生成 token。
而实际在某种应用（如在线流式应用），我们也应该在Latency 和 Throughput 之间进行权衡，提高 Throughtput 可以提高硬件承担的并发数，从而降低推理成本。
</p>
</details> 

## LLM基座的推理能力

核心评估维度：
逻辑推理  比如mmlu（涵盖57个主题的多项选择题）
数学推理  GSM8K, MATH, AQuA-RAT
常识推理 HellaSwag, PIQA, ARC
多步推理  BigBench-Hard, DROP
代码推理  HumanEval, MBPP，livecodebench，livecodebench2025

## LLM 推理的性能卡点
1. KV-Cache 大小导致并发能力受限
LLM推理的过程是一个自回归的过程，也就是说前i次的token会作为第i+1次的预测数据送入模型，拿到第i+1次的推理token。

对最大长度是 4096 的 LLaMa2-7B fp16 模型，服务端每创建 1 个并发，都需要大约 2GB 显存保存 kv_cache，即便是 A100 80G，能并发服务的用户也非常有限。

2. 带宽瓶颈导致 TPOT 受限
将 LLM 托管到现代 GPU 时，计算能力一般不是瓶颈，显存带宽才是瓶颈。一般的衡量指标是 MBU（模型带宽利用率，Model Bandwidth Utilization）。
<details><summary>Details</summary>
<p>
MBU 定义为（实际内存带宽）/（峰值内存带宽），而（实际内存带宽） = （总模型参数大小 + KV 缓存大小） / TPOT。举个例子，假如 7B fp16 模型的 TPOT 是 14ms，那么它就需要在 14ms 内把 14GB 参数从显存加载到计算单元，也就是 1TB/s 的带宽使用量。假设显卡的峰值带宽为 2TB/s，那么 MBU = 0.5，即显存带宽利用率是 50%。
</p>
</details> 


# 大模型架构Qwen、GPT系列、llama系列，Deepseek系列，Transformer
## Qwen
核心改进总结：RMSNorm + SwiGLU + RoPE 已成为新一代LLM（如LLaMA, Qwen等）的标准配置。
1. RMSNorm (Root Mean Square Normalization)： RMSNorm 把 减均值和学偏置两步都删了，只剩一个缩放向量 γ
2. SwiGLU激活函数： 取代了传统的ReLU或GELU激活函数。SwiGLU是GLU（Gated Linear Unit）的一种，引入了门控机制，被证明在语言模型中能带来更好的性能。
3. 旋转位置编码 (RoPE, Rotary Position Embedding)： 取代了绝对或相对位置编码。RoPE通过旋转矩阵的方式将位置信息编码到注意力计算中，能更好地处理长序列，并具有很好的外推性（ extrapolation）。
4. 分组查询注意力 (GQA, Grouped-Query Attention)： 在较大的模型（如Qwen2-7B/72B）中使用了GQA。它介于MHA（多头注意力）和MQA（多查询注意力）之间，在几乎不损失效果的情况下，极大地减少了推理时KV Cache的内存占用，提高了推理速度。
5. 更长的上下文长度： Qwen2支持128K token的上下文长度，这得益于RoPE和相关的工程优化。
6. Tokenizer改进： 使用了更高效的分词器，词汇表大小扩大到152K，压缩率更高（特别是对中文和代码），减少了序列长度，提升了处理效率。


### RMSNorm以及LayerNorm公式
<details><summary>Details</summary>
<p>
LayerNorm：
$\text{LayerNorm}(\mathbf{x}) = \frac{\mathbf{x} - \mu}{\sigma} \odot \mathbf{g} + \mathbf{b}$
其中 $\mu = \frac{1}{H}\sum_{i=1}^H x_i$, $\sigma = \sqrt{\frac{1}{H}\sum_{i=1}^H (x_i - \mu)^2 + \epsilon}$, $\mathbf{g}$ 和 $\mathbf{b}$ 是可学习的增益和偏置参数。

RMSNorm：
$\text{RMSNorm}(\mathbf{x}) = \frac{\mathbf{x}}{\text{RMS}(\mathbf{x})} \odot \mathbf{g}$
其中 $\text{RMS}(\mathbf{x}) = \sqrt{\frac{1}{H}\sum_{i=1}^H x_i^2 + \epsilon}$， 通常省略偏置 $\mathbf{b}$。

SwiGLU的公式是什么？
SwiGLU是Swish激活函数和GLU（Gated Linear Unit）结构的结合。

公式：
$\text{SwiGLU}(x, W, V, b, c) = \text{Swish}(xW + b) \odot (xV + c)$
其中：

- $x$ 是输入
- $W, V$ 是权重矩阵
- $b, c$ 是偏置（可选）
- $\odot$ 是逐元素乘法（Hadamard product）
- $\text{Swish}(x) = x \cdot \sigma(x)$， $\sigma$ 是sigmoid函数。

GLU：这种 (A) ⊙ (B) 的结构被称为“门控线性单元”（Gated Linear Unit）。Swish(xW1) 是“门”，它控制 (xV) 哪些部分可以通过。
在Transformer的FFN层中，通常用SwiGLU取代原来的ReLU激活
</p>
</details> 



Q，K，V计算的实际过程：
<details><summary>Details</summary>
<p>

- Query (Q - 查询)：可以理解为“当前正在关注的 token”发出的一个询问：“与我相关的信息是什么？”
- Key (K - 键)：可以理解为所有 token 提供的一个“标识”，用来匹配 Query。Query 会与所有 Key 进行相似度比较。
- Value (V - 值)：可以理解为每个 token 所包含的“实际信息”或“内容”。一旦通过 Q-K 匹配确定了哪些 token 重要，我们就对这些 token 的 Value 进行加权求和。

**实际过程：**
1. 线性投影：通过三个权重矩阵 W^Q, W^K, W^V，将输入 X 分别投影到 Q, K, V 空间。
2. Q = X * W^Q, K = X * W^K, V = X * W^V
3. 计算注意力得分：计算 Query 和所有 Key 的点积，得到相似度得分。Score = Q * K^T
4. 缩放：为了防止点积结果过大导致 Softmax 梯度消失，将得分除以 Key 向量维度的平方根。Scaled_Score = Score / √d_k
5. 应用 Softmax：对缩放后的得分应用 Softmax 函数，得到归一化的注意力权重（所有权重和为1）。Attention_Weights = Softmax(Scaled_Score)
6. 加权求和：用注意力权重对 Value 向量进行加权求和，得到最终的输出。Output = Attention_Weights * V

这个输出就包含了当前 token 从序列所有其他 token 那里聚合到的上下文信息。


1. 多头自注意力机制 (Multi-Head Self-Attention)
- “自” (Self) 的含义：输入序列自身内部元素之间进行注意力计算。例如，句子中的每个词都会与句子中的所有词（包括自己）进行关联，从而捕捉词与词之间的语义和语法关系。
- attention核心公式：Attention(Q, K, V) = softmax(QK^T / √d_k) V
- 让模型能够“同时看到”整个序列，并理解每个词在上下文中的真正含义。
1. 前馈神经网络 (Feed-Forward Network, FFN)
- 一个简单的全连接网络，通常包含一个隐藏层和激活函数（如 ReLU）。
- 对自注意力层的输出进行非线性变换和空间映射，增强模型的表达能力。
1. 残差连接 (Add) 和层归一化 (Norm)
- 残差连接 (Residual Connection)：将子层的输入直接加到其输出上（Output = Sublayer(input) + input）。这有效地缓解了深层网络中的梯度消失问题，使得模型可以堆叠得很深。
- 层归一化 (Layer Normalization)：对残差相加后的结果进行归一化，稳定训练过程，加快收敛
公式表示一个 Encoder Layer：
EncoderOutput = LayerNorm( SelfAttention(Input) + Input )
FinalOutput = LayerNorm( FFN(EncoderOutput) + EncoderOutput )
</p>
</details>

**Decoder解码器组成**
根据 Encoder 产生的中间表示，自回归地（一个一个地）生成输出序列（如翻译后的句子）。
<details><summary>Details</summary>
<p>

1. 掩码多头自注意力机制 (Masked Multi-Head Self-Attention)
- “掩码” (Masked) 的含义：为了防止在训练时“偷看”未来信息（即解码第 t 个词时只能看到 1 到 t-1 的词），通过一个掩码矩阵将当前位置之后的所有信息屏蔽掉（设置为负无穷，softmax 后变为 0）。
1. 多头交叉注意力机制 (Multi-Head Cross-Attention)
- “交叉” (Cross) 的含义：Decoder 的表示 与 Encoder 的最终输出 进行注意力计算。
- Query (Q) 来自 Masked Self-Attention 的输出。
- Key (K) 和 Value (V) 来自 Encoder 的最终输出。
- 这是 Encoder 和 Decoder 之间信息交互的桥梁，让 Decoder 在生成每一个词时都能有针对性地关注输入序列中最相关的部分。
1. 前馈神经网络 (Feed-Forward Network, FFN)
每个子层都伴随着残差连接和层归一化。
</p>
</details>


# 不同推理框架
目前推理框架包含vllm、SGlang、LMDeploy、TensorRT-LLM

四者对比：
图编译器（生成引擎） → TensorRT-LLM  ，优化是预编译+算子融合+FP8-TC，编译慢，但是执行快
推理引擎（动态调度） → vLLM，适合需要处理大量并发请求的在线服务
前缀缓存 → SGLang
服务框架（HTTP/gRPC+批处理）→ Triton，优化是动态批+并发执行+多模型编排
国产化/训练推理一体 → LMDeploy
具体区别如下：
<details><summary>Details</summary>
<p>
SGLang：追求开发效率，适合需要复杂提示词处理和快速迭代的研究场景
只追求延迟/吞吐 → TensorRT
快速上线+多硬件 → vLLM
要 REST/gRPC+监控+多模型编排 → Triton（后端可挂 TensorRT 或 vLLM）
</p>
</details> 

## vllm推理框架架构

vLLM 采用分层架构设计，主要包括控制面和执行面。

- 控制面负责请求调度、显存管理和资源分配，核心是 *Scheduler* 和 *BlockSpaceManager*。
- 执行面负责模型计算和推理任务执行，核心是 *Worker*（内含 ModelRunner 负责模型前向传播，CacheEngine 管理 KV Cache）

## 请求调度

vLLM 的调度器 (Scheduler) 是大脑，其核心策略是 Continuous Batching (连续批处理)

<details><summary>Details</summary>
<p>
- 基于 Token 调度：以 Token 为最小调度单位
- 调度过程：Scheduler 维护 waiting（新请求）、running（正在处理）、swapped（因资源不足被换出的请求）队列。其调度优先级通常是：swapped > waiting > running，优先处理已被换出的请求以避免资源浪费，并在资源足够时从 waiting 队列加入新请求。
- 工作流程：
1. 新请求进入 waiting 队列。
2. Scheduler 根据调度策略（如 FCFS）、当前 running 队列的负载、空闲物理块等因素，决定将哪些 waiting 或 swapped 队列中的请求加入 running 队列以进行下一批计算。
3. Worker 执行 running 队列中请求的推理计算。
4. 生成 Token 后，更新序列状态。若请求完成则释放资源，否则根据情况放回 running 或换出到 swapped 队列。
5. 循环上述过程。
</p>
</details> 

## vllm推理整体流程
vLLM 构建了一套面向分布式推理的执行架构。
- V0架构: - 使用BlockManager和Scheduler作为独立组件 - 调度器直接与BlockManager交互管理KV缓存 - 在LLMEngine中集成了大量功能逻辑；
- V1架构: - 引入了更模块化的设计，将功能分散到专门的组件中 - 使用KVCacheManager替代BlockManager，提供更抽象的接口 - 引入EngineCoreClient和Processor等组件实现更清晰的职责分离 - 采用更统一的调度方式，不再严格区分预填充和解码阶段


用户请求
   ↓
AsyncLLM / Client 进程（Process-0）
   ├─ tokenize + 参数校验
   └─ 通过 ZMQ 把「request_id + prompt_ids」发给 EngineCore

EngineCore 进程（Process-1）  ←─── 双进程异步，CPU-GPU 并行
   ├─ Scheduler 产出 {req_id: num_tokens} 调度表
   ├─ KVCacheManager 分配/复用 paged block
   ├─ prepare_input 只增量更新张量（persistent batch）
   ├─ model_forward → logits
   ├─ sampling → next_token
   ├─ 把新 token 写回 req 对象
   └─ ZMQ 返回给 Client 做 detokenize / 回包

关键加速点

- 双进程：Process-0 做前后处理，Process-1 只做调度和推理，CPU 流水线重叠。
- Persistent Batch：每步只改「新增 token」那一列，省掉 V0 的“全表重建”开销。
- Prefix Caching 默认开：0% 命中率时吞吐量下降 <1%，高命中率时数倍提升。
- Chunked Prefill：长 prompt 被切成多段，与 decode 混跑，decode 延迟更平滑。

### 推理任务的调度与抢占：

vllm采用了 FCFS（first-come-first-serve） 的策略；
vllm中块回收策略：All-or-Nothing

### 块恢复策略：Swapping 与 Recomputation

Swapping（交换）：将被抢占请求的 KV Cache 块从 GPU 移动到 CPU 内存。当有空闲 GPU 块时再从 CPU 恢复回来继续执行。为了控制资源使用，被交换（swapped）到 CPU 内存中的 KV Cache 块数量，永远不会超过 GPU 中物理块的总数。
Recomputation（重计算）：直接丢弃已生成的 KV Cache，待请求恢复后重新计算。
根据论文中的实验结果，Swapping 更适用于 block size 较大的场景，而 Recomputation 则在不同 block size 下的表现更为稳定。在中等 block size（16 到 64）范围内，两种方式的端到端性能基本相当。


# HCCL
华为对应集合通信库，支持allreduce、broadcast、allgather、reduceScatter、AlltoAll等通信原语
broadcast：1到多
gather：多到1
scatter：1个npu数据切分到其它npu
reduce：多个npu获取数据并做规约运算

allreduce：从多个npu获取数据到所有npu并做归约运算
reduce scatter：按维度执行规约并发送到对应npu
allgather：收集每个npu所有的数据到每个npu上
alltoallv：所有npu互相scatter和gather数据













































































































































































