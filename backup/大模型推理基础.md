# LLM的推理/transformer架构/vllm

## LLM 推理的推理过程
### QKV
**将QKV三条独立线拆开原因**,核心原因是 ‘参数专属 + 梯度解耦’,原因如下：
<details><summary>Details</summary>
<p>
把 Q、K、V 拆成 三条独立线性 而不是一个权重乘三次，核心原因是 ‘参数专属 + 梯度解耦’：
一份权重乘三遍 → Q/K/V 被迫共享同一参数空间，梯度累加在同一矩阵，导致：
更新方向被平均，query 要高频、key 要低秩、value 要高熵 的冲突需求互相拉扯；
秩被压缩，多头子空间重叠度↑，注意力退化趋同。
独立线性层 → 各学各的投影，梯度不打架，子空间正交性↑，模型容量直接×3，无需增加深度就能提升表达能力；
训练稳定、收敛更快，这是 Transformer 比单矩阵×3 更深却不难训的关键。
</p>
</details> 


目前主流LLM都是基于transformer的Decoder-only架构，因为 Decoder-only 架构在“大规模无监督预训练”这个特定范式下，展现出了更好的可扩展性（Scalability）和任务通用性。

输入token经过transformer总体过程如下：
输入Token -> 嵌入 -> 加上位置编码 -> 通过N个Transformer层 -> 最终层归一化 -> 语言模型头 -> 概率分布 -> 输出Token

> [!TIP]
>  
让我们以 Llama2-7B（4096 序列长度，float16精度）为例，计算一下 batch_size = 1的理想推理速度。

1. **prefill（预填充，并行处理输入的 tokens）**：假设 prompt 的长度是 350 token，那么预填充所需要的时间 = number of tokens * ( number of parameters / accelerator compute bandwidth) = 350 * (2 * 7B) FLOP / 125 TFLOP/s = 39 ms（A10)。这个阶段主要是**计算瓶颈。**
2. **decoding（解码，逐个生成下一个 token。）**：time/token = total number of bytes moved (the model weights) / accelerator memory bandwidth = (2 * 7B) bytes / (600 GB/s) = 23 ms/token（A10)。这个阶段的瓶颈是**带宽。**

**kvcache**
原理：自回归生成时，把之前所有 token 已经算过的 K/V 矩阵缓存到 HBM，下次只算新 token 的 Q/K/V，然后拼接历史 K/V 做注意力，避免 O(n²) 重复计算。
数学公式
KVCache(字节) = B × L × n_kv_heads × head_dim × 2 × sizeof(dtype) × 层数
B：batch 内同时服务的序列条数
L：当前已生成 token 数（=seq_len）
n_kv_heads：GQA 压缩后的 KV 头数（若无压缩 = 原头数） n_kv_heads = n_h // G
head_dim：每个头的维度
2：K 与 V 各一份
sizeof(dtype)：FP16/BF16→2字节；INT8/FP8→1字节；FP32→4字节


相关问题： kvcache节省了那部分计算？
<details><summary>Details</summary>
<p>
1. FFN阶段本质上是两个矩阵乘计算，token之前互不影响，因此使用kv cache可以将计算量节省为原来的1/S
2. RMSNorm本质是对每一个token的隐状态做归一化，token之间是互相独立的，因此计算量为原来的1/S
3. Attention阶段由于Q和K做gemm，会得到一个shape为[S，S]的向量，在seqlen比较大的情况下，计算复杂度为O(S^2)，使用kv cache之后，Attention计算复杂度变为O(S)，计算Q、K、V的GEMM运算降为GEMV运算，整体计算量为原来的1/S;
4. 其他的Embedding等计算量比较小，大约也为原来计算量的1/S；

### MFU

<img width="1860" height="81" alt="Image" src="https://github.com/user-attachments/assets/e021e126-6c30-41bb-838b-5dc74c45ed9e" />

<img width="1683" height="81" alt="Image" src="https://github.com/user-attachments/assets/e756f77e-25d2-47d8-be69-115026beb411" />

<img width="998" height="86" alt="Image" src="https://github.com/user-attachments/assets/b27a70f7-f7fe-41e8-a543-42e294496b95" />


###  Transformer 中每一层对应细节
**Transformer block**（一个transformer块）
一个 Transformer Block = Attention（全局）+ MLP（局部）+ LN/残差，交替堆叠 L 次，就是现在大模型（GPT、ViT、Qwen 等）的核心骨架。

- Attention 负责全局找关系（谁重要）
- MLP 负责局部深加工（把重要信息揉进非线性空间）


Transformer 中典型的 Token-wise （“对每个 token 单独做同样的计算，token 之间不交换信息”）操作：
对于 Token-wise 操作（如FFN、Norm、新token的QKV投影），它们天然地只需要处理新 token 即可。计算量是固定的，与生成长度 S 无关


| 操作名称 | 功能 | 为什么是 Token-wise？ |
| :--- | :--- | :--- |
| **词嵌入 (Embedding)** | 将离散的 token ID 转换为高维向量。 | 查表操作，每个 token ID 独立地映射为一个向量。 |
| **线性投影/层 (Linear Layer)** | 如计算 Q, K, V 的投影层或 FFN 中的两个大矩阵乘。 | 矩阵乘法 `X * W` 可以看作是对 `X` 的每一行（即每个 token 的向量）进行独立的线性变换。 |
| **前馈网络 (FFN)** | 对每个 token 的特征进行非线性变换和增强。 | 本质上是两个连续的线性投影层加上激活函数，所以也是 token-wise 的。 |
| **层归一化 (LayerNorm)** | 对每个 token 的特征向量进行标准化。 | 计算一个 token 向量的均值和方差，然后用其来标准化这个**同一token**的向量。 |
| **残差连接 (Add)** | 将一层的输入和输出相加。 | 直接将对应位置的 token 向量相加。 |
| **激活函数 (e.g., ReLU, GELU, SiLU)** | 引入非线性。 | 对向量中的每个元素独立进行操作。 |

</p>
</details> 


QKV计算：
Score = Q * K^T
缩放：为了防止点积结果过大导致 Softmax 梯度消失，将得分除以 Key 向量维度的平方根。Scaled_Score = Score / √d_k
应用 Softmax：Attention_Weights = Softmax(Scaled_Score)
加权求和：用注意力权重对 Value 向量进行加权求和，得到最终的输出。Output = Attention_Weights * V

前馈神经网络 (Feed-Forward Network, FFN)

- 标准的Transformer FFN层由两个线性变换和一个ReLU激活函数组成： FFN_ReLU(x) = ReLU(xW1 + b1) W2 + b2 其中： 1. x 是输入，维度为 [batch_size, seq_len, d_model] 。
- 对自注意力层的输出进行非线性变换和空间映射，增强模型的表达能力。

残差连接 (Add) 和层归一化 (Norm)
**残差连接 (Residual Connection)**：将子层的输入直接加到其输出上（Output = Sublayer(input) + input）。这有效地缓解了深层网络中的梯度消失问题，使得模型可以堆叠得很深。
**层归一化 (Layer Normalization)**：对残差相加后的结果进行归一化，稳定训练过程，加快收敛
公式表示一个 Encoder Layer：
EncoderOutput = LayerNorm( SelfAttention(Input) + Input )
FinalOutput = LayerNorm( FFN(EncoderOutput) + EncoderOutput )

**位置编码positional encoding**
提供单词在序列中的绝对或相对位置信息，增加语言的有序性，使 Transformer 能够理解顺序。

**ROPE位置编码**
“把 Q、K 向量拆成二维一对，在每对构成的复平面上按位置序号等比例旋转不同角度，再点积；旋转差构成相对距离，点积随距离衰减——零参数量、可外推、相对位置一次到位。”
零参数量，长度外推、远程衰减、并行友好

<details><summary>Details</summary>
<p>

现在常用的是rope，rope好处：
| 收益        | 一句话解释                                                    |
| --------- | -------------------------------------------------------- |
| **外推性强**  | 训练 4k，推理 128k 不微调也能用，LLaMA-2 32k、ChatGLM-6B 32k/64k 全靠它。 |
| **无额外参数** | 旋转矩阵是固定的，0 可学习权重，过拟合风险低。                                 |
| **计算高效**  | 仅需对 Q/K 做一次旋转，矩阵乘法可融合进注意力，GPU 友好。                        |
| **长程衰减**  | 角度随距离增大而“转得远”，远距离点积自然变小，抑制噪声。                            |
| **缓存友好**  | KV-cache 里每个位置只存一次旋转后的向量，增量推理省内存。                        |


ROPE公式计算
ROPE = 分块对角旋转矩阵 R(m,Θ) 乘以向量 x，这边向量 x 就是多头注意力里某一个头的单条 token 向量。整体保持相对位置信息且零参数。
其中
  Θ_i=10000^(−2i/d)，i=0,1,…,d/2−1
结果：q_m、k_n 分别带上“相对距离”m−n 的旋转角，Attention 内积自然只与相对位置有关，而不再依赖绝对位置

 旋转角度怎么来？
每维频率先定好（跟 Sinusoidal 相同）：
θᵢ = 10000^(−2i/d) , i = 0,1,…,d/2−1
位置 m 的旋转角 = m·θᵢ
→ 越靠后的 token 转得越多；同一维度 i 越远转得越快（像时钟秒针）。

它的完整公式是？
注意力内积的魔法
查询 q 在位置 m，键 k 在位置 n，分别做 RoPE 后点积：
RoPE(q,m) · RoPE(k,n)
⇒ 只与相对距离 (m−n) 有关，绝对位置被消掉，天然相对位置编码；

</p>
</details> 

**MHA/GQA)原理**
Multi-Head Attention多头注意力原理：
- 每个注意力头拥有独立的 Q/K/V 投影矩阵，并行计算后再拼接输出，可捕捉不同子空间信息。
- 优点：模型容量最大，效果通常最好；缺点：KV 缓存随头数线性增长，长序列推理时显存与访存压力最大。
- 因此 MHA 常被当作“精度基准”，但在大模型长上下文部署里最先遇到内存墙
GQA（Grouped-Query Attention）
本质：把“ Query 头”分组，让同组内的 Query 共享同一对 K/V，用“分组共享”在精度与内存之间取甜点。
展开：
- 若总头数为 A，分成 G 组，则只需 G 套 K/V 参数（G=1 时退化为 MQA，G=A 时就是 MHA）。
- KV 缓存量从 MHA 的 A·n·d 降到 G·n·d，推理速度明显加快，而实验表明模型质量损失可忽略 。
- Llama 2、GPT-4、Mistral 等主流大模型因此把 GQA 作为“新默认”，兼顾长序列与高吞吐需求  

| 机制      | 实现要点          | KV 缓存大小     | 质量-速度权衡                              |
| ------- | ------------- | ----------- | ------------------------------------ |
| **MHA** | 每头独享 K/V      | H·n·d       | 最高质量，最慢、最费显存                         |
| **MQA** | 全部头共享 1 组 K/V | 1·n·d       | 速度最快，显存最小，精度掉最多                      |
| **GQA** | 头分成 G 组，组内共享  | G·n·d       | 质量≈MHA，显存-速度居中；GPT-4、Llama2 采用       |
| **MLA** | 低秩投影后缓存潜向量    | R·n·d (R≪H) | 缓存比 MQA 还小，质量反超 MHA；DeepSeek-V2 已上线  |

在deepseek中，MLA 里，“MQA vs MHA”不再是两种静态结构，而是同一组低秩 latent 权重的两种“展开姿势”——
pre-fill 时拆成独立头（MHA mode），decode 时共享同一份 KV（MQA mode），靠 reshape 即时切换，无需额外参数，也不掉精度。

分组查询注意力 (Grouped-Query Attention, GQA)/原理：
将 n_heads 个查询头 (Query Heads) 分成 g 个组。
<details><summary>Details</summary>
<p>
1. 分组：将 n_heads 个查询头 (Query Heads) 分成 g 个组。
2. 共享：每组内的所有查询头共享同一套 K 和 V。
3. 操作：
Q 的个数不变（仍是 n_heads）。
K 和 V 的个数减少到 g 个（n_heads >= g）。
如过 g = n_heads，GQA 退化为 MHA。
如果 g = 1，GQA 退化为 MQA（所有头共享一套 K, V），也就是多头查询注意力 (Multi-Query Attention, MQA)
</p>
</details> 


**MOE原理**
一个共享专家，若干路由专家组成，由门控网络选择top-k个专家。对应结构原理，计算不爆炸原因，好处如下：
<details><summary>Details</summary>
<p>

核心概念与架构
一个标准的MoE层主要由两个部分组成：

1. 专家：
这些专家本身就是一个前馈神经网络，通常是结构相同但参数不共享的小型全连接层。
每个专家都倾向于在输入数据的某个特定子集或某种特定模式上“专业化”。
例如，一个专家可能擅长处理数学问题，另一个可能擅长处理文学问题。

2. 门控网络：
这是一个路由机制，它的职责是“审阅”输入数据。
根据输入，它会产生一个概率分布，决定将输入发送给哪些专家。
最终，只有Top-K（通常K=1或2）个得分最高的专家会被激活来处理这个输入。

工作流程：
输入 → 门控网络 → 选择Top-K专家 → 输入被发送给选中的专家 → 专家们分别处理输入 → 门控网络根据得分加权求和专家们的输出 → 得到最终输出。

</p>
</details>

**PageAttention原理：**
1. 分页切块
KV cache 不再整段连续，而是按固定块大小（如 1 MB ≈ 256 tokens）切成等长块，块号→物理块表。
2. 逻辑→物理映射
每个序列维护一张逻辑块号 → 物理块号的表，生成新 token 时只给最后一个逻辑块追加；前面块只读共享。
3. 引用计数 & 写时复制
物理块被多序列共享时计数 +1；只要有一个序列要继续写，就复制出新块，其余序列仍读旧块——零浪费、零等待。

**Mask原理** 

今天的大模型掩码全是下三角 Causal，可选 pad/文档屏蔽；形状统一写成 (B,1,L,L) Boolean，True 表示可见。

比如因果掩码：防止标签泄漏，应用在Decoder的自注意力层（在训练和推理中均使用）。
填充掩码 (Padding Mask)：忽略填充符号，应用在Encoder和Decoder的自注意力层（主要用于处理批量数据）。

qwen3的mask如下：

<details><summary>Details</summary>
<p>
qwen3
| 掩码类型          | 形状                           | 值                | 说明            |
| ------------- | ---------------------------- | ---------------- | ------------- |
| **默认 Causal** | `(B, 1, L, L)`               | 下三角 1，其余 0       | 单条生成时完全自动     |
| **序列级 mask**  | `(B, L)` → 扩为 `(B, 1, L, L)` | 有效 token=1，pad=0 | 批处理时屏蔽右侧 pad  |
| **完全自定义**     | `(B, 1, L, L)`               | 用户任意 Bool        | 支持人工控制可见范围    |

</p>
</details>

## LLM 推理的核心指标
<details><summary>Details</summary>
<p>
- Time To First Token (TTFT): 首 Token 延迟，即从输入到输出第一个 token 的延迟。在在线的流式应用中，TTFT 是最重要的指标，因为它决定了用户体验。
- Time Per Output Token (TPOT)： 每个输出 token 的延迟（不含首个Token）。在离线的批处理应用中，TPOT 是最重要的指标，因为它决定了整个推理过程的时间。
- Latency：延迟，即从输入到输出最后一个 token 的延迟。 Latency = (TTFT) + (TPOT) * (the number of tokens to be generated). Latency 可以转换为 Tokens Per Second (TPS)：TPS = (the number of tokens to be generated) / Latency。
- Throughput：吞吐量，即每秒针对所有请求生成的 token 数。以上三个指标都针对单个请求，而吞吐量是针对所有并发请求的。

我们将 LLM 应用分为两种：
  - 在线流式应用：对 TTFT、TPOT、Latency 敏感，需要尽可能快的生成 token。
  - 离线批量应用：对 Throughput 敏感，需要在单位时间内尽可能多的生成 token。
而实际在某种应用（如在线流式应用），我们也应该在Latency 和 Throughput 之间进行权衡，提高 Throughtput 可以提高硬件承担的并发数，从而降低推理成本。
</p>
</details> 

## LLM基座的推理能力

核心评估维度：
逻辑推理  比如mmlu（涵盖57个主题的多项选择题）
数学推理  GSM8K, MATH, AQuA-RAT
常识推理 HellaSwag, PIQA, ARC
多步推理  BigBench-Hard, DROP
代码推理  HumanEval, MBPP，livecodebench，livecodebench2025

## LLM 推理的性能卡点

**短输入长输出情况下如何提升decode性能，TPOT效果？**
在“短输入 + 长输出”场景里，Decode 阶段占 90 % 以上时间，TPOT（每输出 token 延迟）是主要矛盾。核心思路是 “让每步 Decode 算得少、访存量低、批得大”。具体可做 6 件事：
<details><summary>Details</summary>
<p>

1. PD 分离 + 高 Decode 专用卡
把 Prefill 与 Decode 部署到不同 GPU，Decode 卡只做自回归，消除 Prefill 抢占，TPOT 立刻平稳 。
2. 连续批处理调大“Decode 池”
在 vLLM / TensorRT-LLM 里用 MAX_UTILIZATION 策略，保持 32-64 个序列同时解码，单卡 TPOT 随 batch 增大而下降（内存带宽打满即可）。
3. KV-Cache 压缩 + 显存换带宽
- KV-int8/int4 量化：把 KV 头压到 4-8 bit，显存↓50 % → 同显存可放更大 batch → TPOT↓15-30 %（百度云千帆实测多轮对话降 30 % Token 重复计算 ）。
- GQA / MLA：把 KV 头数从 64→8 或 1，同样 batch 下带宽需求线性下降。

4. 投机解码（MTP / Medusa）
用 1 主 + 1-3 草稿模型并行跑，70 % 接受率下每步可吐 1.7 token；华为云 Ascend 实测 TPOT 从 100 ms→50 ms 。
5. 长输出分段流水 & 前缀缓存
把 8 k-32 k 生成分成多段，段间 KV 复用；Prefix Cache 命中时首段之后 TTFT≈0，等效 TPOT 再降 20-40 % 。
6. 低比特 GEMM + 算子融合
W8A8 或 W4A16 让 Decode 的 MatMul 计算量减半；vLLM 的 Marlin/cutlass INT8 kernel + 单步 fused-QKV 投影可把每步延迟再削 5-10 %。

落地顺序（性价比）
PD 分离 → 连续批处理调大 → KV-int8 → GQA/MLA → 投机解码 → 分段流水
按此阶梯，TPOT 通常可从 100 ms 级压到 30-50 ms，长输出越长收益越大。

</p>
</details> 

长输入 + 短输出 → TTFT（首 token 时间）≈ Prefill 耗时。
核心矛盾：O(n²) Attention 与 HBM 带宽。优化思路：“算得少、算得快、吃得下、吐得快”。
<details><summary>Details</summary>
<p>
1. chunked-prefill（必开）
把提示按 512-2048 token 切块，与 Decode 交替进 batch；HBM 读写从 O(n²) 变 O(k·n)，TTFT 线性化，vLLM 默认开启后 128 k 输入 TTFT ↓40-60 %。
2. 长序列专用 Attention 内核
FlashAttention-2 / Flash-Decoding：SRAM 分块 + 并行归约，A100 上 64 k seq 提速 2.2×
MLA / GQA：KV 头数 64→1-8，带宽需求同比例下降，TTFT 再减 20-30 %。
3. KV-cache 压缩 + 前缀缓存
KV-int8 / int4 量化：显存↓50 % → 同样卡可放更长序列或更大 batch，128 k 输入 OOM 率从 30 %→0。
Prefix Cache：命中后首段 KV 免算，重复系统提示场景 TTFT 直接 0 ms。
4. 4/8-bit 权重 GEMM
W4A16/W8A8 让线性层计算量减半；cutlass/Marlin INT8 kernel 在 Prefill 阶段带宽瓶颈下仍可再削 10-15 % 延迟。
5. 多卡并行策略

- TP+SP（Tensor + Sequence Parallel）：把 128 k 序列按 2k-4k token 切片到多卡，每卡 Attention 降到 (seq/TP)²，TTFT 随卡数线性下降（Megatron-LM 实测 8 卡↓8×）。
- PD 分离（Prefill 专用 4-8 卡，Decode 1 卡）：Prefill 阶段独占高算力，无 Decode 抢占，TTFT 方差 <5 %。

6. 输入前处理加速

- 动态 Early-exit（Draft-LLM）：前 50 % 层提前退出，logits 误差 <1 %，TTFT 再降 25 %。
- 层次化 Tokenizer（字节对→子词→词）：长提示编码时间从 2 s→0.2 s。

</p>
</details> 



# 大模型架构Qwen、GPT系列、llama系列，Deepseek系列，Transformer
## Qwen
核心改进总结：RMSNorm + SwiGLU + RoPE 已成为新一代LLM（如LLaMA, Qwen等）的标准配置。
1. RMSNorm (Root Mean Square Normalization)： RMSNorm 把 减均值和学偏置两步都删了，只剩一个缩放向量 γ
2. SwiGLU激活函数： 取代了传统的ReLU或GELU激活函数。SwiGLU是GLU（Gated Linear Unit）的一种，引入了门控机制，被证明在语言模型中能带来更好的性能。
3. 旋转位置编码 (RoPE, Rotary Position Embedding)： 取代了绝对或相对位置编码。RoPE通过旋转矩阵的方式将位置信息编码到注意力计算中，能更好地处理长序列，并具有很好的外推性（ extrapolation）。
4. 分组查询注意力 (GQA, Grouped-Query Attention)： 在较大的模型（如Qwen2-7B/72B）中使用了GQA。它介于MHA（多头注意力）和MQA（多查询注意力）之间，在几乎不损失效果的情况下，极大地减少了推理时KV Cache的内存占用，提高了推理速度。
5. 更长的上下文长度： Qwen2支持128K token的上下文长度，这得益于RoPE和相关的工程优化。
6. Tokenizer改进： 使用了更高效的分词器，词汇表大小扩大到152K，压缩率更高（特别是对中文和代码），减少了序列长度，提升了处理效率。


### RMSNorm以及LayerNorm公式
目前主流的都是rmsnorm
为什么 RMSNorm 更快？
少一次 reduce-mean：LayerNorm 要先算均值，再算方差；RMSNorm 只算一次 RMS。
通信量减半：并行场景下 reduce 次数从 2→1，GPU 空闲时间↓。
无 β 偏移：参数内存减半，加载/优化开销↓。

| 维度    | LayerNorm | RMSNorm       |
| ----- | --------- | ------------- |
| 计算内容  | 减均值 → 除方差 | 直接除 RMS（无减均值） |
| 参数量   | 2 倍（γ, β） | 1 倍（仅 γ）      |
| 零均值   | ✅ 强制      | ❌ 不强制         |
| 速度    | 基准        | **7 %–64 %↑** |
| 大模型表现 | 持平        | **持平或略好**     |

具体公式如下：
<details><summary>Details</summary>
<p>
LayerNorm：
$\text{LayerNorm}(\mathbf{x}) = \frac{\mathbf{x} - \mu}{\sigma} \odot \mathbf{g} + \mathbf{b}$
其中 $\mu = \frac{1}{H}\sum_{i=1}^H x_i$, $\sigma = \sqrt{\frac{1}{H}\sum_{i=1}^H (x_i - \mu)^2 + \epsilon}$, $\mathbf{g}$ 和 $\mathbf{b}$ 是可学习的增益和偏置参数。

RMSNorm：
$\text{RMSNorm}(\mathbf{x}) = \frac{\mathbf{x}}{\text{RMS}(\mathbf{x})} \odot \mathbf{g}$
其中 $\text{RMS}(\mathbf{x}) = \sqrt{\frac{1}{H}\sum_{i=1}^H x_i^2 + \epsilon}$， 通常省略偏置 $\mathbf{b}$。

SwiGLU的公式是什么？
SwiGLU是Swish激活函数和GLU（Gated Linear Unit）结构的结合。

公式：
$\text{SwiGLU}(x, W, V, b, c) = \text{Swish}(xW + b) \odot (xV + c)$
其中：

- $x$ 是输入
- $W, V$ 是权重矩阵
- $b, c$ 是偏置（可选）
- $\odot$ 是逐元素乘法（Hadamard product）
- $\text{Swish}(x) = x \cdot \sigma(x)$， $\sigma$ 是sigmoid函数。

GLU：这种 (A) ⊙ (B) 的结构被称为“门控线性单元”（Gated Linear Unit）。Swish(xW1) 是“门”，它控制 (xV) 哪些部分可以通过。
在Transformer的FFN层中，通常用SwiGLU取代原来的ReLU激活
</p>
</details> 



Q，K，V计算的实际过程：
<details><summary>Details</summary>
<p>

- Query (Q - 查询)：可以理解为“当前正在关注的 token”发出的一个询问：“与我相关的信息是什么？”
- Key (K - 键)：可以理解为所有 token 提供的一个“标识”，用来匹配 Query。Query 会与所有 Key 进行相似度比较。
- Value (V - 值)：可以理解为每个 token 所包含的“实际信息”或“内容”。一旦通过 Q-K 匹配确定了哪些 token 重要，我们就对这些 token 的 Value 进行加权求和。

**实际过程：**
1. 线性投影：通过三个权重矩阵 W^Q, W^K, W^V，将输入 X 分别投影到 Q, K, V 空间。
2. Q = X * W^Q, K = X * W^K, V = X * W^V
3. 计算注意力得分：计算 Query 和所有 Key 的点积，得到相似度得分。Score = Q * K^T
4. 缩放：为了防止点积结果过大导致 Softmax 梯度消失，将得分除以 Key 向量维度的平方根。Scaled_Score = Score / √d_k
5. 应用 Softmax：对缩放后的得分应用 Softmax 函数，得到归一化的注意力权重（所有权重和为1）。Attention_Weights = Softmax(Scaled_Score)
6. 加权求和：用注意力权重对 Value 向量进行加权求和，得到最终的输出。Output = Attention_Weights * V

这个输出就包含了当前 token 从序列所有其他 token 那里聚合到的上下文信息。


1. 多头自注意力机制 (Multi-Head Self-Attention)
- “自” (Self) 的含义：输入序列自身内部元素之间进行注意力计算。例如，句子中的每个词都会与句子中的所有词（包括自己）进行关联，从而捕捉词与词之间的语义和语法关系。
- attention核心公式：Attention(Q, K, V) = softmax(QK^T / √d_k) V
- 让模型能够“同时看到”整个序列，并理解每个词在上下文中的真正含义。
1. 前馈神经网络 (Feed-Forward Network, FFN)
- 一个简单的全连接网络，通常包含一个隐藏层和激活函数（如 ReLU）。
- 对自注意力层的输出进行非线性变换和空间映射，增强模型的表达能力。
1. 残差连接 (Add) 和层归一化 (Norm)
- 残差连接 (Residual Connection)：将子层的输入直接加到其输出上（Output = Sublayer(input) + input）。这有效地缓解了深层网络中的梯度消失问题，使得模型可以堆叠得很深。
- 层归一化 (Layer Normalization)：对残差相加后的结果进行归一化，稳定训练过程，加快收敛
公式表示一个 Encoder Layer：
EncoderOutput = LayerNorm( SelfAttention(Input) + Input )
FinalOutput = LayerNorm( FFN(EncoderOutput) + EncoderOutput )
</p>
</details>

**Decoder解码器组成**
根据 Encoder 产生的中间表示，自回归地（一个一个地）生成输出序列（如翻译后的句子）。
<details><summary>Details</summary>
<p>

1. 掩码多头自注意力机制 (Masked Multi-Head Self-Attention)
- “掩码” (Masked) 的含义：为了防止在训练时“偷看”未来信息（即解码第 t 个词时只能看到 1 到 t-1 的词），通过一个掩码矩阵将当前位置之后的所有信息屏蔽掉（设置为负无穷，softmax 后变为 0）。
1. 多头交叉注意力机制 (Multi-Head Cross-Attention)
- “交叉” (Cross) 的含义：Decoder 的表示 与 Encoder 的最终输出 进行注意力计算。
- Query (Q) 来自 Masked Self-Attention 的输出。
- Key (K) 和 Value (V) 来自 Encoder 的最终输出。
- 这是 Encoder 和 Decoder 之间信息交互的桥梁，让 Decoder 在生成每一个词时都能有针对性地关注输入序列中最相关的部分。
1. 前馈神经网络 (Feed-Forward Network, FFN)
每个子层都伴随着残差连接和层归一化。
</p>
</details>


# 不同推理框架
目前推理框架包含vllm、SGlang、LMDeploy、TensorRT-LLM

四者对比：
图编译器（生成引擎） → TensorRT-LLM  ，优化是预编译+算子融合+FP8-TC，编译慢，但是执行快
推理引擎（动态调度） → vLLM，适合需要处理大量并发请求的在线服务
前缀缓存 → SGLang。SGLang = “前端结构化生成语言 + 后端三层推理引擎”，用 RadixAttention 做 KV-Cache 前缀复用，再辅以 PD 分离、Rust 高并发 Router 与 多精度 Cube 内核，把大模型推理的延迟、吞吐、显存一起打下来。
服务框架（HTTP/gRPC+批处理）→ Triton，优化是动态批+并发执行+多模型编排
国产化/训练推理一体 → LMDeploy
具体区别如下：
<details><summary>Details</summary>
<p>
SGLang：追求开发效率，适合需要复杂提示词处理和快速迭代的研究场景
只追求延迟/吞吐 → TensorRT
快速上线+多硬件 → vLLM
要 REST/gRPC+监控+多模型编排 → Triton（后端可挂 TensorRT 或 vLLM）
</p>
</details> 

## vllm推理框架架构

vLLM 采用分层架构设计，主要包括控制面和执行面。

- 控制面负责请求调度、显存管理和资源分配，核心是 *Scheduler* 和 *BlockSpaceManager*。
- 执行面负责模型计算和推理任务执行，核心是 *Worker*（内含 ModelRunner 负责模型前向传播，CacheEngine 管理 KV Cache）

## 请求调度

vLLM 的调度器 (Scheduler) 是大脑，其核心策略是 Continuous Batching (连续批处理)

<details><summary>Details</summary>
<p>
- 基于 Token 调度：以 Token 为最小调度单位
- 调度过程：Scheduler 维护 waiting（新请求）、running（正在处理）、swapped（因资源不足被换出的请求）队列。其调度优先级通常是：swapped > waiting > running，优先处理已被换出的请求以避免资源浪费，并在资源足够时从 waiting 队列加入新请求。
- 工作流程：
1. 新请求进入 waiting 队列。
2. Scheduler 根据调度策略（如 FCFS）、当前 running 队列的负载、空闲物理块等因素，决定将哪些 waiting 或 swapped 队列中的请求加入 running 队列以进行下一批计算。
3. Worker 执行 running 队列中请求的推理计算。
4. 生成 Token 后，更新序列状态。若请求完成则释放资源，否则根据情况放回 running 或换出到 swapped 队列。
5. 循环上述过程。
</p>
</details> 

## vllm推理整体流程
### 一条请求发送到vllm全过程：
请求进 api_server → 预算调度 scheduler.schedule() → Worker execute_model 一次 forward → sample 得 next_token → ZMQ （嵌入进程的异步通信库）流回客户端；token 完立即 free_blocks 回收 KV。

vLLM V1 是一次意义重大的架构升级，把「调度-执行-IO」拆成独立进程，用增量通信换掉全局广播，默认打开 chunk-prefill、前缀缓存，对称张量并行和持久化批次等新优化，同等 GPU 下吞吐提升 10-30%，TTFT 降 30% 以上。
- V0架构: - 使用BlockManager和Scheduler作为独立组件 - 调度器直接与BlockManager交互管理KV缓存 - 在LLMEngine中集成了大量功能逻辑；
- V1架构: - 引入了更模块化的设计，将功能分散到专门的组件中 - 使用KVCacheManager替代BlockManager，提供更抽象的接口 - 引入EngineCoreClient和Processor等组件实现更清晰的职责分离 - 采用更统一的调度方式，不再严格区分预填充和解码阶段


### 推理任务的调度与抢占：

vllm采用了 FCFS（first-come-first-serve） 的策略；
vllm中块回收策略：All-or-Nothing

### 块恢复策略：Swapping 与 Recomputation
Swapping（交换）：将被抢占请求的 KV Cache 块从 GPU 移动到 CPU 内存。当有空闲 GPU 块时再从 CPU 恢复回来继续执行。为了控制资源使用，被交换（swapped）到 CPU 内存中的 KV Cache 块数量，永远不会超过 GPU 中物理块的总数。
Recomputation（重计算）：直接丢弃已生成的 KV Cache，待请求恢复后重新计算。
根据论文中的实验结果，Swapping 更适用于 block size 较大的场景，而 Recomputation 则在不同 block size 下的表现更为稳定。在中等 block size（16 到 64）范围内，两种方式的端到端性能基本相当。




# HCCL
华为对应集合通信库，支持allreduce、broadcast、allgather、reduceScatter、AlltoAll等通信原语
broadcast：1到多
gather：多到1
scatter：1个npu数据切分到其它npu
reduce：多个npu获取数据并做规约运算

allreduce：从多个npu获取数据到所有npu并做归约运算
reduce scatter：按维度执行规约并发送到对应npu
allgather：收集每个npu所有的数据到每个npu上
alltoallv：所有npu互相scatter和gather数据

前沿思想：
## DSA
MHA：同时用 H 个固定头算注意力，显存 ∝ H；
DSA：细粒度稀疏注意力
• 动态分层稀疏：结合粗粒度压缩与细粒度选择，保留全局和局部信息。
• 硬件对齐设计：采用分块处理，适配GPU连续内存访问，优化Tensor Core利用

技术路线角度：每个 token 动态选 1 个“最优单头” 计算，显存 ≈ 1；（模型学会“抓关键”，只计算最相关的词关系子集）
靠在线路由把“多头多样性”转成“时序多样性”，KV 缓存与计算量都降 H 倍，长序列成本直降 50 % 以上。
<details><summary>Details</summary>
<p>

NSA 算法 = “三路径分层稀疏”
粗粒度：对 KV 做 per-block 压缩表示（类似 Quest 的 block-summary）
细粒度：对整段 KV 做 Top-k 重要 token 选择（可学习门控）
局部：固定 滑动窗口 保证邻近上下文
三条路径并行算完再合并输出，每 query 只落在绿色区域（≈10 % 位置），复杂度从 O(n²) 降到 O(n) 且 原生可训练。
架构落地 = Lightning Indexer + NSA Triton Kernel
Lightning Indexer 负责在 预填充前 用 128-d 低秩向量 + 少头 ReLU 打分，毫秒级给出 Top-k 稀疏块索引，替代传统“全量 Softmax”海选；
NSA Triton Kernel 基于 FlashAttention-2 改造，以 GQA 组为粒度 把三段 KV 块一次性载入 SRAM，TensorCore 利用率>90 %，解码/前向/反向均提速。
在 vLLM 代码中对应：
vllm/attention/backends/flash_attn.py  → 新增 NsaFlashAttention
vllm/v1/core/lightning_indexer.py      → Lightning Indexer 实现

相关文章：https://mp.weixin.qq.com/s/-dBxaYyisXwIvPNRFZCrCQ

</p>
</details> 

## AFD思想
Transformer模型中的Attention层和FFN层具有截然不同的计算和访存特性。
Attention层：在解码阶段通常是内存带宽瓶颈型的，需要频繁访问KV缓存，对内存带宽要求高。
FFN层（尤其是MoE模型中的专家）：则更多是计算瓶颈型的，涉及大量矩阵运算，对计算能力要求高。

将两者分离，允许我们为它们分别匹配最合适的硬件资源。例如，可以将Attention模块部署在高带宽内存的GPU上，而将FFN/专家模块部署在高算力的GPU或NPU上，并通过高速网络连接。这种异构部署能够显著提升整体系统的吞吐量和资源利用率。

AFD作为一种前沿的系统架构思想，正受到业界越来越多的关注和应用：

vLLM的集成计划：高效能的LLM推理框架vLLM项目已经在GitHub上开启了关于为MoE模型引入ATTN-FFN解耦（AFD） 的提案和讨论，旨在未来版本中支持这一特性。
与Prefill-Decoding解耦的结合：AFD通常被构建在Prefill-Decoding (PD) 分离的架构之上，这使得系统设计可以更专注于优化解码阶段，从而实现更精细化的资源管理和调度


## TPA（Tensor Product Attention）
背景通点：标准 MHA 的 KV 缓存 = O(T·h·d_h)，序列一拉长显存指数级膨胀；MLA 虽压缩但难与 RoPE 直接兼容（旋转要作用在最终 key/token 维，而 MLA 存的是压缩向量）。
TPA 用上下文张量分解把 KV 缓存压到 1/10，即时重算全尺寸 key/query → 既省内存又天然兼容 RoPE，T6 模型已证明长序列 PPL 再降 12%，是 2025 首个**‘压缩+旋转’一举两得**的注意力升级。

### SepLLM kv压缩
核心理念来源于一个关键观察：在 Transformer 的 KV Cache 中，有大量信息是冗余的，尤其是在处理语义相近的连续文本时。

SepLLM 的 KV Cache 压缩原理本质上是：通过在线、动态地识别并仅存储上下文序列中信息量最大、最具代表性的 token（骨干序列），同时辅以一个小的滑动窗口（最近序列）来保证局部语言模型效果，从而在极高地压缩显存占用的同时，最大限度地保持了模型的原生性能。 它是一种巧妙利用语义冗余的、高效且实用的推理加速技术。

<details><summary>Details</summary>
<p>

它将 KV Cache 中的信息分为两类：

骨干序列：一组稀疏的、具有代表性的 Key-Value 对。它们构成了当前上下文的“骨架”或“概要”，负责捕捉和保留整个序列的核心语义信息。
最近序列：最近生成的几个 token 的完整的 Key-Value 对。它们负责捕捉局部的、细粒度的语言模式和连贯性。

“分离” 就体现在这里：模型在推理时，会动态地、有选择地更新和维护 骨干序列，而 最近序列 则像滑动窗口一样固定不变。

具体步骤的话：

步骤 1：计算相似度并选择骨干
当一个新的 token 被生成并其 KV 向量准备加入缓存时，SepLLM 不会直接将其加入骨干序列。相反，它会执行以下操作：

- 计算相似度：将新 token 的 Key 向量与当前 骨干序列 中所有已有的 Key 向量进行相似度比较（通常使用余弦相似度）。
- 选择策略：

如果新 token 的 Key 与骨干序列中任何一个现有 Key 的相似度高于某个预设阈值，则认为这个新 token 的语义信息已经被现有的骨干节点所“代表”。此时，不将其加入骨干序列，从而避免了冗余。
如果新 token 的 Key 与所有骨干 Key 的相似度都低于阈值，则认为它带来了新的、独特的语义信息。此时，将其加入骨干序列，作为新的骨干节点。

通过这个动态选择过程，骨干序列只保留那些信息量最大、最具代表性的 token，形成了一个高度压缩的上下文概要。

步骤 2：构建混合 KV Cache 进行注意力计算
在计算注意力时，SepLLM 使用的是混合的 KV Cache：

混合KV Cache = 骨干序列 + 最近序列
- 骨干序列：代表了从序列开始到现在的全局、压缩的语义上下文。
- 最近序列：一个固定大小（例如 128 或 256）的滑动窗口，包含了最近生成的 token，保证了局部语言的流畅性和连贯性。

模型在计算注意力时，Query 会同时与这个混合 Cache 中的 Key 进行交互。这样，模型既能把握全局的语义脉络（通过骨干序列），又能确保下一个生成的词在语法和局部语境上是准确的（通过最近序列）。

步骤 3：维护与更新
- 最近序列：像一个 FIFO（先进先出）队列，新的 token 加入，最老的 token 被移出（如果它不在骨干序列中）。
- 骨干序列：根据上述相似度阈值策略动态增长。为了防止其无限膨胀，也可以设置一个最大骨干数量。当超过时，可以基于一些启发式方法（如最久未使用）合并或移除最不重要的骨干。

</p>
</details> 










































































































































































