## 昇腾 NPU 深度优化
#### 量化回退
**具体过程**
<details><summary>Details</summary>
<p>
1. 离线权重预筛（一次性）
a. 扫描目录下所有 .safetensors
b. 对每张权重张量计算
• 离散度：R = (max-min)/(Q3-Q1)
• 量化误差：KL(P_logit ‖ Q_quant)
c. 规则
• R < 10 且 KL < 0.015 → 标记为 W4A16/W8A8
• 10 ≤ R < 50 或 0.015 ≤ KL < 0.05 → W8A8 per-channel
• 其余 → FP16
d. 结果写入 sensitive_layers.json，上传到对象存储（版本化）。

2. 在线激活监控（毫秒级）
a. 推理框架启动时把 sensitive_layers.json 拉进内存，并起 30 s 热更新协程（检测 md5 变化即重载）。
b. 每个 forward 前插钩子：

if abs(x) > running_mean + 4σ → layer_token_mask |= 1
计数器每 1000 请求清零一次。
c. 若某层计数器 > 1 %，则把该层 KV-Cache 动态切回 FP16（仅对该层生效）。

3. 实时回退路径（零重启）
a. 权重双份存储：INT8 与 FP16 同时存在显存，用 32-bit bit-mask 指示每层/每 token 走哪条路径。
b. 回退粒度
• per-layer：mask 按层开位（最常用）
• per-token：mask 扩成 token 级向量（极端离群）
• per-batch：整 batch 回退（兜底）
c. kernel 内一条 if (mask & (1<<id)) 选路，额外延迟 < 2 µs。

4. 闭环进化（每周自动）
a. 所有回退事件（层、粒度、KL、token-id）实时写 Kafka → 数据湖。
b. 每周 Spark 任务：
• 重新计算各层最新 KL 分布；
• 自动更新 sensitive_layers.json 与钩子阈值；
• 生成 PR 合并主干，实现阈值自进化。

────────────────
效果
• 70B 模型显存再省 12 %，首 token & 增量延迟持平；
• 线上 30 % 灰度 7 天无幻觉回退；
• 全程零重启，阈值随数据自动刷新。
</p>
</details> 


#### Q：• 请量化说明你在 **Page-Attention 上具体改**了哪几行代码/哪几个 kernel，15% 内存节省是如何测出来的？

把 KV-Cache 从「一次性 max_seq_len 连续分配」改成「**16-token 物理页按需分配**」，显存峰值从 38.4 GB 降到 32.6 GB（-15 %），吞吐 +8 %，首 token 延迟持平。



1. 代码级改动（可 diff），cache_manager.cu
page_attention_kernel，对应 davinci 汇编（6 条指令）：
<details><summary>Details</summary>
<p>
1. UBUF  R0, [block_table, seq_idx, token_idx>>4]   // 取 page_id  
2. LSL   R1, token_idx, #4                          // token_idx & 0xF  
3. MUL   R2, R1, #hidden                            // page_off  
4. UBUF  R3, [page_table, R0]                       // 取物理页基址  
5. ADD   R4, R3, R2                                 // kv_ptr  
6. MOV   kv_ptr, R4                                 // 写回  
</p>
</details> 

2. 页大小 16 的量化依据
16 × 8192 × 2 × 2 B = 512 KB，对齐 Ascend L2 cache line；实测碎片与表开销的 Pareto 最优点。

3. 15 % 内存节省测量
• 工具：Ascend Profiling Tool（ms-level 采样）
• 数据：1000 条金融 FAQ，batch=64，max_seq_len=4096
• 结果：峰值显存 38.4 GB → 32.6 GB（-15 %），吞吐 +8 %，P99 延迟持平。



## vLLM 框架适配 & 开源贡献
### 量化改进点
W8A8C8，C8 指 KV-Cache 也压到 8-bit。C8 内部确实分两条路线：

C8-S：static per-channel scale（离线校准，一条序列一个 scale）；
C8-D：dynamic per-token scale（运行时实时统计，误差更低，但多一次 reduce）。
我的改进点：
• 在 C8-S 基础上加 “**token-wise 残差补偿**”：对 >3 σ 的离群 token 保留 4 位残差，显存只多 0.3 %，Rouge-L 从 42.1 → 43.8；
• 把 C8-D 的 reduce 操作融合到 NPU 的 cube 指令里，额外延迟 < 5 µs，完全隐藏；
• 最终线上 70B 模型显存再省 12 %，P99 延迟持平。

**token-wise 残差补偿**
<details><summary>Details</summary>
<p>
token-wise 残差补偿 = 对每个 token 的 KV-Cache 8-bit 量化误差做“小补丁”，只存离群 token 的 4-bit 残差，显存几乎不涨，却能把长序列 Rouge-L 拉回 1.5-2 pt。下面给出可直接落地的 3 步实现细节。

────────────────

离线找离群 token
• 用 512 条真实 prompt（平均 2 k token）跑 FP16 基线，记录每个 token 的 K/V 向量。
• 对同一 token 的 128-dim K/V 做 abs-max 量化到 INT8，计算相对误差 ε = ‖FP16 – Deq(INT8)‖₂ / ‖FP16‖₂。
• 设定阈值 τ = 3 × 平均 ε（经验值 0.035），把 ε > τ 的 token 标记为离群，占比 ≈ 4 %。

4-bit 残差编码
• 对每个离群 token 的 128-dim K/V 向量，用 block-fp4（1×1.3.0 格式：1 sign + 3 exponent + 0 mantissa）存残差。
• 128 个数 → 64 B；再加 1 B 的 token-id 索引，平均每条序列额外 4 %×2 k×65 B ≈ 5 KB，显存膨胀 < 0.3 %。

运行时融合
• kernel 伪码：

int8_kv  = q8_table[token_id];                 // 主 8-bit 值
if (mask[token_id]) {                          // 离群表
    fp4_res  = load_fp4(residual_ptr, idx);    // 64 B
    kv       = deq8(int8_kv) + deq4(fp4_res);  // FMA
} else {
    kv       = deq8(int8_kv);
}
• 把 load_fp4 和 deq4 写成一条 Ascend vector 指令，用的是 VEC_FMA_I4_F16（INT4→FP16 FMA），延迟 0.8 µs，完全落在计算 bubble 里。

────────────────
效果
• 70B 模型、4k ctx：Rouge-L 从 42.1 → 43.8（+1.7），显存只多 0.3 %，P99 延迟持平。
• 线上灰度 30 % 流量 7 天无回退，已合入内部推理框架。

</p>
</details> 


## 大模型推理通用方法论
#### • 客户要求 99.9% 的请求 P99 延迟 < 200 ms，但模型参数量翻倍，你会先做哪三项优化？
客户要求 99.9% 的请求 P99 延迟 < 200 ms，但模型参数量翻倍，你会先做哪三项优化？
A：

4-bit weight-only 量化：把 20 GB 权重压到 5 GB，加载时间 -70%，kernel 延迟 +3%，可接受；
Chunked Prefill + Continuous Batching：prefill 阶段按 512-token chunk 切分，batch 内已解码 token 继续走 decode 路径，P99 延迟从 260 ms 降到 185 ms；
投机解码（2-step）：用 1/10 参数的小模型生成候选 token，主模型一次验证 4 个 token，实测在百科问答场景 acceptance rate 0.72，整体延迟再降 22%。
三项叠加后，P99 延迟 175 ms，满足 SLA，吞吐还提升 1.4×。

#### • 解释为什么 dynamic batching 在 NPU 上比在 GPU 上收益更高/更低？
– 计算密度决定：NPU 的矩阵单元（如达芬奇 Cube）峰值算力高，但 scalar/vector 单元相对弱，只要能把 batch 做大，就能让矩阵单元吃满；GPU 则因为 CUDA core 与 Tensor Core 并存，小 batch 也能保持较高利用率。
– 内存层次差异：NPU 的片上 SRAM 容量小，prefill 阶段 KV Cache 必须一次性驻留，dynamic batching 可以把多条短 prompt 拼成一个长序列，减少 SRAM 换入换出；GPU 有更大的 L2/L3，碎片问题没那么尖锐。
– 实测：在 910B 上把 8×512 token 拼成 1×4096 token，矩阵单元利用率从 42 % 提升到 78 %，而 A100 仅提升 8 %。



#### • 如果百度自研芯片只有 16 GB HBM，而模型权重 20 GB，你会怎么设计 weight-streaming？
<details><summary>Details</summary>
<p>
按层切分：每层权重 < 1 GB，运行时只驻留「当前层 + 下一层」共 2 GB，其余放 host DDR；
double-buffer pipeline：
  – 计算当前层时，DMA 异步把下一层权重拉到预留的 2 GB buffer；
  – 计算完立即切换指针，延迟隐藏约 95 %；
压缩 + 量化：权重先 4-bit group-wise 量化，磁盘 20 GB → 5.5 GB，传输带宽需求再降 4×；
预取策略：根据 prompt 长度预测 decode 步数，提前 2 步把后续 3-4 层拉上来，防止 bubble；
实测：在 70B 模型、4k prompt 场景下，首 token 仅增加 18 ms（< 5 %），decode 阶段无回退。
</p>
</details> 