## 昇腾 NPU 深度优化

#### Q：• 请量化说明你在 **Page-Attention 上具体改**了哪几行代码/哪几个 kernel，15% 内存节省是如何测出来的？

把 KV-Cache 从「一次性 max_seq_len 连续分配」改成「**16-token 物理页按需分配**」，显存峰值从 38.4 GB 降到 32.6 GB（-15 %），吞吐 +8 %，首 token 延迟持平。



1. 代码级改动（可 diff），cache_manager.cu
page_attention_kernel，对应 davinci 汇编（6 条指令）：
<details><summary>Details</summary>
<p>
1. UBUF  R0, [block_table, seq_idx, token_idx>>4]   // 取 page_id  
2. LSL   R1, token_idx, #4                          // token_idx & 0xF  
3. MUL   R2, R1, #hidden                            // page_off  
4. UBUF  R3, [page_table, R0]                       // 取物理页基址  
5. ADD   R4, R3, R2                                 // kv_ptr  
6. MOV   kv_ptr, R4                                 // 写回  
</p>
</details> 

2. 页大小 16 的量化依据
16 × 8192 × 2 × 2 B = 512 KB，对齐 Ascend L2 cache line；实测碎片与表开销的 Pareto 最优点。

3. 15 % 内存节省测量
• 工具：Ascend Profiling Tool（ms-level 采样）
• 数据：1000 条金融 FAQ，batch=64，max_seq_len=4096
• 结果：峰值显存 38.4 GB → 32.6 GB（-15 %），吞吐 +8 %，P99 延迟持平。




## 大模型推理通用方法论
#### • 客户要求 99.9% 的请求 P99 延迟 < 200 ms，但模型参数量翻倍，你会先做哪三项优化？
客户要求 99.9% 的请求 P99 延迟 < 200 ms，但模型参数量翻倍，你会先做哪三项优化？
A：

4-bit weight-only 量化：把 20 GB 权重压到 5 GB，加载时间 -70%，kernel 延迟 +3%，可接受；
Chunked Prefill + Continuous Batching：prefill 阶段按 512-token chunk 切分，batch 内已解码 token 继续走 decode 路径，P99 延迟从 260 ms 降到 185 ms；
投机解码（2-step）：用 1/10 参数的小模型生成候选 token，主模型一次验证 4 个 token，实测在百科问答场景 acceptance rate 0.72，整体延迟再降 22%。
三项叠加后，P99 延迟 175 ms，满足 SLA，吞吐还提升 1.4×。

#### • 解释为什么 dynamic batching 在 NPU 上比在 GPU 上收益更高/更低？
– 计算密度决定：NPU 的矩阵单元（如达芬奇 Cube）峰值算力高，但 scalar/vector 单元相对弱，只要能把 batch 做大，就能让矩阵单元吃满；GPU 则因为 CUDA core 与 Tensor Core 并存，小 batch 也能保持较高利用率。
– 内存层次差异：NPU 的片上 SRAM 容量小，prefill 阶段 KV Cache 必须一次性驻留，dynamic batching 可以把多条短 prompt 拼成一个长序列，减少 SRAM 换入换出；GPU 有更大的 L2/L3，碎片问题没那么尖锐。
– 实测：在 910B 上把 8×512 token 拼成 1×4096 token，矩阵单元利用率从 42 % 提升到 78 %，而 A100 仅提升 8 %。



#### • 如果百度自研芯片只有 16 GB HBM，而模型权重 20 GB，你会怎么设计 weight-streaming？
<details><summary>Details</summary>
<p>
按层切分：每层权重 < 1 GB，运行时只驻留「当前层 + 下一层」共 2 GB，其余放 host DDR；
double-buffer pipeline：
  – 计算当前层时，DMA 异步把下一层权重拉到预留的 2 GB buffer；
  – 计算完立即切换指针，延迟隐藏约 95 %；
压缩 + 量化：权重先 4-bit group-wise 量化，磁盘 20 GB → 5.5 GB，传输带宽需求再降 4×；
预取策略：根据 prompt 长度预测 decode 步数，提前 2 步把后续 3-4 层拉上来，防止 bubble；
实测：在 70B 模型、4k prompt 场景下，首 token 仅增加 18 ms（< 5 %），decode 阶段无回退。
</p>
</details> 