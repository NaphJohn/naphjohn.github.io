## 昇腾 NPU 深度优化
#### Q：• 请量化说明你在 **Page-Attention 上具体改**了哪几行代码/哪几个 kernel，15% 内存节省是如何测出来的？


A：把原来一次性为每个 seq 预分配 max_seq_len 的 KV Cache，改成按页（page=16 tokens）按需申请。
关键动作：
• 修改 cache_manager.cu 中 allocate_block() 逻辑，把原来的 new float[max_seq_len*hidden] 换成 new float[page_size*hidden]，并维护一张 bitmap 记录已用页；
• 在 page_attention_kernel 里把 token_idx 到物理地址的映射改成查两级表（block_table + offset），新增 6 行汇编级指令；
• 用 Ascend Profiling Tool 跑 1000 条真实金融 FAQ，采样显存峰值。
量化结果：同并发下峰值显存从 38.4 GB 降到 32.6 GB，节省 15%；首 token 延迟持平，吞吐 +8%。

#### • 如果 KV-Cache 从 FP16 压到 INT4，精度掉 2%，你会怎么 trade-off？
#### 描述一次你在达芬奇 core 上做 pipeline 调优时遇到的 bank conflict，如何定位并解决？

## vLLM 框架适配 & 开源贡献
#### Q：vLLM 在昇腾上跑不通 first-token 的场景，你第一步会看哪个模块的日志？
A：
思路：first-token 卡死通常是 kernel launch 失败或输入 shape 不合法。
关键动作：
• 先把 VLLM_LOGGING_LEVEL=DEBUG 打开，定位到 model_runner.py 的 execute_model()；
• 发现 fused_attention fallback 到 torch.nn.MultiheadAttention，而昇腾不支持某些 mask 类型；
• 立即在 ops/ascend/ 下加一条 is_ascend() 分支，改用 flash_attn_varlen_func 的兼容实现，并补 3 个单测。
量化结果：patch 后 first-token 延迟从 2.1 s 降到 180 ms，CI 通过，已合入 vLLM upstream #3456。


## 大模型推理通用方法论
• 客户要求 99.9% 的请求 P99 延迟 < 200 ms，但模型参数量翻倍，你会先做哪三项优化？
• 解释为什么 dynamic batching 在 NPU 上比在 GPU 上收益更高/更低？
• 如果百度自研芯片只有 16 GB HBM，而模型权重 20 GB，你会怎么设计 weight-streaming？