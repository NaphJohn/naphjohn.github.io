# 训练主要流程

##SFT（Supervised Fine-Tuning，监督微调）
监督微调是“大模型出厂”后的第一道工序：
把已经在海量无标注文本里预训练好的基座模型（如 LLaMA、Qwen）拿来；
用少量人工写好的“指令-答案”对（通常几万～几十万条）继续训练；
训练目标跟预训练一样仍是“预测下一个 token”，但只让模型在答案部分算 loss，迫使它学会“听指令、按要求回答”，而不是继续无脑续写。

┌──────────────┐
│  1. 数据工程   │──→ 3T-30T raw tokens → 质量分层（PageRank+fastText+LLM-Score）
│  清洗+去重+配比 │──→ 对话:代码:多模态=6:3:1，token 级采样
└──────┬─────────┘
       ↓
┌──────────────┐
│  2. 预训练     │──→ FP8 混合精度 + 3D-Parallel(TP-PP-DP) + ZeRO-3
│  通用底座      │──→ 14.8T-30T tokens，2-4 个月，H100×2k-8k
│  目标：下个 token 预测 │──→ 保存 final ckpt + 中间 5-10 个 milestone
└──────┬─────────┘
       ↓
┌──────────────┐
│  3. 长上下文扩展 │──→ 继续预训练 10B-100B tokens，RoPE 基频 500k→2M
│  256k-2M ctx   │──→ MLA/Ring-Attention + 序列并行，1-2 epoch
└──────┬─────────┘
       ↓
┌──────────────┐
│  4. 监督微调   │──→ 100k-2M 高质量指令样本（CoT+对话+工具）
│  SFT           │──→ 2-5 epoch，LR 1e-5-5e-6，BF16/FP8，3-7 天
└──────┬─────────┘
       ↓
┌──────────────┐
│  5. 规则/偏好 RL │──→ GRPO or PPO，无人工答案，仅规则/ORM 奖励
│  （CoT 自我进化） │──→ 1-3 轮，每轮 10k-100k 在线轨迹，10-30 天
└──────┬─────────┘
       ↓
┌──────────────┐
│  6. 拒绝采样蒸馏 │──→ 用 5.b 策略产 1M 正确轨迹 → 短 SFT → 更小模型
│  RFT + DPO     │──→ 同时做 DPO/RLHF 对齐人类可读 & 安全
└──────┬─────────┘
       ↓
┌──────────────┐
│  7. 评估 & 量化上线 │──→ Open-LLM-Leaderboard + 业务私评
│  量化+推理优化   │──→ GPTQ/AWQ/FP8 W8A8 + TensorRT-LLM/vLLM
│  灰度→全量      │──→ 监控幻觉、延迟、成本，回滚 gate
└──────────────┘
“先 3D-并行 FP8 预训练 20 T token 拿底座，再长上下文扩展 → 百万级 SFT → 规则 RL（GRPO/PPO）让模型自己写 CoT → 拒绝采样蒸馏出小模型 → DPO 对齐人类偏好 → 量化上线，全程 3-4 个月，H100×2k 卡。”

# 训练框架
### verl框架
**verl框架**：verl 是一个专为大语言模型（LLM） 设计的强化学习（RL）训练框架

核心创新：HybridFlow 与 3D-HybridEngine
verl是论文《HybridFlow》的开源实现。verl 的核心是其 HybridFlow 架构和 3D-HybridEngine。
核心创新点是提出了基于3D-HybridEngine的高效actor模型动态分片:消除内存冗余,显著降低训练与生成阶段切换时的通信开销。

HybridFlow 通过混合编程模型解耦了RLHF训练中的控制流（高层逻辑，如多个模型角色的交互）和计算流（底层执行，如单个模型的前向/反向传播）。这使得算法工程师可以更专注于高层逻辑的设计，而无需过度纠缠于底层的分布式执行细节。

强化学习算法（GRPO、DAPO、PPO）；推理后端（sglang、vllm）；训练后端（Pytorch FSDP）


HybridFlow内部实际执行过程为:
1. dispatch_fn 将数据分发到每个worker
2. execute_fn 执行真正的远程操作
3. collect_fn 汇总计算的结果

HybridEngine流程：
在RLHF的第i+1轮迭代中,3D-HybridEngine执行以下流程:
1.参数收集 获取第:轮迭代更新的actor模型参数,用于更新各micro-DP组内的推理任务。
2.提示词分发 将prompts加载至每个模型副本,生成response(RLHF的rollout阶段)
3.结果聚合 在micro-DP组内对生成结果执行全收集操作(all-gather)
4.参数重分区 根据3D并行策略重新分配模型参数,准备行动者训练。(实际是直接放弃推理权重,从cpu上加载回训练权重+优化器)
5.模型训练 1、计算actor模型的损失;2、按RLHF算法更新actor模型权重

主要特点：
<details><summary>Details</summary>
<p>

主要特点：
1. 易于扩展的多样化RL算法:采用 Hybrid 编程模型,结合了单控制器和多控制器范式的优点,用户只需几行代码即可构建复杂的 RL数据流。
2. 与现有 LLM 基础设施的无缝集成:通过解耦计算和数据依赖,veRL 能够与现有的 LLM 框架(如 PyTorch FSDP、Megatron-LM 和 vLLM、SGLang)无缝集成,用户也可以轻松扩展到其他 LLM 训练和推理框架。
3. 灵活的设备映射和并行化:支持将模型灵活地映射到不同的GPU 组上,以实现高效的资源利用,并在不同规模的集群上具有良好的扩展性。
4. 与 HuggingFace 模型的轻松集成:veRL能够方便地与 HuggingFace 模型进行集成,提升开发效率。

我们将 actor 的推理特定称为 rollout,而其他模型的推理称为 inference。
Rollout 过程是auto-regressive decoding,是进行训练数据生成,为了性能提升会使用VLLM或SGLang。
Inference过程只会prefill, 为了精度会使用训练框架(FSDP、 Megatron)的前向进行。
训练会使用FSDP、 Megatron。
VeRL可以实现任意方式的资源分配,但是默认的ppo代码实现类似于DeepSpeed-Chat,是将所有的所有的模型都放置在一组资源池上。在不同阶段通过onload/offload和resharding操作来切换。训练和rollout是必须串行的。verl通过hydra工具来管理训练配置。



RL框架的设计问题:
1. 多个模型之间怎么进行调度的,数据是怎么流动的?
2.多个模型是怎么进行资源分配的?,
3. Actor模型在训练和推理阶段是怎么做的权重更新?


**问题一**
single controller vs multi controller
single controller: 整个训练或推理流程由一个中央节点(Controller) 统一协调,所有工作节点(Worker) 接收来自该控制器的指令,执行计算,任务并返回结果。例如pytorch训练中的 DataParallel。代码改起来简单方便。

Single-controller选择Ray作为verl的资源管理后端。然而,Ray默认仅支持"- 个方法调用对应一次RPC”, 而训练LLM通常需要跨多个进程进行协调。为了向用户隐藏这种"单个方法需要调用多个Ray actors"的复杂性,VeRL引入了以下组件:
WorkerGroup(工作进程组)——管理一组远程工作进程,为多进程分布式计算提供统一接口;
ResourcePool (资源池) -将计算资源与工作进程绑定;
ClassWithArgs(带参类)—-支持通过指定初始化参数实现延迟远程实例化。

multi controller: 多个控制器分布式协作管理任务,每个控制器可能负责一部分工作节点或数据分片,通过通信协议(如ring-allreduce算法)协
调全局状态。例如pytorch训练中的 DDP。

当前主流的训练引擎都是 multi controller的,譬如 FSDP、 Megatron 和DeepSpeed, 各个节点之间的状态通过集合通信算法(HCCL/NCCL) 同步当前VLLM 0.7.x版本以上已经支持SPMD,也是multi controller的。



</p>
</details> 


**工作流程（以PPO为例）：**
一个典型的RL训练循环（例如PPO）在verl中大致包含以下步骤：
1. 准备一个 batch 的 prompts;
2. 将这个 batch 的 prompts 输入给 Actor, 通过rollout 得到 responses;
生成 (Generation): Actor模型 接收一批提示词（prompts），生成回应（responses）。
3. 将 prompt+ responses 输入给 Critic/Reward/Reference, 进行inference,分别计算得到 values、reward score和 log probs, 将这些整合称为 experiences;
也就是生成的回应会交由Critic模型（评估状态价值）、奖励模型(RM)（计算即时奖励）和参考模型(Reference Model)（计算与初始模型的KL散度，防止策略偏离太远）进行评估。
4. 根据 experiences 多轮计算 actor loss 和 critic loss 并更新Actor和Critic

### llama-factory框架
llama-factory没有模型并行能力，激活值是一个瓶颈。激活值的增长与序列长度的平方呈线性关系。

底层是transformers+deepspeed
优点：
使用简单，内置了许多开源模型、训练方式、数据集，包含了训练、评测和推理部署多个流程。
兼容huggingface社区开源模型，可以开箱即用。
缺点：
使用zero【让每张 GPU 只存一部分模型状态，用通信换显存】并行，当前在大参数和长序列场景性能差。解决长序列问题的两种并行方式，比如ulysess，ring-attention
数据是边读取边训练，在大数据集上断点续训dataloader会是瓶颈。

训练方法包含PT、SFT、DPO；deepseed zero并行；flashattention

# 大模型中量化方法
| 分类                                     | 发生阶段                | 典型算法                           | 一句话特点                          |
| -------------------------------------- | ------------------- | ------------------------------ | ------------------------------ |
| **PTQ**<br>Post-Training Quantization  | 训练完成后一次性量化          | GPTQ、AWQ、SmoothQuant、ZeroQuant | 零训练成本，工程落地最快，<1% 精度损失即可压到 INT4 |
| **QAT**<br>Quantization-Aware Training | 预训练或微调阶段插入“伪量化”节点   | LLM-QAT、ZeroQuant-LKD、FP8-QAT  | 精度最高，但需修改图、重新训练，成本高            |
| **QA-LoRA**<br>（量化感知参数高效微调）            | 只把 LoRA/Adapter 做量化 | QLoRA、LoftQ、QA-LoRA            | 显存砍 70% 也能微调，科研+产业两头吃香         |

Ascendfactory适配
#3个步骤
ascendfactory-cli config #生成内置模板   --  分布式训练后端MindSpeed（后端基于Megatron-LLM），LlamaFactory（deepspeed+Transformer），Verl；选择模型；训练场景比如grpo
按实际情况修改模板的内容 (***标识的必须修改)   
ascendfactory-cli train xxx.yaml

目前大厂主流落地答案 → “PTQ 为主，QA-LoRA 为辅”
大厂上线 90% 用 PTQ；

比如现在deepseek的训练路线：预训练+SFT VS 纯强化学习
V3：14.8 T token → FP8 预训练 → SFT → RLHF，成本 557 万美元 
R1：用 5 k 条冷启动 CoT → 纯 GRPO 强化学习（无 SFT）→ 二阶段混合训练，激活自我反思与回滚 


GPTQ/AWQ 压到 W4A16（权重 4bit，激活 16bit）已成“事实标准”；同样 7B 模型显存 28 GB→7 GB，首 token 延迟降 2-3×
量化方案前沿技术：比如FP8 混合量化

### 训练结果、参考指标
actor/entropy 模型(Actor)输出的结果分布的熵(entropy),用于模型的探索 熵会逐渐降低,回答趋于稳定程度
critic/score/mean 对当前模型回答结果的评分 持续上升
response_length/mean rollout生成序列的平均长度 先上升后下降(取决于奖励函数的设计)
timing_s/generate sequences 推理需要的时间 和response_length相关
timing_s/update actor Actor训练的时间 和response_length相关
timing_s/step 一个完整step所需要的时间 和response_length相关


## 强化学习中用到算法：
**PPO原理**：
核心思想: 在更新策略时，避免步长太长（更新幅度太大）导致策略崩溃（性能急剧下降）
$L^{CLIP}(\theta) = \hat{\mathbb{E}}_t [\min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t)]$

其中：
- $\theta$： 策略参数。
- $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}$： 新策略与旧策略的概率比。
- $\hat{A}_t$： 时间步t的优势函数估计（表示某个动作比平均动作好多少）。
- $\epsilon$： 超参数（如0.2），限制更新幅度。

关键机制:
比率 (Ratio): r(θ) = π_θ(a|s) / π_θ_old(a|s)。新策略概率除以旧策略概率。
稳定性：因为_直接用sft后的模型会非常不稳定_。裁剪 (Clipping)就是为了稳定性。PPO的损失函数会裁剪这个比率，将其限制在区间 [1 - ε, 1 + ε] 内（ε是一个小超参数，如0.1或0.2）。这确保了每次更新都是“小幅且安全的”。
目标函数: L = min( r(θ) * A, clip(r(θ), 1-ε, 1+ε) * A )。其中A是优势函数。通过取最小值，算法忽略了那些会导致策略发生巨大变化且对性能有负面影响的更新。
性能： 相比于传统的策略梯度方法（如TRPO，虽然理论严谨但计算复杂），PPO用这种简单的裁剪机制达到了相当甚至更好的性能，同时实现更简单、计算更高效。

**GRPO (Group Relative Policy Optimization)**
核心思想: 在有多个人类标注者偏好（可能存在分歧）的场景下，不学习一个单一的奖励模型，而是学习一个奖励模型组。

原理: 训练多个奖励模型 {RM_i}，每个代表不同的人类偏好子群体。在RL优化时，策略的更新要考虑到整个模型组的共识，而不是单个模型。例如，目标函数可能是最小化策略相对于所有奖励模型的最差情况性能（类似于鲁棒优化），或者是根据模型组的平均奖励进行优化。这提高了策略的鲁棒性和公平性。

**DAPO (Distributional Advantage Policy Optimization)**
核心思想: 标准PPO使用优势函数的点估计（一个值），DAPO则使用优势函数的完整分布。

原理: 它建模优势函数的不确定性。通过考虑优势的分布，智能体可以更智能地平衡探索与利用：对于那些优势估计不确定性很高的行动，即使其均值略低，也可能值得探索。这可以带来更稳定、更高效的学习。

**VAPO (Value-Augmented Policy Optimization)**

原理 : 可能通过增加价值函数损失的权重，或使用价值函数来更好地估计优势，以减少方差，提升PPO的稳定性。


一些训练中可能遇到问题：
8. 为什么你要用GRPO？GPRO结果比之前好多少？显存开销多大？训练一个Step需要多久？奖励函数如何设置的，为什么？有没有想过为什么一开始Reward出现大幅度震荡？GRPO是否一定有效，还有什么解决方法？
9. Post-Training 的工作机制，为什么要做三阶段训练？什么情况下应该用GRPO？为什么DeepSeek用了GRPO？如何从V3到R1？
10. 微调是如何进行微调的？为什么LoRA能够work？除了LoRA外，还了解哪些微调方法？
11. 后训练用的哪个框架？你用过什么框架？如何使用deepspeed进行分布式训练？脚本是你自己写的吗？
14. 如何去评价你工作的产出和质量（基本上每个面试官都会问）
15. FLUX的工作原理，LoRA在这个地方起到了什么作用？
链接：https://www.nowcoder.com/feed/main/detail/cb3c1bff8b9d40f0a5431b26ebcc6042?sourceSSR=search


## 不同微调方式
LoRA（低秩适应）：核心思想是用低秩分解来模拟权重更新，适用于绝大多数需要高效微调的场景，是效果和效率的最佳平衡点。
QLoRA（量化LoRA）：核心思想是将模型量化到4位后再进行LoRA微调，适用于显存极其有限（如单张消费级显卡）的场景，极大降低了微调门槛。
AdaLoRA（自适应LoRA）：核心思想是动态分配低秩矩阵的参数量（秩），适用于对微调性能有更高要求或希望智能分配参数预算的场景，能获得比LoRA更优的效果。

## RL中一些基本概念
**人类反馈强化学习(Reinforcement Learning from Human Feedback, RLHF)**：
核心思想:让LLM的输出能够符合人类偏好(即人类反馈),主要通过人类偏好数据集(例如针对一个response,会
有人类手工标注对其打分)训练一个奖励模型,通过奖励模型计算奖励,以此来判断LLM的输出是否符合人类偏好。

早期的RLHF主要涉及到四个模型:
Actor:主模型,即需要更新的策略模型,负责输入prompt,输出token
Ref(reference model) 微调前的模型,负责防止actor模型在更新过程中偏离原模型过大
RM(reward model): 奖励模型,提前训练好的模型,负责为actor的输出打分,主要目标为是否符合人类偏好
Critic (也叫value model):评估模型,负责根据状态来估计按照当前策略进行能够带来多少期望回报,进而计算优势,优势用于更新actor模型,而critic本身也会更新,更新目标是更准确地估计期望回报。
备注：
ref模型，和rm模型都是不会更新，更新的只有critic模型，ref模型（主要防止偏离），现在考虑去掉，因为大模型能力变强

**KL散度（Kullback–Leibler Divergence）公式：**
用于衡量两个概率分布P和Q之间的差异。
离散分布： $D_{KL}(P \parallel Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}$
连续分布： $D_{KL}(P \parallel Q) = \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} dx$
为什么在RLHF中使用KL散度？
在RLHF中，KL散度被用作奖励函数的一部分：
$R_{\text{total}} = R_{\text{Human}}(x, y) - \beta D_{KL}(\pi_\theta(y|x) \parallel \pi_{\text{SFT}}(y|x))$ 
KL散度惩罚项强制新策略 $\pi_\theta$ 的输出分布不要偏离原始的SFT模型 $\pi_{\text{SFT}}$ 太远。

**熵 (Entropy)**： 对于一个概率分布，熵越高，表示分布越均匀，随机性越大，越不确定；熵越低，表示分布越集中（某个行动概率接近1）。
公式: 对于离散策略 π(a|s)，其熵 H 的计算公式为：H(π(·|s)) = -Σ π(a|s) * log π(a|s)
在RL中的作用:
鼓励探索: 在损失函数中加入熵奖励 (Entropy Bonus)，即 +β * H(π(·|s))，可以鼓励策略保持一定的随机性，防止其过早地收敛到一个局部最优的确定性策略，从而探索更多可能的行为。

平衡方法:
ε-贪婪 (ε-Greedy): 以 (1-ε) 的概率选择当前最优行动（利用），以 ε 的概率随机选择行动（探索）。
上置信界算法 (UCB): 为每个行动的期望奖励增加一个不确定性bonus，优先选择期望奖励高或不确定性大的行动。
基于策略的方法 (Policy-based Methods): 直接学习一个随机策略（如输出行动的概率分布），策略本身的随机性就天然地提供了探索。熵是衡量这种随机性的关键指标

**优势函数** (Advantage Function) A(s, a) = Q(s, a) - V(s)：
使用优势函数而不是纯奖励Q，可以降低方差，使学习更稳定。
常见的改进方法：
1. GAE (Generalized Advantage Estimation)： 这是最常用的方法。GAE通过引入一个权衡偏差和方差的参数 $\lambda$，综合了k步TD误差的优势估计，使得估计更加平滑可靠。
2. Value Function优化： 一个训练良好的价值函数（Critic）是准确估计Advantage的基础。确保价值函数有足够的容量和训练步数。
3. Normalization： 对一个batch内的Advantage进行归一化（减去均值，除以标准差），可以稳定策略的更新。
4. Clipping Advantages： 类似价值损失，也可以对Advantage进行裁剪，防止极端值的影响。

**Lora**：核心是低秩适应，通过引入少量可训练参数高效微调模型；
**QLoRA**：在LoRA的基础上，通过对预训练模型进行量化压缩，显著降低了微调对显存的需求

### Reward奖励获取方式分别是什么
1. 人工标注（Human Annotation） 优点是符合人类偏好，缺点是成本高
2. 模型基于规则/启发式生成（Rule-based/Heuristic Reward）
3. 奖励模型（Reward Model - RM） 首先用1人工标注训练一个单独的奖励模型
4. 对抗训练（Adversarial Training）训练一个判别器

## LLM中损失函数
LLM的损失函数是标准的交叉熵损失（Cross-Entropy Loss），具体是语言建模损失。

对于一个样本（序列 $x_1, x_2, ..., x_T$），损失函数为：
$L = -\frac{1}{T} \sum_{t=1}^{T} \log P(x_t | x_{<t})$
其中 $P(x_t | x_{<t})$ 是模型在给定上文 $x_{<t}$ 的条件下，预测下一个词是 $x_t$ 的概率

交叉熵损失计算为：
$L_t = -\sum_{i=1}^{V} y_i \log(p_i) = -\log(p_{x_t})$
因为只有 $y_{x_t}=1$，其他 $y_i=0$。
所以，对于这一个token的损失就是其负对数概率。整个序列的损失是所有token损失的平均。
举例：如果模型对某个目标词预测的概率是 0.01，那么该token的损失就是 -log(0.01) ≈ 4.605。如果模型非常确信（概率为 0.99），损失就是 -log(0.99) ≈ 0.01。


## 长序列切分的一些策略：
Ulysess: 切分sequence维度,在计算attention时通过AllToAll转为切分head_num维度,实现序列
切分的同时不影响attention计算的正确性。
1.输入序列切分为N个块。的
2.每个GPU拿到一个块,在计算Attention之前通过AlltoAll通信每个GPU拿到完整的序列。,
3.计算Attention。
4.AllToAll把Attention按照sequence维度拆分成N个块分发到各个GPU。

Ring-Attention:切分sequence维度,各个GPU分别计算切分后的序列的KV值,通过环形通信同
步全局的KV,最终计算Attention。
1.输入序列被切分为N个块。
2.每个 GPU 拿到一个块,并计算本地的Q、K、V。
3.使用环形拓扑结构,GPU 依次向下一个设备发送自己的 K/V,并接收前一个设备的K/V。,
4.每次接收到新的 K/V 后,就与本地的 Q做 attention 计算。
5.经过 N-1 轮通信后,每个 GPU 都"看到"了全部的 KN, 完成了全局 attention。
