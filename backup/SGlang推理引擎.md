## SGlang框架
1. **前端语言**：
   - 提供原生成API接口，可以直接与本地模型交互和调用。

2. **OpenAI兼容API**：
   - 支持使用OpenAI提供的模型进行交互，并允许用户通过OpenAI API执行任务。

后端RUNtime
**API 分词器**：负责接收客户端请求，并将输入转换为可处理的 token 格式。初步处理后转发到调度器和模型工作者，并返回结果给客户端。
 **解码器**：将生成的 tokens 转换回可读的文本格式，以便于客户端理解输出内容。
 **调度器与模型 worker**
这部分核心组件负责执行和优化模型：
   - **KV 缓存内存池**：用于存储中间数据量，每个页面大小对应一个 token 以支持高效缓存管理。
   - **基数树缓存**：基于基数树的数据结构，提高 KV 索引效率并减少冗余计算和内存使用。
   - **有限状态机分析器**：通过正则表达式转化为有限状态机来约束输出，压缩加速解码过程。
   - **注意力后端**：利用 FlashInfer 和 Triton 工具进行优化，提升推理速度。


## SGLang 的一些核心优化技术包括：

### RadixAttention：
核心思想：通过基数树（Radix Tree）来管理和复用键值缓存（KV Cache）。当多个请求或一个请求内的多次调用存在共享前缀时（例如多轮对话中共同的历史对话上下文、系统提示词等），RadixAttention 可以避免重复计算，显著提高缓存命中率（据报道可达3-5倍），从而降低延迟并提升吞吐量。
技术优势：
内存上相同前缀只存储一份kv cache；
避免重复计算attention；
动态扩展：支持在线插入新前缀节点；
LRU淘汰：智能管理缓存容量     
PS：radix tree 来管理标记序列与相应kv缓存张量之间的映射

### 缓存感知调度
目的是提高缓存命中率。

### 压缩有限状态机（FSM）用于约束解码：
SGLang 支持通过正则表达式等方式约束模型的输出格式（如强制输出 JSON）。它通过将正则表达式转换为压缩的有限状态机，使得模型在解码时能一次性验证和输出多个 token，而不是传统的逐 token 解码，这大大加速了结构化输出的生成速度。

其他优化：SGLang 还集成了连续批处理（Continuous Batching）、张量并行、分块预填充（用于处理长序列）以及量化支持（如 FP8, AWQ）等现代推理引擎常用技术来进一步提升效率


## SGLang与vllm区别

vLLM 更像是为“大规模量产”而生，追求极致的吞吐和并发效率；而 SGLang 则为“精巧复杂工艺”设计，擅长处理需要多次调用、状态保持和格式控制的复杂LLM程序。


# omini
多目标插件框架

