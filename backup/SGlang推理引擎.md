## SGlang框架
1. **前端语言**：
   - 提供原生成API接口，可以直接与本地模型交互和调用。

2. **OpenAI兼容API**：
   - 支持使用OpenAI提供的模型进行交互，并允许用户通过OpenAI API执行任务。

后端RUNtime
**API 分词器**：负责接收客户端请求，并将输入转换为可处理的 token 格式。初步处理后转发到调度器和模型工作者，并返回结果给客户端。
 **解码器**：将生成的 tokens 转换回可读的文本格式，以便于客户端理解输出内容。
 **调度器与模型 worker**
这部分核心组件负责执行和优化模型：
   - **KV 缓存内存池**：用于存储中间数据量，每个页面大小对应一个 token 以支持高效缓存管理。
   - **基数树缓存**：基于基数树的数据结构，提高 KV 索引效率并减少冗余计算和内存使用。
   - **有限状态机分析器**：通过正则表达式转化为有限状态机来约束输出，压缩加速解码过程。
   - **注意力后端**：利用 FlashInfer 和 Triton 工具进行优化，提升推理速度。


## SGLang 的一些核心优化技术包括：

### RadixAttention：
核心思想：通过基数树（Radix Tree）来管理和复用键值缓存（KV Cache）。当多个请求或一个请求内的多次调用存在共享前缀时（例如多轮对话中共同的历史对话上下文、系统提示词等），RadixAttention 可以避免重复计算，显著提高缓存命中率（据报道可达3-5倍），从而降低延迟并提升吞吐量。
技术优势：
内存上相同前缀只存储一份kv cache；
避免重复计算attention；
动态扩展：支持在线插入新前缀节点；
LRU淘汰：智能管理缓存容量     
PS：radix tree 来管理标记序列与相应kv缓存张量之间的映射

一、RadixAttention：用「基数树」做 KV-Cache 前缀复用
核心数据结构
传统缓存：哈希表 or 简单前缀树 → 只能整段命中/未命中。
RadixAttention：基数树（Patricia Trie），边可存“变长 token 序列”，节点即一段 KV-Cache 张量。
工作流程（以多轮对话为例）
① 新请求到达 → 在树里做 最长前缀匹配（O(L) 时间，L=token 数）。
② 命中：直接 复用该节点及祖先的全部 KV-Cache；未命中：沿途分裂/插入新节点。
③ 生成阶段产生的 KV 同样写回树中，成为后续请求的新前缀。
④ 内存满时 LRU 叶子淘汰，并自动合并/分裂节点，保持树平衡 。
收益
官方 Llama-7B 多轮场景：缓存命中率 70%+ → 吞吐量比 vLLM 高 5 倍，首 token 延迟降 40% 

### 编译器式设计
编译阶段
框架把 DSL 解析成 统一计算图（节点=LLM 调用，边=数据依赖）。
图优化：
– 自动 合并相邻 Prompt（降低调度次数）
– 自动插入 RadixAttention 节点（提示复用）
– 运算符融合（连续采样→一次 kernel 发射）
运行时
整张图一次性提交给后端，连续批处理 + KV-Cache 复用 全链路打通，避免 Python→C++ 反复往

### 结构化输出
**原理**
在 采样阶段 就把正则或 CFG 规则注入 logits mask：
每步只保留 符合 JSON 键名/括号匹配 的 token 概率 > 0，其余置 −∞。
保证 首次采样即合法，无需后续重试或修复。
**实现**
SGLang 内置 sgl.gen(regex=r'"name":\s*"[^"]*"')；底层调用 outlines 库，编译成 DFA → token mask 表，GPU 每步查表耗时 < 1 µs 。
**收益**
对比“先自由生成再正则校验”：
结构化成功率 99.8% → 100%
生成速度↑ 10×（省去重采样）
后端代码量减少 40%（无需 retry/修复）

### 压缩有限状态机（FSM）用于约束解码：
SGLang 支持通过正则表达式等方式约束模型的输出格式（如强制输出 JSON）。它通过将正则表达式转换为压缩的有限状态机，使得模型在解码时能一次性验证和输出多个 token，而不是传统的逐 token 解码，这大大加速了结构化输出的生成速度。

其他优化：SGLang 还集成了连续批处理（Continuous Batching）、张量并行、分块预填充（用于处理长序列）以及量化支持（如 FP8, AWQ）等现代推理引擎常用技术来进一步提升效率


## SGLang与vllm区别

vLLM 更像是为“大规模量产”而生，追求极致的吞吐和并发效率；而 SGLang 则为“精巧复杂工艺”设计，擅长处理需要多次调用、状态保持和格式控制的复杂LLM程序。


# omini
多目标插件框架

