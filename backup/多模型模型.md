# ViT模型
ViT = Patch Embedding + Transformer Encoder + cls_token，把“图像”当“句子”直接做自注意力，全局上下文一步到位，无需 CNN。

1. 输入处理
将 H×W×3 图片切成 固定大小 patches（如 16×16），得到 N = HW/16² 个 patch。
每个 patch 展平 → 线性投影到 D 维向量（= token）。
追加 可学习分类 token（cls_token）+ 位置编码（1-D 或 2-D sin/cos），形成长度 N+1 的序列。
2. 模型结构
纯 Transformer Encoder（L 层）：
– 多头自注意力（MSA）
– 前馈网络（MLP）
– LayerNorm + 残差
无卷积、无池化，全局感受野一层到位。
3. 分类头
取 cls_token 的最终表示 → 线性层 → softmax → 类别概率。
4. 训练 & 推理
与 BERT 一样用 大规模监督预训练（ImageNet-21k → ILSVRC）。
推理时一次前向即得全局特征，计算量 ∝ patch 数，适合 GPU 并行。
