# ViT模型
ViT = Patch Embedding + Transformer Encoder + cls_token，把“图像”当“句子”直接做自注意力，全局上下文一步到位，无需 CNN。

1. 输入处理
将 H×W×3 图片切成 固定大小 patches（如 16×16），得到 N = HW/16² 个 patch。
每个 patch 展平 → 线性投影到 D 维向量（= token）。
追加 可学习分类 token（cls_token）+ 位置编码（1-D 或 2-D sin/cos），形成长度 N+1 的序列。
2. 模型结构
纯 Transformer Encoder（L 层）：
– 多头自注意力（MSA）
– 前馈网络（MLP）
– LayerNorm + 残差
无卷积、无池化，全局感受野一层到位。
3. 分类头
取 cls_token 的最终表示 → 线性层 → softmax → 类别概率。
4. 训练 & 推理
与 BERT 一样用 大规模监督预训练（ImageNet-21k → ILSVRC）。
推理时一次前向即得全局特征，计算量 ∝ patch 数，适合 GPU 并行。

# wav2lip
Wav2Lip 用「梅尔频谱 + 参考脸」→ GAN 生成「唇部同步帧」，双判别器保障「对齐 + 清晰」，输入任意音频即可零样本产出嘴型完美对齐的新视频。
**原理总览**
生成对抗网络（GAN） 架构：
– 生成器：编码「语音特征」+「参考人脸」，解码为「唇部同步帧」；
– 双判别器：
① 唇同步判别器（基于改进 SyncNet）→ 判断音-唇是否同步；
② 视觉质量判别器 → 判断画面真实度与清晰度。
损失函数：同步损失 + 对抗损失 + 重建损失联合训练，迫使生成器产出「既对齐又逼真」的口播帧
**网络模块拆解**
Identity Encoder（残差卷积）：提取参考帧的「身份+姿态」特征。
Speech Encoder（2D CNN）：把梅尔频谱编码为「语音动画」特征。
Face Decoder：拼接两类特征 → 上采样 → 生成 96×96 唇部帧。
Lip-sync Expert（预训练 SyncNet）：额外监督，确保音-唇时间对齐 。
**训练 & 推理流程**
人脸检测 → 裁剪嘴部 96×96 区域。
音频转梅尔频谱，滑窗 16 帧为一段。
生成器逐段输出唇部帧；判别器实时反馈同步/真实度。
推理时无需 3D 参数，任何语音 + 任意人脸即可零样本合成



