# ViT模型
ViT = Patch Embedding + Transformer Encoder + cls_token（图像分类），就是「把图切成 patches → 当成一串 token → 做自注意力」，全局上下文一步到位，无需 CNN。

1. 输入处理
将 H×W×3 图片切成 固定大小 patches（如 16×16），得到 N = HW/16² 个 patch。
每个 patch 展平 → 线性投影到 D 维向量（= token）。
追加 可学习分类 token（cls_token）+ 位置编码（1-D 或 2-D sin/cos），形成长度 N+1 的序列。
2. 模型结构
纯 Transformer Encoder（L 层）：
– 多头自注意力（MSA）
– 前馈网络（MLP）
– LayerNorm + 残差
无卷积、无池化，全局感受野一层到位。
3. 分类头
取 cls_token 的最终表示 → 线性层 → softmax → 类别概率。
4. 训练 & 推理
与 BERT 一样用 大规模监督预训练（ImageNet-21k → ILSVRC）。
推理时一次前向即得全局特征，计算量 ∝ patch 数，适合 GPU 并行。

# wav2lip
Wav2Lip 用「梅尔频谱 + 参考脸」→ GAN 生成「唇部同步帧」，双判别器保障「对齐 + 清晰」，输入任意音频即可零样本产出嘴型完美对齐的新视频。
**原理总览**
生成对抗网络（GAN） 架构：
– 生成器：编码「语音特征」+「参考人脸」，解码为「唇部同步帧」；
– 双判别器：
① 唇同步判别器（基于改进 SyncNet）→ 判断音-唇是否同步；
② 视觉质量判别器 → 判断画面真实度与清晰度。
损失函数：同步损失 + 对抗损失 + 重建损失联合训练，迫使生成器产出「既对齐又逼真」的口播帧
**网络模块拆解**
Identity Encoder（残差卷积）：提取参考帧的「身份+姿态」特征。
Speech Encoder（2D CNN）：把梅尔频谱编码为「语音动画」特征。
Face Decoder：拼接两类特征 → 上采样 → 生成 96×96 唇部帧。
Lip-sync Expert（预训练 SyncNet）：额外监督，确保音-唇时间对齐 。
**训练 & 推理流程**
人脸检测 → 裁剪嘴部 96×96 区域。
音频转梅尔频谱，滑窗 16 帧为一段。
生成器逐段输出唇部帧；判别器实时反馈同步/真实度。
推理时无需 3D 参数，任何语音 + 任意人脸即可零样本合成

# Qwen2.5-VL
Qwen2.5-VL 的视觉编码器 = 任意分辨率 ViT + 2D-RoPE + Window Attention + 轻量 Neck，先单独对齐图文特征，再与 LLM 端到端混训，推理时把图像 patch 当「超长文本 token」直接拼进输入，无需额外 CNN 或 Q-Former，这就是「添加」的全部过程。

一、视觉编码器结构（Qwen2.5-VL Vision Encoder）
| 模块                 | 设计要点                        | 作用                     |
| ------------------ | --------------------------- | ---------------------- |
| Patch Embedding    | 16×16 卷积，stride=16          | 把任意长宽图像打成 1D patch 序列  |
| 2D-RoPE            | 将旋转位置编码同时施加在 h、w 两个维度       | 保持 patch 间的二维空间相对位置    |
| Window Attention   | 局部 7×7 window + 移位窗口        | 降低 O(n²) 计算，显存随分辨率线性增长 |
| Global Query Token | 每 4 层插入 1 个可学习 global query | 远程 patch 也能直接交互，防止信息孤岛 |
| Neck（MLP）          | 2 层 MLP 压缩通道 → LLM 词表维度     | 把视觉特征映射到文本 token 空间    |

二、训练流程（3 阶段）
1. Vision-Language Alignment（冻结 LLM）
600 M 图文对 + 400 M OCR 对，只训 Neck 层 + Vision Encoder，让 patch 特征对齐词向量空间。
2. Vision-Language Instruction Tuning（LLM 解冻）
200 M 高质量指令数据（图像描述、图表、定位、视频），全参数训练，学习「看-说」指令。
3. Mixed Instruction Tuning（文本不掉）
再把纯文本 SFT 子集（约 20 M 样本）以 1:1 比例混训，确保文本基准不降级。



