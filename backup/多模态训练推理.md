## Diffusion Models 扩散模型
核心思想是逐步的数据生成过程。
前向过程（Forward Process / Diffusion Process）： 逐步向数据中添加噪声，直到数据完全变成纯高斯噪声。
反向过程（Reverse Process）： 学习如何从纯噪声中一步步地去除噪声，最终恢复出原始数据。
**1. 前向过程（加噪）**
这是一个固定的（非学习的）、线性的过程。它被定义为一个马尔可夫链（Markov Chain），每一步都向数据 $\mathbf{x}_t$ 中添加一小步高斯噪声。

公式： $\mathbf{x}t = \sqrt{\alpha_t} \mathbf{x}{t-1} + \sqrt{1 - \alpha_t} \epsilon_t$， 其中 $\epsilon_t \sim \mathcal{N}(0, \mathbf{I})$
- $\mathbf{x}_0$ 是原始图像。
- $t$ 从 1 到 T（总步数，通常是1000步）。
- $\alpha_t$ 是预先定义好的噪声调度（Noise Schedule），控制每一步添加的噪声量。它随着 $t$ 增大而减小，意味着越往后加的噪声相对越多。

目的： 经过足够多的步数 $T$ 后，$\mathbf{x}_T$ 就完全变成了一个各向同性的高斯噪声（Isotropic Gaussian Noise），不再包含任何原始数据的信息。

**2. 反向过程（去噪）**
这是一个学习的过程，是模型的核心。我们需要训练一个神经网络来学习如何逆转前向过程。

目标： 给定第 $t$ 步的带噪图像 $\mathbf{x}t$ 和时间步 $t$，神经网络需要预测出添加到 $\mathbf{x}{t-1}$ 上的噪声 $\epsilon$，或者直接预测出去噪后的图像 $\mathbf{x}_0$。目前主流是预测噪声。
**神经网络（U-Net）**： 通常使用 U-Net 架构，并加入注意力机制（Attention） 和时间步嵌入（Timestep Embedding）。

- U-Net： 具有编码器-解码器结构，带有跳跃连接，非常适合捕捉图像的多尺度特征并进行像素级的预测。
- 时间步嵌入： 将当前的时间步 $t$ 编码成一个向量，并注入到U-Net的每一层中，让网络知道当前处于去噪的哪个阶段（是早期粗粒度去噪还是后期细粒度修复）。
- 注意力机制： 帮助模型处理图像不同部分之间的长程依赖关系，对于生成全局一致的图像至关重要。
-
**训练过程：**
1. 从训练集中随机取一张图片 $\mathbf{x}_0$。
2. 随机采样一个时间步 $t$ （从 1 到 T）。
3. 采样一个随机噪声 $\epsilon \sim \mathcal{N}(0, \mathbf{I})$。
4. 将噪声按前向过程公式加到图片上，得到 $\mathbf{x}_t$。
5. 将 $\mathbf{x}t$ 和 $t$ 输入神经网络，让网络预测添加的噪声 $\epsilon\theta(\mathbf{x}_t, t)$。
6. 计算预测噪声和真实噪声之间的**均方误差（MSE Loss）**： $L = ||\epsilon - \epsilon_\theta(\mathbf{x}_t, t)||^2$。

**采样/推理过程（生成新图像）：**

1. 从纯高斯噪声 $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$ 开始。
2. 从 $t = T$ 一步步循环到 $t = 1$：
    - 将当前的 $\mathbf{x}t$ 和 $t$ 输入训练好的网络，得到预测的噪声 $\epsilon\theta(\mathbf{x}_t, t)$。
    - 使用公式计算出 $\mathbf{x}_{t-1}$（这个过程会额外加入一点随机噪声，除非是最后一步）。
3. 最终得到生成的高清图像 $\mathbf{x}_0$。

### 与vae/GAN区别
Diffusion Model 相对于 VAE 的最大优势在于生成质量。它牺牲了生成速度。

## DDPM 与 DDIM 的区别
DDPM (Denoising Diffusion Probabilistic Models)：
采样随机而且慢。反向过程的每一步都遵循马尔可夫性（当前状态只依赖于前一步），并且会加入随机噪声。
DDIM (Denoising Diffusion Implicit Models)：
采样确定而且快，支持跳步采样。揭示了扩散模型的反向过程其实是一个_微分方程求解器_。它建立了扩散模型与神经ODE（Neural ODE） 的联系

## Classifier Guidance 与 Classifier-Free Guidance
### Classifier Guidance（分类器引导）
**原理**：
1. 需要训练一个额外的分类器。这个分类器接收带噪图像 x_t 和时间步 t，预测其条件 y（如类别标签）的概率。
2. 在反向去噪过程中，不仅使用扩散模型预测的噪声 ε_θ，还计算这个分类器关于输入 x_t 的梯度 ∇_{x_t} log p(y | x_t)。
3. 用这个梯度来“引导”下一步的生成方向，使其朝着最大化分类器置信度（即更符合条件 y）的方向移动。

**公式（简化）**： ε_guided = ε_θ(x_t, t) - s * σ_t * ∇_{x_t} log p(y | x_t)
s 是引导尺度（guidance scale），控制引导的强度。s 越大，生成结果与条件 y 的对齐越好，但可能会降低多样性。
### Classifier-Free Guidance（无分类器引
**原理：**
1. 彻底省去了分类器。它通过在训练时随机地丢弃条件（例如，有 10% 的概率将文本条件置空），同时训练一个条件扩散模型 ε_θ(x_t, t, y) 和一个无条件扩散模型 ε_θ(x_t, t, ∅)。
2. 在推理采样时，将条件预测和无条件预测进行向量组合，指向更符合条件的方向。

- 公式： ε_guided = ε_θ(x_t, t, ∅) + s * [ε_θ(x_t, t, y) - ε_θ(x_t, t, ∅)]
  -   ε_θ(x_t, t, y)：有条件噪声预测（我们想要的）。
  -   ε_θ(x_t, t, ∅)：无条件噪声预测（随意的，不关心条件的）。
  -   [ε_θ(x_t, t, y) - ε_θ(x_t, t, ∅)]：这个向量指向了“使得生成图像更符合条件 y”的方向。
  -   s 同样是引导尺度，控制方向上的步长。
 总结：Classifier-Free Guidance 通过巧妙的训练方式和采样时的向量运算，实现了更强大、更简单的控制引导，是目前文生图等技术的基石。

## Stable Diffusion
原理
LDM 的架构包含三个核心组件：

1. 自编码器 (VAE)：
- 注意：这个VAE是预先训练好的，在扩散训练和推理过程中是冻结（freeze）的。

2. 扩散模型 (U-Net)：
- 核心创新：扩散过程（加噪/去噪）不是在原始图像 x 上进行，而是在VAE编码后的潜在空间 z 上进行的。
- U-Net 的结构也进行了调整，以适应在潜在空间 z 上操作。

3.条件机制 (Conditioning Mechanism)：
  - 通常使用 CLIP 文本编码器，将输入文本提示词（Prompt）编码成向量表示。
  - 通过 Cross-Attention（交叉注意力） 机制注入到 U-Net 的中间层，实现精准的文本控制生成。

损失函数：
L_SD = E[|| ε - ε_θ(z_t, t, c) ||^2 ]
整个损失函数只做一件事：衡量预测噪声与真实噪声的差距。

**工作流程：**
总结流程：
**随机噪声（潜空间） → U-Net多步去噪（受文本引导） → 干净的潜表示 → VAE解码器 → 最终高清图片**

_训练：_
1. 图像 x 通过编码器 E 得到潜在表示 z = E(x)。
2. 对 z 进行常规的DDPM扩散过程（加噪）。
3. U-Net 学习在潜在空间中去噪，并且通过交叉注意力接受文本条件。

_推理（生成）_：
1. 在潜在空间中随机采样一个噪声 z_T。
2. 用U-Net和DDIM等采样器，在文本条件的引导下，逐步去噪得到 z_0。
3. 将去噪后的潜在表示 z_0 送入VAE解码器 D，得到高清图像 x = D(z_0)。

为什么更高效？
1. 计算复杂度大幅降低：
  - 计算复杂度与数据维度呈指数关系。在 64x64x4 的潜在空间中进行扩散，相比在 512x512x3 的像素空间中进行，计算量和内存占用减少了数十倍。
  - 这是性能提升的最主要原因。
2. 语义集中：
  - 潜在空间 z 是原始图像经过VAE编码压缩后的“精华”，它过滤掉了图像中的高频细节（纹理、噪声）等难以学习又耗费算力的信息，保留了高级的语义和概念信息。
  - 在这个更抽象、更紧凑的空间里进行扩散，模型可以更专注于语义内容的学习和生成，效率更高，效果也更好。
3. 更适合与其它模态对齐：
文本、深度图等控制条件本身也是抽象表示。在潜在空间中进行融合（通过Cross-Attention）比在像素空间更自然、更高效。

结论：Stable Diffusion (LDM) 通过将扩散过程从像素空间迁移到计算成本低得多的潜在空间，在不显著牺牲质量的前提下，极大地降低了计算需求。
SD与VAE区别：
一句话：SD 的 VAE 只是“压缩-解压”工具，图像生成质量与多样性由潜空间扩散模型负责，而非传统 VAE 的采样随机性 

# SD生成模型对应lora微调训练
Stable Diffusion（简称SD）是一种基于扩散过程的图像生成模型，应用于文生图场景，能够帮助我们生成图像。
本文档主要介绍如何利用训练框架PyTorch_npu+华为自研Ascend Snt9B硬件，完成SDXL模型的LoRA微调训练。

# 常见问题
问题：执行训练脚本报SSL证书校验错误。
图1 SSL证书校验错误
解决方法：
在入口py文件中加入os.environ['CURL_CA_BUNDLE'] = ''  ，重新执行即可。
训练入口py文件为 diffusers0.21.0/examples/text_to_image/train_text_to_image_lora_sdxl.py。


# 多模态模型
## qewn2.5 VS qwen3-vl
Qwen2.5-VL 是“强基线”——Dense+128 K+事件级视频；
Qwen3-VL 是“全栈升级”——MoE/DeepStack/Interleaved-MRoPE/256 K→1 M/Thinking 全套上新，长文档、小时级视频、像素级定位、逻辑推理全面跃迁

| 维度     | Qwen2.5-VL      | Qwen3-VL                          |
| ------ | --------------- | --------------------------------- |
| 底座 LLM | Qwen2.5 Dense   | Qwen3 Dense / MoE                 |
| 视觉编码器  | ViT-bigG 14×14  | 更深 ViT 16×16，**DeepStack** 多层融合   |
| 位置编码   | M-RoPE（t, h, w） | **Interleaved-MRoPE**（t/h/w 频率交错） |
| 上下文    | 128 K           | **原生 256 K，可扩 1 M**               |
| 推理分支   | ❌               | ✅ Thinking 版（<think>…</think>）    |
| 最大参数   | 72 B Dense      | 235 B MoE（激活 22 B）                |
| OCR 语种 | 约 20 种          | **32 种** + 倾斜/模糊鲁棒                |
| 视频定位   | 事件片段级           | **秒级时间戳**对齐                       |

二、原理差异速览
DeepStack 融合（Qwen3-VL 独有）
不再只用 ViT 最后一层，而是把 第 8、16、24 层特征同时抽出，经 PatchMerger 对齐后分别注入 LLM 的 1、2、3 层，形成“视觉早融合”，提升细粒度与空间推理 。
Interleaved-MRoPE
把时间、高度、宽度三个维度的旋转频率交错混合，使模型对长视频时序关系更鲁棒，且支持可变帧率输入 。
Text-Timestamp Alignment
视频帧与"<0.8 s>"这类显式时间戳文本绑定，LLM 可直接用自然语言做秒级索引，而 2.5-VL 仅能用粗粒度事件标签 。
Thinking 分支
同 Qwen3 LLM，在 ViL 阶段保留推理头，输出 <think>…</think> 中间思考，再给出最终答案，视觉问答 + 逻辑推理显著上升

输入→输出完整流程（以 Qwen3-VL 为例）
1. 输入预处理
图像：动态分辨率 + 16×16 patch → ViT 多层特征（8/16/24）
视频：每秒采样 1 帧，每帧打时间戳文本 → 与图像同方式编码
文本：正常 tokenizer，但位置 id 由 Interleaved-MRoPE 统一计算 (t, h, w) 。
2. 视觉早融合（DeepStack）
多层 ViT 特征 → PatchMerger → 视觉 token 序列
视觉 token 与文本 token 交错拼接，形成统一序列。
3. LLM 前向
底座：Qwen3-MoE（30 B-A3B 例）
每 token 激活 3 B 参数，Thinking 版会先输出推理链 <think>…</think>，再输出最终文本 。
4. 后处理
支持 JSON 带坐标（BBox/Points）或 秒级时间戳（视频）
可返回 <think> 内容供上层折叠显示。
