## Diffusion Models 扩散模型
核心思想是逐步的数据生成过程。
前向过程（Forward Process / Diffusion Process）： 逐步向数据中添加噪声，直到数据完全变成纯高斯噪声。
反向过程（Reverse Process）： 学习如何从纯噪声中一步步地去除噪声，最终恢复出原始数据。
**1. 前向过程（加噪）**
这是一个固定的（非学习的）、线性的过程。它被定义为一个马尔可夫链（Markov Chain），每一步都向数据 $\mathbf{x}_t$ 中添加一小步高斯噪声。

公式： $\mathbf{x}t = \sqrt{\alpha_t} \mathbf{x}{t-1} + \sqrt{1 - \alpha_t} \epsilon_t$， 其中 $\epsilon_t \sim \mathcal{N}(0, \mathbf{I})$
- $\mathbf{x}_0$ 是原始图像。
- $t$ 从 1 到 T（总步数，通常是1000步）。
- $\alpha_t$ 是预先定义好的噪声调度（Noise Schedule），控制每一步添加的噪声量。它随着 $t$ 增大而减小，意味着越往后加的噪声相对越多。

目的： 经过足够多的步数 $T$ 后，$\mathbf{x}_T$ 就完全变成了一个各向同性的高斯噪声（Isotropic Gaussian Noise），不再包含任何原始数据的信息。

**2. 反向过程（去噪）**
这是一个学习的过程，是模型的核心。我们需要训练一个神经网络来学习如何逆转前向过程。

目标： 给定第 $t$ 步的带噪图像 $\mathbf{x}t$ 和时间步 $t$，神经网络需要预测出添加到 $\mathbf{x}{t-1}$ 上的噪声 $\epsilon$，或者直接预测出去噪后的图像 $\mathbf{x}_0$。目前主流是预测噪声。
**神经网络（U-Net）**： 通常使用 U-Net 架构，并加入注意力机制（Attention） 和时间步嵌入（Timestep Embedding）。

- U-Net： 具有编码器-解码器结构，带有跳跃连接，非常适合捕捉图像的多尺度特征并进行像素级的预测。
- 时间步嵌入： 将当前的时间步 $t$ 编码成一个向量，并注入到U-Net的每一层中，让网络知道当前处于去噪的哪个阶段（是早期粗粒度去噪还是后期细粒度修复）。
- 注意力机制： 帮助模型处理图像不同部分之间的长程依赖关系，对于生成全局一致的图像至关重要。
-
**训练过程：**
1. 从训练集中随机取一张图片 $\mathbf{x}_0$。
2. 随机采样一个时间步 $t$ （从 1 到 T）。
3. 采样一个随机噪声 $\epsilon \sim \mathcal{N}(0, \mathbf{I})$。
4. 将噪声按前向过程公式加到图片上，得到 $\mathbf{x}_t$。
5. 将 $\mathbf{x}t$ 和 $t$ 输入神经网络，让网络预测添加的噪声 $\epsilon\theta(\mathbf{x}_t, t)$。
6. 计算预测噪声和真实噪声之间的**均方误差（MSE Loss）**： $L = ||\epsilon - \epsilon_\theta(\mathbf{x}_t, t)||^2$。

**采样/推理过程（生成新图像）：**

1. 从纯高斯噪声 $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$ 开始。
2. 从 $t = T$ 一步步循环到 $t = 1$：
    - 将当前的 $\mathbf{x}t$ 和 $t$ 输入训练好的网络，得到预测的噪声 $\epsilon\theta(\mathbf{x}_t, t)$。
    - 使用公式计算出 $\mathbf{x}_{t-1}$（这个过程会额外加入一点随机噪声，除非是最后一步）。
3. 最终得到生成的高清图像 $\mathbf{x}_0$。

### 与vae/GAN区别
Diffusion Model 相对于 VAE 的最大优势在于生成质量。它牺牲了生成速度。

## DDPM 与 DDIM 的区别
DDPM (Denoising Diffusion Probabilistic Models)：
采样随机而且慢。反向过程的每一步都遵循马尔可夫性（当前状态只依赖于前一步），并且会加入随机噪声。
DDIM (Denoising Diffusion Implicit Models)：
采样确定而且快，支持跳步采样。揭示了扩散模型的反向过程其实是一个_微分方程求解器_。它建立了扩散模型与神经ODE（Neural ODE） 的联系

## Classifier Guidance 与 Classifier-Free Guidance
### Classifier Guidance（分类器引导）
**原理**：
1. 需要训练一个额外的分类器。这个分类器接收带噪图像 x_t 和时间步 t，预测其条件 y（如类别标签）的概率。
2. 在反向去噪过程中，不仅使用扩散模型预测的噪声 ε_θ，还计算这个分类器关于输入 x_t 的梯度 ∇_{x_t} log p(y | x_t)。
3. 用这个梯度来“引导”下一步的生成方向，使其朝着最大化分类器置信度（即更符合条件 y）的方向移动。

**公式（简化）**： ε_guided = ε_θ(x_t, t) - s * σ_t * ∇_{x_t} log p(y | x_t)
s 是引导尺度（guidance scale），控制引导的强度。s 越大，生成结果与条件 y 的对齐越好，但可能会降低多样性。
### Classifier-Free Guidance（无分类器引
**原理：**
1. 彻底省去了分类器。它通过在训练时随机地丢弃条件（例如，有 10% 的概率将文本条件置空），同时训练一个条件扩散模型 ε_θ(x_t, t, y) 和一个无条件扩散模型 ε_θ(x_t, t, ∅)。
2. 在推理采样时，将条件预测和无条件预测进行向量组合，指向更符合条件的方向。

- 公式： ε_guided = ε_θ(x_t, t, ∅) + s * [ε_θ(x_t, t, y) - ε_θ(x_t, t, ∅)]
  -   ε_θ(x_t, t, y)：有条件噪声预测（我们想要的）。
  -   ε_θ(x_t, t, ∅)：无条件噪声预测（随意的，不关心条件的）。
  -   [ε_θ(x_t, t, y) - ε_θ(x_t, t, ∅)]：这个向量指向了“使得生成图像更符合条件 y”的方向。
  -   s 同样是引导尺度，控制方向上的步长。
 总结：Classifier-Free Guidance 通过巧妙的训练方式和采样时的向量运算，实现了更强大、更简单的控制引导，是目前文生图等技术的基石。

## Stable Diffusion
原理
LDM 的架构包含三个核心组件：

1. 自编码器 (VAE)：
- 注意：这个VAE是预先训练好的，在扩散训练和推理过程中是冻结（freeze）的。

2. 扩散模型 (U-Net)：
- 核心创新：扩散过程（加噪/去噪）不是在原始图像 x 上进行，而是在VAE编码后的潜在空间 z 上进行的。
- U-Net 的结构也进行了调整，以适应在潜在空间 z 上操作。

3.条件机制 (Conditioning Mechanism)：
  - 通常使用 CLIP 文本编码器，将输入文本提示词（Prompt）编码成向量表示。
  - 通过 Cross-Attention（交叉注意力） 机制注入到 U-Net 的中间层，实现精准的文本控制生成。

损失函数：
L_SD = E[|| ε - ε_θ(z_t, t, c) ||^2 ]
整个损失函数只做一件事：衡量预测噪声与真实噪声的差距。

**工作流程：**
总结流程：
**随机噪声（潜空间） → U-Net多步去噪（受文本引导） → 干净的潜表示 → VAE解码器 → 最终高清图片**

_训练：_
1. 图像 x 通过编码器 E 得到潜在表示 z = E(x)。
2. 对 z 进行常规的DDPM扩散过程（加噪）。
3. U-Net 学习在潜在空间中去噪，并且通过交叉注意力接受文本条件。

_推理（生成）_：
1. 在潜在空间中随机采样一个噪声 z_T。
2. 用U-Net和DDIM等采样器，在文本条件的引导下，逐步去噪得到 z_0。
3. 将去噪后的潜在表示 z_0 送入VAE解码器 D，得到高清图像 x = D(z_0)。

为什么更高效？
1. 计算复杂度大幅降低：
  - 计算复杂度与数据维度呈指数关系。在 64x64x4 的潜在空间中进行扩散，相比在 512x512x3 的像素空间中进行，计算量和内存占用减少了数十倍。
  - 这是性能提升的最主要原因。
2. 语义集中：
  - 潜在空间 z 是原始图像经过VAE编码压缩后的“精华”，它过滤掉了图像中的高频细节（纹理、噪声）等难以学习又耗费算力的信息，保留了高级的语义和概念信息。
  - 在这个更抽象、更紧凑的空间里进行扩散，模型可以更专注于语义内容的学习和生成，效率更高，效果也更好。
3. 更适合与其它模态对齐：
文本、深度图等控制条件本身也是抽象表示。在潜在空间中进行融合（通过Cross-Attention）比在像素空间更自然、更高效。

结论：Stable Diffusion (LDM) 通过将扩散过程从像素空间迁移到计算成本低得多的潜在空间，在不显著牺牲质量的前提下，极大地降低了计算需求。
SD与VAE区别：
一句话：SD 的 VAE 只是“压缩-解压”工具，图像生成质量与多样性由潜空间扩散模型负责，而非传统 VAE 的采样随机性 

 Stable Diffusion 1.5 → SD 2 → SD 3 三代官方基础模型的「横向差异速览表」，1.5/2 用 UNet+CLIP，SD3 把 UNet 换成 MM-DiT，文本三编码器。
<details><summary>Details</summary>
<p>

| 维度      | SD 1.5        | SD 2.0 / 2.1      | SD 3 / 3.5                     |
| ------- | ------------- | ----------------- | ------------------------------ |
| 发布时间    | 2022-10       | 2022-11 / 2023-01 | 2024-06                        |
| 参数规模    | ≈ 0.98 B      | ≈ 0.98 B          | 2 B–8 B（多档）                    |
| 文本编码器   | CLIP ViT-L/14 | OpenCLIP-ViT/H    | 全新 3×Transformer（T5 + 两个 CLIP） |
| 最大原生分辨率 | 512×512       | 768×768           | 1024×1024–2048×2048            |
| 显存门槛    | 4 GB          | 6 GB              | 8 GB+（官方建议 12 GB）              |
| 训练数据    | LAION-2B      | LAION-5B（NSFW 过滤） | LAION-5B + 内部高美学子集             |
| 负面提示依赖  | 中             | 高（必须）             | 低（理解力↑）                        |
| 文字渲染能力  | 差             | 差                 | 可嵌入短词                          |
| 社区生态    | 最丰富（Lora/插件）  | 中等                | 逐步完善中                          |


</p>
</details> 

# SD生成模型对应lora微调训练
Stable Diffusion（简称SD）是一种基于扩散过程的图像生成模型，应用于文生图场景，能够帮助我们生成图像。
本文档主要介绍如何利用训练框架PyTorch_npu+华为自研Ascend Snt9B硬件，完成SDXL模型的LoRA微调训练。

# 常见问题
问题：执行训练脚本报SSL证书校验错误。
图1 SSL证书校验错误
解决方法：
在入口py文件中加入os.environ['CURL_CA_BUNDLE'] = ''  ，重新执行即可。
训练入口py文件为 diffusers0.21.0/examples/text_to_image/train_text_to_image_lora_sdxl.py。

# Opensora模型
实际项目：opensora基于modelarts 适配npu
「先压后去噪」
VAE 把视频压成隐码 → DiT 在隐码里玩 Diffusion → 推理用最终权重解码回视频
「OpenSora = 时空 VAE + STDiT + T5；WebVid-2M 学；VAE loss ↘ 0.8e-3，DiT loss ↘ 0.05 停；输出 4-16 s 720p 视频。」
opensora模型与SD模型架构比较

| 模块    | SD 1.5/2      | SD 3                 | OpenSora                      |
| ----- | ------------- | -------------------- | ----------------------------- |
| 主干去噪  | UNet          | MMDiT（双 Transformer） | STDiT（时空 DiT）                 |
| 文本编码  | CLIP/OpenCLIP | CLIP-L/G + T5-XXL    | CLIP（PixArt 权重）               |
| 时空建模  | 无（单帧）         | 无（单帧）                | **时间注意力层**插在空间 DiT 块之间        |
| 隐空间压缩 | 8×8 VAE（仅空间）  | 8×8 VAE（空间）          | **8×8×4** 视频 VAE（空间 8×，时间 4×） |
| 训练策略  | 静态图 + 随机噪声    | 静态图 + 随机噪声           | 视频 + 时间步采样 + Rectified-Flow   |

**全流程**如下表格
<details><summary>Details</summary>
<p>

| 环节         | 输入               | 输出                       | 权重作用                                                   | 数据集                                       | 收敛/loss 含义                                     |
| ---------- | ---------------- | ------------------------ | ------------------------------------------------------ | ----------------------------------------- | ---------------------------------------------- |
| **VAE 训练** | WebVid-2M 原始视频帧  | 时空隐码 `z` (B, C, T, H, W) | OpenSora-VAE-v1.2 **初始化**<br>PixArt-SDXL-VAE 提供空间编码器结构 | WebVid-2M 视频帧<br>→ webvid\_meta\_data.csv | **MSE + KL** 阶段递减<br>stage3 loss ≈ 0.8×10⁻³ 停止 |
| **DiT 训练** | 隐码 `z` + T5 文本嵌入 | 去噪后隐码 `ẑ`                | STDiT-v3 **初始化权重**<br>T5-XXL 提供文本编码                    | WebVid-2M 隐码 + 文本                         | **MSE (ε-预测)**<br>stage3 loss ≈ 0.05 停止        |
| **推理**     | 文本 prompt + 噪声   | 4-16 s 720p/1080p 视频     | **三阶段最终权重**<br>VAE-decoder 把隐码变像素                      | 无（纯生成）                                    | 无 loss，人工/CLIP 打分                              |

1. 权重清单 & 作用
| 文件 | 放置位置 | 用途 |
|---|---|---|
| OpenSora-VAE-v1.2 | weights/ | 时空压缩/解压初始化 |
| PixArt-SDXL-VAE | weights/ | 空间编码器结构参考 |
| STDiT-v3 | weights/ | DiT 主干初始化 |
| T5-XXL | weights/ | 文本编码器（冻结） |
| VGG16 | torch cache | perceptual loss 计算（仅 VAE 阶段）|
2. 数据集 = 学什么
WebVid-2M（约 2.3 M 段 5-30 s 视频）
→ 处理后 webvid_meta_data.csv 含 id, text, 帧路径
→ 实际只取 fmin ≥ 1（≥ 1 帧）且分辨率 ≥ 360 p 的子集
3. 收敛标准 & Loss
VAE 三阶段：
loss = MSE(像素重建) + β·KL(隐码)
日志 loss.txt 每步打印；stage3 < 0.0008 手动停
DiT 三阶段：
loss = MSE(噪声预测 ε)
stage3 < 0.05 且验证集 CLIP ≥ 0.33 视为收敛

</p>
</details> 

# 多模态模型
## qewn2.5 VS qwen3-vl
Qwen2.5-VL 是“强基线”——Dense+128 K+事件级视频；
Qwen3-VL 是“全栈升级”——MoE/DeepStack/Interleaved-MRoPE/256 K→1 M/Thinking 全套上新，长文档、小时级视频、像素级定位、逻辑推理全面跃迁

| 维度     | Qwen2.5-VL      | Qwen3-VL                          |
| ------ | --------------- | --------------------------------- |
| 底座 LLM | Qwen2.5 Dense   | Qwen3 Dense / MoE                 |
| 视觉编码器  | ViT-bigG 14×14  | 更深 ViT 16×16，**DeepStack** 多层融合   |
| 位置编码   | M-RoPE（t, h, w） | **Interleaved-MRoPE**（t/h/w 频率交错） |
| 上下文    | 128 K           | **原生 256 K，可扩 1 M**               |
| 推理分支   | ❌               | ✅ Thinking 版（<think>…</think>）    |
| 最大参数   | 72 B Dense      | 235 B MoE（激活 22 B）                |
| OCR 语种 | 约 20 种          | **32 种** + 倾斜/模糊鲁棒                |
| 视频定位   | 事件片段级           | **秒级时间戳**对齐                       |

二、原理差异速览
DeepStack 融合（Qwen3-VL 独有）
不再只用 ViT 最后一层，而是把 第 8、16、24 层特征同时抽出，经 PatchMerger 对齐后分别注入 LLM 的 1、2、3 层，形成“视觉早融合”，提升细粒度与空间推理 。
Interleaved-MRoPE
把时间、高度、宽度三个维度的旋转频率交错混合，使模型对长视频时序关系更鲁棒，且支持可变帧率输入 。
Text-Timestamp Alignment
视频帧与"<0.8 s>"这类显式时间戳文本绑定，LLM 可直接用自然语言做秒级索引，而 2.5-VL 仅能用粗粒度事件标签 。
Thinking 分支
同 Qwen3 LLM，在 ViL 阶段保留推理头，输出 <think>…</think> 中间思考，再给出最终答案，视觉问答 + 逻辑推理显著上升

输入→输出完整流程（以 Qwen3-VL 为例）
1. 输入预处理
图像：动态分辨率 + 16×16 patch → ViT 多层特征（8/16/24）
视频：每秒采样 1 帧，每帧打时间戳文本 → 与图像同方式编码
文本：正常 tokenizer，但位置 id 由 Interleaved-MRoPE 统一计算 (t, h, w) 。
2. 视觉早融合（DeepStack）
多层 ViT 特征 → PatchMerger → 视觉 token 序列
视觉 token 与文本 token 交错拼接，形成统一序列。
3. LLM 前向
底座：Qwen3-MoE（30 B-A3B 例）
每 token 激活 3 B 参数，Thinking 版会先输出推理链 <think>…</think>，再输出最终文本 。
4. 后处理
支持 JSON 带坐标（BBox/Points）或 秒级时间戳（视频）
可返回 <think> 内容供上层折叠显示。

# ViT模型
ViT = Patch Embedding + Transformer Encoder + cls_token（图像分类），就是「把图切成 patches → 当成一串 token → 做自注意力」，全局上下文一步到位，无需 CNN。

具体处理过程如下：
<details><summary>Details</summary>
<p>

1. 输入处理
将 H×W×3 图片切成 固定大小 patches（如 16×16），得到 N = HW/16² 个 patch。
每个 patch 展平 → 线性投影到 D 维向量（= token）。
追加 可学习分类 token（cls_token）+ 位置编码（1-D 或 2-D sin/cos），形成长度 N+1 的序列。
2. 模型结构
纯 Transformer Encoder（L 层）：
– 多头自注意力（MSA）
– 前馈网络（MLP）
– LayerNorm + 残差
无卷积、无池化，全局感受野一层到位。
3. 分类头
取 cls_token 的最终表示 → 线性层 → softmax → 类别概率。
4. 训练 & 推理
与 BERT 一样用 大规模监督预训练（ImageNet-21k → ILSVRC）。
推理时一次前向即得全局特征，计算量 ∝ patch 数，适合 GPU 并行。

</p>
</details> 


# wav2lip
Wav2Lip是一种基于对抗生成网络的由语音驱动的人脸说话视频生成模型。主要应用于数字人场景。不仅可以基于静态图像来输出与目标语音匹配的唇形同步视频，还可以直接将动态的视频进行唇形转换，输出与输入语音匹配的视频，俗称“对口型”。该技术的主要作用就是在将音频与图片、音频与视频进行合成时，口型能够自然。
Wav2Lip模型的**输入为一段视频和一段语音**，**输出为一段唇音同步的视频**。
Wav2Lip的网络模型总体上分成三块：**生成器、判别器和一个预训练好的唇音同步判别模型Pre-trained Lip-sync Expert。**
生成器是基于encoder-decoder的网络结构，分别利用2个encoder（speech encoder和identity encoder）去对输入的语音和视频人脸进行编码，并将二者的编码结果进行拼接，送入到face decoder中进行解码得到输出的视频帧。
判别器Visual Quality Discriminator对生成结果的质量进行规范，提高生成视频的清晰度。
引入预训练的唇音同步判别模型Pre-trained Lip-sync Expert，作为衡量生成结果的唇音同步性的额外损失，可以更好地保证生成结果的唇音同步性。

Wav2Lip 用「梅尔频谱 + 参考脸」→ GAN 生成「唇部同步帧」，双判别器保障「对齐 + 清晰」，输入任意音频即可零样本产出嘴型完美对齐的新视频。
**梅尔频谱**= “音频的指纹图”
先把任意音频转成 80 条频率带的时间-能量图，横轴时间，纵轴频率，颜色=能量大小。
好处：扔掉相位、采样率等冗余信息，只保留“人嘴该动”的节奏与音高线索，图小、算得快。

参考脸 = “一张静态证件照”
从原视频里随机抽一帧（或给单张照片），裁出 96×96 的嘴周区域，告诉网络“这张脸长这样，唇色、齿形、皮肤纹理别给我改”。

唇部同步帧 = “GAN 新生成的嘴型图”
生成器把梅尔谱的节奏 + 参考脸的外观拼起来，输出一帧帧 96×96 的“嘴”，再贴回原图，就得到“声音到哪儿，嘴张到哪儿”的新视频。
双判别器负责 A. 嘴型对齐（同步判别器）+ B. 画面清晰（视觉判别器），让结果既跟拍又高清。



**原理总览**
生成对抗网络（GAN） 架构：
– 生成器：编码「语音特征」+「参考人脸」，解码为「唇部同步帧」；
– 双判别器：
① 唇同步判别器（基于改进 SyncNet）→ 判断音-唇是否同步；
② 视觉质量判别器 → 判断画面真实度与清晰度。
损失函数：同步损失 + 对抗损失 + 重建损失联合训练，迫使生成器产出「既对齐又逼真」的口播帧
**网络模块拆解**
Identity Encoder（残差卷积）：提取参考帧的「身份+姿态」特征。
Speech Encoder（2D CNN）：把梅尔频谱编码为「语音动画」特征。
Face Decoder：拼接两类特征 → 上采样 → 生成 96×96 唇部帧。
Lip-sync Expert（预训练 SyncNet）：额外监督，确保音-唇时间对齐 。
**训练 & 推理流程**
人脸检测 → 裁剪嘴部 96×96 区域。
音频转梅尔频谱，滑窗 16 帧为一段。
生成器逐段输出唇部帧；判别器实时反馈同步/真实度。
推理时无需 3D 参数，任何语音 + 任意人脸即可零样本合成

<details><summary>Details</summary>
<p>

Wav2Lip 训练全流程速记（一页 A4 能看完）
总体目标
「让一段音频驱动一张脸，嘴型完美对上节拍，画面还不糊」——靠两套预训练权重（人脸检测预训练模型+专家唇形同步鉴别器） + 三步训练完成。LRS2数据集
预训练权重（必下载）
| 权重 | 下载后改名/路径 | 作用 |
|---|---|---|
| 人脸检测 S3FD | face_detection/detection/sfd/s3fd.pth | 裁出 96×96 嘴部 ROI |
| 专家唇同步 SyncNet | checkpoints/lipsync_expert.pth | 训练时当“节拍老师”，逼生成器对齐音频 |
数据流水线（一条命令）
bash
复制
使用的数据集是LRS2（Lip Reading Sentences 2）是牛津大学与 BBC 合作推出的英文唇语同步公开数据集，也是目前训练Wav2Lip、AV-ASR 等“对口型”模型最常用的基准数据。
**原始视频 → 帧 + 16 kHz 音频**
python preprocess.py --data_root LRS2_partly --preprocessed_root lrs2_preprocessed/
产出结构
复制
lrs2_preprocessed/main/00001/xxx.jpg  # 每帧嘴图
lrs2_preprocessed/main/00001/audio.wav （16 kHz 单声道音频）
训练路线图
① （可选）重训专家鉴别器
bash
复制
python color_syncnet_train.py --data_root lrs2_preprocessed/main/ \
                              --checkpoint_dir savedmodel/syncnet_model/
目标 loss ≈ 0.25（官方已给权重可跳过）
② 主模型训练
bash
复制
python wav2lip_train.py --data_root lrs2_preprocessed/main/ \
                        --checkpoint_dir savedmodel \
                        --syncnet_checkpoint_path checkpoints/lipsync_expert.pth
默认 每 3000 step 存一次 ckpt；首次 700 step 做评估，耐心等。
关键超参速查（hparams.py）
| 参数 | 默认值 | 面试一句话 |
|---|---|---|
| batch_size | 80 | 显存够就拉满，嘴型更稳 |
| syncnet_wt | 0.03 | 同步损失权重，↑ 嘴型对齐 ↑ 但可能模糊 |
| eval_interval | 3000 | 隔多少 step 测一次同步/重建 loss |
| img_size | 96 | 只改嘴部 96×96，省算力 |
| fps | 25 | 与数据集帧率一致，否则音画错位 |
收敛指标（背下来）
专家 SyncNet loss ≈ 0.25
Wav2Lip sync loss ≈ 0.2
低于这两个值，推理基本“对得上、不糊脸”。
面试金句
“两步权重 + 三步训练，SyncNet 当节拍老师，GAN 保清晰，只改 96×96 嘴区，实现零样本唇同步。”

</p>
</details> 

# Qwen2.5-VL
Qwen2.5-VL 的视觉编码器 = 任意分辨率 ViT + 2D-RoPE + Window Attention + 轻量 Neck，先单独对齐图文特征，再与 LLM 端到端混训，推理时把图像 patch 当「超长文本 token」直接拼进输入，无需额外 CNN 或 Q-Former，这就是「添加」的全部过程。

一、视觉编码器结构（Qwen2.5-VL Vision Encoder）
| 模块                 | 设计要点                        | 作用                     |
| ------------------ | --------------------------- | ---------------------- |
| Patch Embedding    | 16×16 卷积，stride=16          | 把任意长宽图像打成 1D patch 序列  |
| 2D-RoPE            | 将旋转位置编码同时施加在 h、w 两个维度       | 保持 patch 间的二维空间相对位置    |
| Window Attention   | 局部 7×7 window + 移位窗口        | 降低 O(n²) 计算，显存随分辨率线性增长 |
| Global Query Token | 每 4 层插入 1 个可学习 global query | 远程 patch 也能直接交互，防止信息孤岛 |
| Neck（MLP）          | 2 层 MLP 压缩通道 → LLM 词表维度     | 把视觉特征映射到文本 token 空间    |

二、训练流程（3 阶段）
1. Vision-Language Alignment（冻结 LLM）
600 M 图文对 + 400 M OCR 对，只训 Neck 层 + Vision Encoder，让 patch 特征对齐词向量空间。
2. Vision-Language Instruction Tuning（LLM 解冻）
200 M 高质量指令数据（图像描述、图表、定位、视频），全参数训练，学习「看-说」指令。
3. Mixed Instruction Tuning（文本不掉）
再把纯文本 SFT 子集（约 20 M 样本）以 1:1 比例混训，确保文本基准不降级。



