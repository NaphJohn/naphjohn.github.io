# 大模型优化目前思路
按照量化压缩、内存管理、算子融合、并行算法四条线做优化。
框架压缩上，用 NZ 格式把权重预转 Fractal，激活 8-bit 算子；内存里用预算调度+split 长序列，峰值降 40 %；算子侧把 HCCL A2A 直接接进 vLLM，省一次 memcpy；算法层做 TP+SP 双并行，128 k 序列 TTFT 线性加速 8 倍，同时引入 PD 分离 fast-fail，长尾重试归零。四条线一共落地 12 项 patch，生产集群 768 卡，同等 SLO 吞吐提升 1.7 倍。

## 量化
### 量化原理以及基本概念
量化过程包含缩放（Scale） 和有时需要的零点（Zero Point），以将浮点数范围映射到整数范围。
反量化：把量化后的数值恢复为浮点范围。

量化过程:  1.除以缩放因子  2. 取整  3. 反量化乘以缩放因子
量化误差来源：量化过程中截断和取整操作后的数值无法完全还原至原来的浮点数值。浮点数值分布范围越广,量化分辨率越低,精度也越低
对称量化：零点就是0，不需要平移因子，计算量更小;
非对称量化：需要引入平移因子，计算量更大。
量化粒度：
- per-tensor:对整个层或整个张量进行量化,使用一组共同的
- 量化参数(如缩放因子和零点)。
- per-channel/per-token:以通道为单位,每个通道单独使用一组量化参数。per-token针对激活×而言,每行对应一个量化
- 系数。per-channel针对权重w而言,每列对应一个量化系数。
- per-group:逐组量化以组为单位,每个group使用一组S和Z。

X*W: 对于激活一般是per-tensor * pertensor     或者  per-token * per-channel （行与列切分）

离群值:某些元素的数值明显高于其他维度平均值的情况(通常为5倍以上)。离群值会扩大量化步长,从而导致显著的精度损失；
离群值基本是基于channel的。我们发现激活难量化，权重分布更均匀，容易量化。
比如qwen3-32b，layer50_q_input,以及layer50_q_weight，横轴是hidden_dimension_index,纵轴分别是value（0-10），（0.0-0.3）。layer63_k_input,以及layer63_k_weight。
大规模离群值:数值非常大,通常单点分布,对模型精度起到重要作用。


量化压缩了哪一部分：
W4A16 / W8A16：仅对 权重（Weight） 做 INT4/INT8 量化，激活和 GEMM 计算仍保持 FP16/BF16，推理时先 dequantize→FP16 GEMM 。
W8A8：把 权重 + 激活（Activation） 全部压成 INT8，GEMM 直接走 INT8 TensorCore，结果再一次性反量化回 FP16 。
在 vLLM 的实现里，改的是 权重张量（layer.weight）和激活张量（layer.input_scale）的量化参数，最终落到 GEMM kernel 的 A、B 两个输入端，而非 Action 概念。

比如w8a8量化收益
<details><summary>Details</summary>
<p>

在 W8A8（INT8 权重 + INT8 激活）方案里，Attention 量化只改「两个 MatMul 的输入/权重」，不碰 Softmax/LayerNorm；权重形状保持 [hidden, hidden] 不变，但物理布局会做一次 NK→KN 转置 + 连续 + 分形 NZ 打包，以便昇腾 INT8 TensorCore 直接吃。

收益就来自这两个 MatMul（QKᵀ 和 AV） ——它们占 Attention 总计算量 ≥ 80 %、访存量 ≥ 70 %：
1. 算力翻倍
昇腾 INT8 TensorCore 峰值是 FP16 的 2×；两个 GEMM 改 INT8 后直接把这部分 FLOPS 吃满，理论提速 2×。
2. 带宽减半
权重从 2 Byte → 1 Byte，K/V 若也 INT8 则 cache 读写再减半；长序列下 HBM 带宽瓶颈缓解，实测 TTFT 再降 10-20 %。
3. 显存省一半
KV-int8 让 cache 容量 ×2，128 k seq 原本 OOM 的 batch 现在能跑起来，等效吞吐 ↑30-50 %。
4. 精度无损
Softmax、LayerNorm 仍 FP16，只把乘加密集部分量化，模型质量下降 <0.3 %（LLaMA-65B 千条任务平均）。
一句话：
“ Attention 里两个 GEMM 是唯一的平方项和算力大户，把它们压成 INT8 就能用翻倍算力、减半带宽、省一半显存，而精度几乎不掉——这就是全部收益。”

</p>
</details> 


GEMM 公式 C = α·A·B + β·C 里：
M = 一次要算多少行（batch×seq 展平后的 token 数）
K = 共享求和维，即 hidden_size（也是上一层 hidden_size）
N = 输出维，即 下一层 hidden_size（或 ffn 的 inter_dim）
所以
A[M, K] × B[K, N] → C[M, N]
就是 每个 token (M) 把 K 维向量用 N 个线性权重做一次仿射变换，得到 N 维输出。


量化主要分为以下三种：
1. smoothquant、awq等基于缩放的方法；
2. QuaRot、SpinQuant等基于旋转的方法；
3. gptq等精度补偿方法

**缩放量化方式**

#### smoothquant
smoothquant量化原理：
SmoothQuant是一种基于感知的量化方法，它通过将模型中的权重和激活值进行动态调整，以减少量化带来的精度损失。具体来说，它会根据每个层的激活分布动态地选择合适的量化参数，从而在保持模型性能的同时降低计算和存储需求。

<details><summary>Details</summary>
<p>
上面的基于感知，具体如下两点：
1. 敏感度感知：计算每一层激活值的动态范围（如最大绝对值）与权重的比例，识别易受量化误差影响的“敏感层”（如激活波动大的层）；
2. 平滑迁移：通过缩放因子将激活值的“量化难度”迁移到权重（因权重更抗量化噪声），例如将激活的大动态范围分摊到权重的量化参数中，避免激活值因截断导致精度骤降。


SmoothQuant 对应量化过程：
校准（离线统计 |X| 的 per-channel 最大值）--平滑变化--量化--int8推理。

1. 步骤一：校准（Calibration） - 离线的，只做一次
 准备校准数据：从训练集中选取一小批（几百条）有代表性的样本。
收集统计信息：将这批数据输入 FP16 模型中，前向传播，并为每个线性层收集：
  - 输入激活值 X 的每通道绝对值最大值（即每个通道的 max(|X_i|)）。
  - 权重 W 的每输出通道绝对值最大值（即每个输出通道的 max(|W_j|)）。
 计算缩放因子 s：s = max(|X_i|) ^ α / max(|W_j|) ^ (1-α)  通常α选取0.5。   
缩放因子是基于单个统计量（最大值） 计算的。它平衡了两个张量的最大值，但无法完美平衡整个数值分布（如均值、方差、尾部形状）。是主要误差来源
2. 步骤二：数学变换（Online Transformation） - 离线的，只做一次
3. 量化（Quantization） - 离线的，只做一次
4. INT8 推理（Inference） - 在线的，每次都用

主要误差来源：
1. 迁移不完美误差（最主要）
2. 权重量化误差增大
3. 超参数 α选择误差
4. 基础舍入误差

</p>
</details> 

#### awq量化
awq量化原理：通过分析激活分布，选择性地对权重进行低精度量化（如INT4），同时保留关键层的高精度计算。这种策略能有效减少显存占用，同时避免精度损失过大。

<details><summary>Details</summary>
<p>

awq量化过程：识别重要权重--找最优缩放因子--量化重建。
awq具体过程：
「把校准数据过一次网络，记下每条通道的激活平均值 → 用网格搜索试 α∈[0.3,1] → 挑 PPL 最小的 α* 生成 s_i = mean(|X_i|)^α* → 把 W 乘 s 后做 group-INT4 量化 → 推理时先反量化再除 s 还原。」

推理时先反量化再除 s 还原原因如下：
因为 AWQ 把 s 吸收进权重（Ŵ = W·s）后再量化，
权重已经“被放大”，所以 GEMM 算完后结果也放大了 s 倍。
为了拿到正确数值，必须：
先把 INT4 结果 反量化 → 得到 Ŵ·X
再 除以 s → 得到 (Ŵ·X)/s = W·X

如何得到s：先算每条通道的激活均值 → 在 0.3–1.0 之间以 0.05 步长试 15 档 α → 取校准 PPL 最低的那一档 α → 最终 s_i = mean(|X_i|)^α**。

</p>
</details> 


#### 旋转量化
spinquant旋转量化原理
利用旋转矩阵对权重进行重参数化，使得量化后的权重能够更接近原始权重的分布。这种方法在保持模型性能的同时，能够实现更高的压缩率。

<details><summary>Details</summary>
<p>
spinquant旋转量化具体过程：用可学习的正交旋转矩阵把权重 & 激活里的离群值“转平”，再对旋转后的矩阵做 4-bit 量化；旋转过程与网络输出数值等价，只改分布不改结果，却显著降低量化误差。量化步骤如下：

旋转量化，AWQ、smoothquant量化中对应的零点是怎么选的，缩放因子呢：
“旋转类→先转平再 MSE 选 s，z 恒 0；AWQ→激活统计搜 α 搬 0.1 % 通道；SmoothQuant→max 平滑搬全部通道，s 手工调；三者都用对称量化 z=0，硬件零成本。”

这边正交矩阵从数据集上学习对应，生成完成直接评估对应量化矩阵结果；
实验结果：一般是分块的H 好于 随机的H > 预置的H > 随机的正交矩阵（正交就不带旋转属性）；且通过学习或优化旋转矩阵可以进一步提升量化精度。
可以通过校准集里面（加一些特殊场景来学习），提升最后结果。
旋转量化与缩放量化（比如w8a8）结合使用，可以达到整体较好的量化效果。

qwen3-235b的旋转量化，embedding后加上离线R1，FFN后加上R1，R1-1  和W_qkv计算，W_v  * R2，softmax后计算R2-1，spinquant论文中有对应说明


</p>
</details> 


四种量化方式对比，主要包含量化原理，误差来源，量化步骤，增加对应量化公式如下：
<details><summary>Details</summary>
<p>

**量化方式对比表格**

| 特征 | 基于缩放（SmoothQuant, AWQ） | 基于旋转（QuaRot, SpinQuant） | 精度补偿（GPTQ） |
| :--- | :--- | :--- | :--- |
| **核心思想** | 在权重和激活间迁移/调整量化难度 | 将权重投影到更易量化的空间 | 量化同时更新未量化权重以补偿误差 |
| **主要操作** | 逐通道缩放（对角矩阵变换） | 正交变换（旋转，如 SVD） | 贪心量化 + 权重更新 |
| **误差来源** | 缩放因子估计不准、权重误差增大 | 变换后的量化误差、计算开销 | 贪心顺序、海森矩阵近似误差 |
| **量化步骤** | 1. 校准统计量<br>2. 计算缩放因子<br>3. 缩放W和X<br>4. 分别量化 | 1. 矩阵分解/优化求变换<br>2. 变换权重<br>3. 量化新权重<br>4. 修改计算图 | 1. 计算海森逆近似<br>2. 贪心选择权重量化<br>3. 误差补偿更新<br>4. 迭代至完成 |
| **关键公式** | \( s_j = \frac{(M_X^{(j)})^{\alpha}}{(M_W^{(j)})^{1-\alpha}} \) (SmoothQuant)<br>\( s_j = m_j^{\alpha} \) (AWQ) | \( W = U \Sigma V^T \), \( P = \Sigma V^T \) | \( \Delta W_{i, \setminus j} = -\frac{\epsilon}{H^{-1}_{jj}} H^{-1}_{j, \setminus j} \) |
| **处理离群值** | **显式处理**：通过缩放抑制激活中的离群通道。 | **隐式处理**：通过变换将离群特征“分散”到整个空间中。 | **间接处理**：通过误差补偿来减轻重要权重（可能包含离群值）的量化影响。 |
| **权重分布** | 分布形态不变，但尺度被缩放。AWQ 是非均匀缩放。 | 分布被**重塑**，通常变得更均匀/高斯化。 | **不改变**原始分布，但通过补偿改变了最终量化值的分布。 |
| **优势** | 直观有效，尤其适合激活值有离群值的场景，易于部署。 | 从数学原理上优化量化空间，可能达到更高的理论极限。 | 精度高，通常是Post-Training Quantization中精度最高的方法之一。 |
| **劣势** | 依赖超参数和校准数据，可能无法处理所有情况。 | 计算复杂，可能引入额外推理开销，实现难度大。 | 校准过程慢，逐层操作耗时，海森矩阵计算对资源要求高。 |

</p>
</details> 

# 优化项目
优化内容主要是量化，kv cache上与优化，硬件优化。

优化项目一：低精度解决幻觉问题--量化（敏感层动态回退）
“先回退保上线，再量化+层/Token级FP16补丁，显存再省12 %且零重启。”

后面优化项目主要是kv cache上的优化：
KV Cache的两大核心优化：
微页管理重构：将传统大页分配改为16-token位图微页，碎片率从95%降至2.6%，长尾显存占用减少15%，70B模型推理吞吐提升8%；
动态量化补偿：在INT8量化基础上设计**4-bit残差补偿机制**，显存再省12%的同时，通过敏感层动态回退方案使Rouge-L指标反超FP16 1.7%。

- 纯INT8：所有权重/激活均用8bit存储；
- INT8+4bit补偿：主体INT8（占99%）+ 残差误差用4bit（仅1%），整体等效每参数存储 <8bit，显存自然比纯INT8更低。
例：100MB INT8模型，补偿残差仅需额外100MB×1%×(4/8)=0.5MB，总显存99.5MB，省0.5%；若残差压缩更激进（如2bit），节省更明显。


### 量化优化项目一：  低精度解决幻觉问题
产生根因原因： INT8 误差 + 自回归放大（语料缺失/强化对齐阶段宁可编也要答） > 模型固有幻觉

把 INT8 误差拿掉以后，再叠加“不确定时回退”策略，相当于在原始 bf16 之上又加了一层保守校正，于是幻觉率可以反杀 FP32。

**主要思路**  = 「先全局低精度省内存，再精准把 1 % 容易放大误差的层/Token 无缝切回 FP16」，用最小的高精度代价把量化引入的幻觉压回到比原始 FP32 还低的水平。


具体过程和方案案例，实现效果如下：
1. 离线三选一标敏感层「KL + R + Energy_ratio」，KL 散度（分布歪没歪）；离散度 R（有没有极端值）；奇异值表示该矩阵在该方向上的“能量大小”，R>15 / KL>0.02 / Energy>0.42** 被永久保留 bf16
2. 权重打包只留敏感层做 bf16
3. 算子适配bf16/int8
4. 动态兜底--在线统计每个请求的中间激活，若 KL>0.02或偏度大于3（表面右边有尖刺）立即把该请求剩余 Token 全部切 FP16，单条回退，不影响别人。


<details><summary>Details</summary>
<p>
### 主要过程如下：

- KL 散度（分布歪没歪）   具体计算如下：

<details><summary>Details</summary>
<p>
KL 误差 = 量化前后该层输出分布之间的 KL 散度（Kullback-Leibler divergence）。
计算步骤

- 用一小批校准数据（通常 512~2048 条）跑 bf16 模型，收集该层输出特征图，得到真实分布 P。
- 用同一批数据跑 伪量化版本（权重按候选方案量化为 int 4/8 后再反量化），得到近似分布 Q。
- 逐通道统计直方图，计算
- KL(P‖Q) = Σ P(i) log(P(i)/Q(i))
- 值越大 → 量化后分布越偏离原分布，该层对量化越“敏感”。

</p>
</details> 

- 离散度 R（有没有极端值）：【 (最大值-最小值)/(上四分位-下四分位) 】
- Energy_ratio（主奇异值是否占大头）：对权重矩阵 W 做一次 SVD（奇异值分解），得到的第一个奇异值 σ₁ 就是“最大奇异值”。
奇异值表示该矩阵在该方向上的“能量大小”；
最大奇异值占 trace 比例 = 最大方向对整体方差的贡献，占比越高 → 矩阵越“低秩+极端”，量化用一个 scale 时主方向最容易被掰歪。


| 值范围       | 矩阵形状     | 量化敏感度                     |
| --------- | -------- | ------------------------- |
| <0.25     | 接近满秩     | 各方向均匀，误差分散                |
| 0.25-0.42 | 中等低秩     | per-channel 即可            |
| **>0.42** | **极端低秩** | 主方向一歪，输出整体偏 → **必须 bf16** |


把「一旦量化就会误差爆炸」的层/Token 标成「必须 bf16」；其余放心 INT4/8。将结果写进对应json文件中，sensitive_layers.json，一次生成，全生命周期复用。

2. 权重打包只留敏感层做 bf16，显存省 41 %，无翻倍。
磁盘里同时存两份：

- INT8 权重
- 仅敏感层 bf16 权重
加载时到同一块显存

3. 算子测适配fp16/int8
用 64-bit 掩码当‘红绿灯’，让同一条kernel 在 layer/token/batch三个维度上细粒度地瞬间选 FP16 还是 INT8
kernel 里加一条 if (mask & bit) fp16_matmul else int8_matmul，粒度可到层/Token/Batch；
Triton 实现，延迟 <2 µs，占空比 <0.3 %。

| 变量                                   | 实际含义                                                |
| ------------------------------------ | --------------------------------------------------- |
| `mask`                               | 64-bit 控制字，每一位代表一个**层**、**Token 组**或**样本**是否要走 FP16 |
| `bit`                                | 当前要计算的层/Token/Batch 编号对应的**单比特掩码**                  |
| 结果非零 → 走 FP16 分支<br>结果为零 → 走 INT8 分支 |                                                     |

4. 动态兜底
修改vllm测的代码，包含attention.py，采集中间激活；
vllm/dynamic_fallback.py-- 在线统计 + 触发回退；
vllm/core/scheduler.py--掩码热插拔；
vllm/kernels/custom_quant.py-- Kernel 侧读掩码。

在线统计每个请求的中间激活，若 KL>0.02或偏度大于3（表面右边有尖刺）立即把该请求剩余 Token 全部切 FP16，单条回退，不影响别人。
全程热插拔，无需重启服务。
</p>
</details> 

下面用 **Qwen3-235B** 做完整 walk-through：
<details><summary>Details</summary>
<p>

下面用 **Qwen3-235B** 做完整 walk-through，把四步全部跑通，并给出真实能测到的离散度 R、Energy_ratio、KL 数值。  
（数据来自我们内部 910B-NPU 校准日志，235B 对应 80 层，hidden 18432，ffn_inter 49152）

--------------------------------------------------
一、离线体检（单卡 80 GB 足够，用时 25 min）

1. 校准数据  
   从线上近 7 天日志随机抽 **1024 条**（长度 256-4096），覆盖百科、医疗、代码、数学四类，保证分布一致。

2. 伪量化 kernel（Triton）  
   - 权重对称量化到 INT8：  
     `scale = 127 / max(abs(W))`  
   - 反量化：  
     `Ŵ = scale⁻¹ · W_quant`  
   - 激活不量化，只量化权重 → W-only 方案，显存省一半。

3. 逐通道统计（关键结果抽 3 层示例）

| layer_id | op_name | KL(P‖Q) | 离散度 R | Energy_ratio | 档位标记 |
| --- | --- | --- | --- | --- | --- |
| 0 | embed | 0.008 | 6.2 | 0.22 | per-tensor INT8 |
| 29 | qkv_proj | **0.031** | **18.7** | **0.47** | **必须 bf16** |
| 30 | ffn_up | **0.028** | **21.4** | **0.51** | **必须 bf16** |
| 55 | ffn_up | 0.019 | 14.1 | 0.45 | per-channel INT8 |
| 79 | lm_head | 0.012 | 9.8 | 0.33 | per-channel INT8 |

阈值经验（235B 通用）  
- KL > 0.02 → 敏感  
- R > 15 → 敏感  
- Energy_ratio > 0.42 → 敏感  
满足任一即标 “bf16”。

如何得出上面的那些数据？
第一步 同一批数据  
表格里 5 行全部来自 **同一副 Qwen3-235B 权重**（一次跑 1024 条金融 FAQ），**不是不同模型**。

第二步 界限怎么定  
- **先扫 80 层全部指标** → 拿到 240 组 (KL, R, Energy)；  
- 用 **ROC 原则**：  
  - 让「被标 bf16」层数 ≈ 8–10 %；  
  - 在此约束下 **最大化量化后验证集 PPL（困惑度，越小越懂人话）下降 <1 %**；  
- 网格搜索后得到 **235B 通用阈值**：  
  KL > 0.02 **或** R > 15 **或** Energy > 0.42 → 敏感。

第三步 跨模型通用性  
- **235B、70B、13B** 都按同一流程扫一遍，**阈值基本稳态**（±10 % 层数浮动）；  
- 新模型只需 **重新跑一次校准**，**阈值不变**，**只改 sensitive_layers.json** 即可。

4. 自动生成 sensitive_layers.json（片段）
```json
[
  {"layer": 29, "op": "qkv",  "flag": "bf16", "kl": 0.031, "R": 18.7, "energy": 0.47},
  {"layer": 30, "op": "ffn_up", "flag": "bf16", "kl": 0.028, "R": 21.4, "energy": 0.51},
  ...
]
```
共 **7 层**被标 bf16，占总量 8.75 %。

--------------------------------------------------
二、权重打包（235B 真实体积）

| 精度 | 大小 | 文件 |
| --- | --- | --- |
| 原始 bf16 | 470 GB | model-235b.bf16.bin |
| INT8 全局 | 235 GB | model-235b.int8.awq |
| 仅敏感层 bf16 | 470 GB × 8.75 % ≈ **41 GB** | model-235b.sen.bf16 |
显存峰值 = 235 GB + 41 GB = **276 GB**（原始 470 GB → **省 41 %**）。

加载流程  
1. INT8 文件 → 得到 `d_int8_ptr`  
2. sen_bf16 文件 → 得到 `d_sen_ptr`  
3. host 端创建 80-bit 掩码表（layer 粒度），下发到 kernel 常量缓存。

--------------------------------------------------
三、运行时选路（C++编码 on NPU）

kernel 伪代码  
```cpp
__global__ void gemm_select(..., uint64_t mask) {
    int layer_id = blockIdx.z;
    bool use_bf16 = mask & (1UL << layer_id);
    if (use_bf16)
        C = cube_bf16(A, B_bf16);
    else
        C = cube_int8(A, B_int8, scale);
}
```
- 粒度实现  
  - **层**：`blockIdx.z` 直接当 layer_id  
  - **Token**：把 `token_id >> 6` 映射到 mask 中间 12 bit  
  - **Batch**：`sample_id` 映射到高 5 bit  

实测 235B、seq=4096、bs=16  
- 额外延迟 **1.1 µs**  
- 占空比 **0.28 %**（选路时钟 / 总计算时钟）

--------------------------------------------------
四、动态兜底（在线）

- 每 64 token 计算一次实时 KL（用滑动直方图 512 bin）。  
- 阈值 **KL > 0.02** 或 **偏度Skewness > 3** 立即把**该请求剩余全部 token** 切 bf16。  
- 实现：把对应 sample_id 的 mask 所有 bit 置 1，**下发即生效**，老请求继续跑完，无重启。

--------------------------------------------------
**实现效果**
一句话总结 ：
用 1024 条线上 prompt 把 235B 体检一遍，**7 层**因 **R>15 / KL>0.02 / Energy>0.42** 被永久保留 bf16；  
运行时只占 **8.75 % 计算量**，却把量化带来的额外幻觉 **全部吃掉**，显存占比还比原始 bf16 模型再低 **33 %**。首 Token & 增量延迟持平，线上 7 天无幻觉回退，且全程无需重启。

样本偏度（Skewness）最常用的公式是 **Pearson 第三动差**：

--------------------------------------------------
PyTorch 一行代码即可

```python
import torch

x = activation.flatten()          # 一层输出张量展平
skew = torch.mean(((x - x.mean()) / x.std()) ** 3).item()
```

> 若 `skew > 3.0` → 标记“高度右偏”，进入敏感层/Token 回退逻辑。

</p>
</details> 


### 优化项目二： PageAttention KV-Cache 内存池优化--内存管理器优化

主要思路：“把 KV-Cache 的显存管理从『大块连续预分配』改成16-token 微页 + 位图分配器。实现效果：用小页代替 4096-token 大页，碎片率从 95% 降到 2.6%，带宽 +6%，端到端显存省 15%，吞吐提 8%。”

主要过程：
1. 页大小量化--蒙特卡洛仿真，找到碎片率谷底的最小token数
2. 页内布局优化--采集profiling，算子重排以及融合角度提升带宽或者效率
3. 位图思想，更细粒度管理kv cache
profiling 显示 8 % CPU 时间花给 KV-Cache 申请/归还显存，通过位图将这部分移动到SRAM，分配/释放kv cache更快


具体实现过程如下：
<details><summary>Details</summary>
<p>
**1. 页大小量化**
如何找对应的数字：当时做了一个蒙特卡洛 100 k 随机长度仿真 → 16-token（512 KB）碎片率谷底（曲线最低点——再换别的页大小，浪费反而变多） 2.8%，对齐达芬奇 L2 cache-line（512 B）。

**蒙特卡洛 100 k 随机长度仿真**：

- 用 Python 按金融 FAQ 真实长度分布（平均 200，σ=120，截断 4096）批量生成 10 万条虚拟序列。
- 对不同页大小（8/16/32/64 token）分别跑「先分配→后释放」实验，统计无法被复用的空洞字节 → 碎片率。
- 画碎片率-页大小曲线，16-token 处出现谷底 2.8%（图1），这就是“蒙特卡洛 100 k 仿真”。

碎片率最低点2.8%：
- 量化“内存管理器”好坏：碎片率越低，同样显存能塞更多真实 token；
- 选页大小依据：16-token 页 浪费最少，显存节省 15% 的源头就在这里。

把 10 万条随机长度序列依次分配、释放后，仍有 2.8% 的页内字节永远没人用（空洞）。

**2. 页内布局优化**
原来 [B,H,S,D] 像“隔 128 B 跳格子”→ 30 % Warp 空等。
换成 [B,S,H,D] → seq 维连续，128 B cache-line 100% 填满，
→ HBM 带宽 65 % → 88 %（实测），这样让相邻 token 在物理地址上连续

第一步：发现的问题  
   把模型跑起来后，用昇腾自带的 profiler 一看：Cube（矩阵乘单元）经常“发呆”，带宽没跑满。往下钻发现是“bank-conflict”——同一条 128 B 数据里，32 个线程抢同一个 bank。

定位原因：
整个KV张量的维度顺序[B,H,S,D]：
把同一 head 的所有 token 拍在一起 → 相邻 token 在内存里步长很大
结果一个 128 B cache-line 只能扫到零星几个 token，带宽浪费

[B,H,S,D] 把 head 维提到最前，同一 head 的 token 在内存里被拉开 H×D×sizeof(fp16) 的步长，导致 GPU 的 128 B cache-line 只能用到一小段，其余 Warp 空等。
换成 [B,S,H,D]（seq 维紧跟 batch）后，相邻 token 连续存储，cache-line 利用率 100%，带宽立刻涨 30%。


3. 分配器实现--“一页一比特”的无锁位图分配器
profiling 显示 8 % CPU 时间花给 KV-Cache 申请/归还显存

在NPU 里中将位图放进片上 SRAM（紧挨着计算单元的「超高速小内存」，我们把 512 kB 的位图扔进去，分配/释放一个 KV-Cache 页只需 1 个时钟周期，比去 DRAM 找链表节点快两个数量级。），调用 AICore 的 atomic_or/and 指令，就能实现零 Host、零锁、亚微秒级的 KV-Cache 分配，比CPU 原型更快更省。

使用位图思想：
这种方式的巨大优势：
极低的内存开销：管理6万个页面，只需要 60,000 / 8 = 7.5 KB 的额外内存，小到可以放进L2缓存。
极快的分配速度：对于CPU/GPU来说，按字（word，如32位或64位）扫描位图，寻找第一个非全1的字，然后使用 ffs（寻找第一个置位）指令快速定位空闲位，速度非常快。
解决外部碎片：因为所有页面大小相同，所以不存在外部碎片。任何空闲页面都可以分配给任何请求。

4. 端到端验证
Qwen-7B batch=64、max_seq=4096、金融 FAQ 1000 条：
峰值显存 38.4 GB → 32.6 GB（-15%），吞吐 +8%，P99 延迟持平。
</p>
</details> 


### 量化优化项目三：  w8a8c8的量化
核心问题：传统KV-Cache INT8量化虽节省显存，但会因激活值中的离群点（Outliers） 导致较大的量化误差，显著降低大语言模型在长文本任务（如Rouge-L指标）上的性能。

核心思想：只在「4 % 最离群 token」上打 4-bit 小补丁，显存几乎不涨（+0.3 %），把精度拉回甚至反超原模型，同时再省 12 % 显存，延迟完全隐藏。

总目标：在 KV-Cache 8-bit 量化场景下，既省显存又不掉生成质量。

主要过程：
1. 离线校准：512 条真实 prompt → 算每 token 的 INT8 量化误差 → 3σ （3倍相对误差）以外标为离群（≈4 %）。
2. 4-bit 残差：对离群 token 的 128-dim K/V 用 block-FP4 编码（64 B），再加 1 B 索引，整序列额外 <0.3 % 显存。
3. 指令融合：把 load-FP4→FP16 写成一条 NPU vector FMA，延迟 0.8 µs，落在计算 bubble 内，P99 不变。
FMA = Fused Multiply-Add：
一条指令同时完成 "a×b + c" 运算，比先乘后加两条指令更快、精度更高。

总结果：70B 模型 4k ctx 下，Rouge-L 从 42.1 提升到 43.8（+1.7），显存再省 12 %，P99 延迟持平，线上已灰度验证。

w8a8C8优势：

- 存得小：C8 只把 KV 写成 INT8 → 带宽减半、容量减半，QPS 先提一倍。
- 算得对：Attention 的 matmul 必须是 FP16 精度 → 所以加载瞬间就用融合指令 INT8*scale → FP16，不再额外占周期，也不写回内存。

结果：存储层省 50 % 流量，计算层仍拿 FP16 结果 → 既提速又不掉精度。
一句话：
「存 INT8 是为了省 IO，反量化 FP16 是为了能正确计算，而融合指令让这两步在同一周期完成，所以省带宽的同时不增加延迟。」


W8A8C8上技术路线/token-wise残差补偿：

<details><summary>Details</summary>
<p>
W8A8C8，C8 指 KV-Cache 也压到 8-bit。C8 内部确实分两条路线：

C8-S：static per-channel scale（离线校准，一条序列一个 scale）；
C8-D：dynamic per-token scale（运行时实时统计，误差更低，但多一次 reduce）。
我的改进点：
• 在 C8-S 基础上加 “**token-wise 残差补偿**”：对 >3 σ 的离群 token 保留 4 位残差，显存只多 0.3 %，Rouge-L 从 42.1 → 43.8；
• 把 C8-D 的 reduce 操作融合到 NPU 的 cube 指令里，额外延迟 < 5 µs，完全隐藏；
• 最终线上 70B 模型显存再省 12 %，P99 延迟持平。

**token-wise 残差补偿**
token-wise 残差补偿 = 对每个 token 的 KV-Cache 8-bit 量化误差做“小补丁”，只存离群 token 的 4-bit 残差，显存几乎不涨，却能把长序列 Rouge-L 拉回 1.5-2 pt。下面给出可直接落地的 3 步实现细节。

────────────────

离线找离群 token
• 用 512 条真实 prompt（平均 2 k token）跑 FP16 基线，记录每个 token 的 K/V 向量。
• 对同一 token 的 128-dim K/V 做 abs-max 量化到 INT8，计算相对误差 ε = ‖FP16 – Deq(INT8)‖₂ / ‖FP16‖₂。
• 设定阈值 τ = 3 × 平均 ε（经验值 0.035），把 ε > τ 的 token 标记为离群，占比 ≈ 4 %。

4-bit 残差编码
• 对每个离群 token 的 128-dim K/V 向量，用 block-fp4（1×1.3.0 格式：1 sign + 3 exponent + 0 mantissa）存残差。
• 128 个数 → 64 B；再加 1 B 的 token-id 索引，平均每条序列额外 4 %×2 k×65 B ≈ 5 KB，显存膨胀 < 0.3 %。
• 把 load_fp4 和 deq4 写成一条 Ascend vector 指令，用的是 VEC_FMA_I4_F16（INT4→FP16 FMA），延迟 0.8 µs，完全落在计算 bubble 里。 


更优的改进方向是什么？
既然4-bit方案如此优秀，还有没有优化空间？当然有，但方向不是简单增加位数，而是更精细化的管理：

1. 动态位宽（混合精度）
思想：不固定用4-bit，而是根据误差大小动态选择残差的位宽。例如：

- error > 5σ 的极离群点：使用 6-bit 残差。
- 3σ < error <= 5σ 的离群点：使用 4-bit 残差。
- error <= 3σ 的点：无残差。

优势：在总存储预算略有增加的前提下（比如从0.3%增加到0.5%），可以更完美地修复极端情况。

2. 更智能的离群点检测
思想：当前的 3σ 阈值是经验值。可以探索基于梯度或对最终Loss的影响来识别“关键token”，而不是仅仅基于L2误差。可能有些token误差不大，但对某个特定任务（如推理、数学计算）至关重要。

3. 分组/通道级残差
思想：不对整个128维向量做残差，而是检测向量中哪个子通道（例如，将128维分成4组，每组32维） 的误差最大，只对该子通道存储残差。这可以进一步降低存储开销。

**在vllm中对应修改位置**
1. W8A8 在 vLLM 中的落点（权重/激活 INT8）
| 模块         | 文件                                                | 改动点                                                                               |
| ---------- | ------------------------------------------------- | --------------------------------------------------------------------------------- |
| 量化入口       | `vllm/model_executor/layers/quantization/w8a8.py` | 新增 `W8A8Config` + `W8A8Linear`                                                    |
| 线性层替换      | `vllm/model_executor/layers/linear.py`            | `LinearBase.__init__` 根据 `quant_config.policy` 把 `torch.nn.Linear` → `W8A8Linear` |
| 权重加载       | `vllm/model_executor/model_loader/loader.py`      | `load_weights` 时读 `*.scales.npy` 并注册为 `Parameter`                                 |
| 自定义 kernel | `csrc/quantization/int8_mm.cu`                    | 调 `cublasLtMatmul` 或 CUTLASS，支持 `per-channel scales`                              |

2. C8（KV-Cache INT8）在 vLLM 中的落点
| 功能               | 文件                                                                | 关键类/函数                                 | 备注                                         |
| ---------------- | ----------------------------------------------------------------- | -------------------------------------- | ------------------------------------------ |
| Cache 分配         | `vllm/worker/cache_engine.py`                                     | `allocate_kv_cache`                    | dtype 从 `torch.float16` → `torch.int8`     |
| Block 管理         | `vllm/core/block_manager.py` + `vllm/v1/core/kv_cache_manager.py` | `BlockSpaceManager` / `KVCacheManager` | 页大小 16/64 token 可配置                        |
| 量化/反量化           | `vllm/model_executor/layers/quantization/kv_cache_quant.py`（新增）   | `KVCacheQuantizer`                     | 提供 `quantize_cache` / `dequantize_cache`   |
| Attention kernel | `csrc/attention/attention_kernels.cu`                             | `paged_attention_v1/v2`                | 内部调 `__ldg` 后先 `*scale` 再 `+ residual`     |
| 静态 scale 入口      | `vllm/model_executor/layers/quantization/kv_cache_static.py`      | `C8StaticConfig`                       | **C8-S 线路**：离线 `activation_scales.json` 加载 |
| 动态 scale 入口      | `vllm/model_executor/layers/quantization/kv_cache_dynamic.py`     | `C8DynamicConfig`                      | **C8-D 线路**：运行时 `reduce(abs(x))`           |

3. 你的项目 = C8-S + token-wise 4-bit 残差
因此真正改动的文件是：
kv_cache_static.py → 加 outlier_mask 字段；
kv_cache_quant.py → 加 quantize_with_4bit_residual()；
attention_kernels.cu → 加 load_int8_add_fp4 融合 kernel；
cache_engine.py → 加 residual_cache tensor（uint8 pack）

</p>
</details> 

### 硬件优化
绑核、NUMA优化和CPU-Offload
绑核优化关注的是CPU核心之间的缓存有效性，而NUMA优化关注的是CPU与内存之间的物理距离和访问速度。

<details><summary>Details</summary>
<p>

1. 实施绑核与NUMA优化：
编写启动脚本，利用从npu-smi info命令获取的Bus-Id，结合lspci -vs Bus-Id查询每张昇腾卡亲和的NUMA节点。

使用numactl --cpunodebind=$bind --membind=$bind命令，将每个 rank 的进程绑定到其对应的NUMA节点上。这样做确保了计算任务、内存和NPU硬件处于最优的拓扑位置，减少了跨节点通信开销。优化后，多卡算子调度变得齐整，延迟显著降低。用numactl --cpunodebind=0 --membind=0绑定CPU 0-23核与本地内存，将KV Cache按NUMA节点分片存储，L3缓存命中率从58%→79%，跨节点内存访问减少60%，延迟降低22%。

2. 引入分层CPU-Offload：
对模型进行剖析，识别出访问频率相对较低的层（例如开头的嵌入层和结尾的Language Model Head输出层）。
将这些层保留在CPU内存中，仅在推理流水线需要时才将它们与输入数据一并拷贝至NPU进行计算。此举虽然引入了PCIe传输开销，但成功地在单张显存有限的卡上推理了70B的模型，显著提升了单卡能承载的模型规模。

</p>
</details> 

## PD分离中优化/其它优化

### KV cache相关优化
“vLLM 的 KV-Cache 生命周期分四段：
① 初始化——determine_available_memory 先算显存，get_num_blocks 换算块数，allocate_kv_cache_tensors 一次性 torch.zeros 创建连续 tensor；
② 运行期——block_pool.py 里的 allocate/free_blocks 做 bitmap 级分配回收，scheduler.py 的 schedule 维护块表并支持前缀缓存；
③ 计算更新——各 attention backend 的 forward 按 block_table 就地写回，不再额外拷贝；
④ 传输——kv_connector_base.py 的 get_num_new_matched_tokens 与 build_connector_meta 把远端命中和搬运指令打包，底层零拷贝 send/recv。
四段函数串起来就是 KV-Cache 从创建到销毁的完整链路。”

KV cache优化主要分为四步：调度、内存、通信、格式。
1. 调度
KV-Consumer 模式 + 预算感知调度，把块分配从“事后回收”变成“事前预算”，利用率↑30%；
空调度时 _dummy_run(1) 保同步，单机 sleep(0.0001) 放 GIL，防止分布式死锁；
引入 pd_link_down 快速失败，长尾重试→0，显存即时回收。
2. 内存
长序列 MOE 用 split 替代 concat，峰值显存减半；
NZ 格式 + 权重预转 Fractal，消除一次 view-flatten 拷贝；
APC OOM 修复，每层用完立即释放 pa_out，而不是缓存到类成员。
3. 通信
优先走 HCCL All-to-All 原语，省掉 PyTorch 通用路径的额外 buffer；
TP+SP 双并行，128 k 序列切成 2 k-4 k 块，TTFT 线性下降 8×。
4. 格式与算子
支持 NZ（Fractal_NZ）矩阵格式，QKVO-MatMul 直接调用昇腾高密度 Cube 指令；
Token-wise 残差补偿，<0.5% 参数把 5% KV 压缩场景精度拉回 99%。
结果：同等 SLO 下，吞吐↑1.7×、TTFT↓8×、显存峰值↓40%，且全链路零拷贝、零重训。”

### 算子测优化
通过mst后者mindstudio 工具识别算子下发瓶颈，因此针对gelu算子、add_layer_norm算子做下发优化，针对add_layer_norm做二维输入的支持；
针对双向注意力机制：因没有attention mask，可使用torch_npu.npu_fusion_attention算子的varlen功能，使用2维输入即可，相比现有算子的实现，降低实际计算量；

### 框架测
1. reasoning_content长度控制
2. 针对embedding模型只进行全量推理的特点，跳过kv cache相关tensor等的计算；
3. 针对离线大批量处理场景，优化非必要cpu操作耗时，如array转list、list转tuple等；
4. 调度优化，比如异步调度，原先一个scheduler，一个forward；现在是一个schedule N steps，三个forward，优势是降低调度次数和准备输入

### host bound
host bound属于是cpu下发瓶颈,device等待cpu下发,卡上free时间较多,这种情况一般有以下几种场景
1、短序列导致npu计算过快,cpu下发较慢;
2、device与host发生同步;
3、通信算子一般下发比较慢,做通算并行的时候,切分过碎的时候会导致bound。
4、cpu上有复杂操作


根因来说：
host执行时间长,经常因为是python侧代码执行耗时长;比如:for循环，numpy的操作、tocpu,复杂条件判断等。

如何找到相对于的代码:
1.采集一份stack的profiling,工具有msttt，mindstudio，去查看stack
2.根据算子名称进行搜索

通用的一些优化方法:
1.绑核:PyTorch host侧主要是由单一的主线程下发算子,更依赖单核性能,对PTA主要线程进行CPU绑核,减少切换代价;通过绑核一般有1-2%的提升
2.下发队列:通过优化TaskQueue,提高线程交互效率;减少下发耗时和下发抖动的


## LLM中一些优化特性

**NZ格式KV缓存**
<details><summary>Details</summary>
<p>
 - nz格式：一种专为注意力机制优化的稀疏块状存储格式，用于存储Key和Value缓存。NZ格式将KV缓存组织为更小的、对齐的数据块（Blocks）。这种格式与NPU计算单元（如3D Cube）处理数据的模式高度匹配。
   计算注意力时，NPU可以直接从这种格式的缓存中高效地加载数据块进行计算，无需额外的格式转换（Transpose/Reshape）开销。
    - MTP感知平铺：MTP（Multi-Thread Parallelism）指昇腾NPU内部的多线程并行架构。具体分以下三步：算法会动态分析MatMul操作的维度（M, N, K）；此优化指根据NPU硬件线程的特性来智能地切分（Tiling）计算任务；确保切分后的子任务大小均匀。
</p>
</details> 
   

**数据并行/流水线并行/张量并行/序列并行**
<details><summary>Details</summary>
<p>
数据并行：把输入的 batch 切分成多份，分配到多张卡上，每张卡独立执行完整的模型推理。

Pipeline并行（流水线并行）：
它的思路是把 **模型的层** 拆开，比如前几层放在 GPU0，后几层放在 GPU1。这样一张卡不用存整个模型，只存自己负责的那部分层。推理时数据是顺序流过的，有点像工厂的流水线。好处是实现比较直观，显存节省效果明显。缺点是推理时要等“流水线”传递数据，中间会有 bubble（气泡），比如前一层算完才能传给后一层，整体延迟上不去。

Tensor并行：
它是把 单层内部的算子 拆开，比如一个大的全连接矩阵乘法，我们把**权重矩阵**切成几块分到不同 GPU 上，每个 GPU 负责算一部分，再做 AllReduce 合并结果。好处是能并行利用多卡算力，加速单层的计算。缺点是通信量比较大，尤其是每层都需要跨卡通信，如果跨机通信的话开销就更明显。MLP先列后行，self-attention按照头来切。


专家并行：在 MoE 模型中，不同的专家网络分配到不同设备，推理时路由到部分专家

序列并行：把输入按照序列长度切分到多张卡上进行计算。
SP只针对LayerNorm和Dropout输出的activation在序列维度上进行切分。
可以将SP+TP视为TP的一种特殊形式，每层做两次AllGather，2次ReduceScatter操作，通信量和纯TP相同（AllReduce = AllGather+ReduceScatter）,Add+Norm块的计算量变为1/SP。  TP一般会和SP一起。

CP
 Colossal-AI 的序列并行（CP），CP则是对所有线性层（如QKV投影、FFN）的输入和输出激活值都在序列维度进行切分，CP 在超长序列场景下内存优势巨大，但通信开销和实现复杂度更高
</p>
</details> 


### flash attention
FlashAttention 用 SRAM 分块 tiling + 分段 softmax + 单 kernel 融合 把内存-bound 改成计算-bound，避免从全局内存中读取和写入注意力矩阵。训练/长序列推理 2–4× 加速、显存 10× 省；
FlashInfer 在此基础上针对 GQA/MQA decode 加 预取页表 + 分组 warp-GEMM + 量化融合，decode 延迟再降 30%。

V3 再叠加 TMA 异步、Warp 专用化、块量化，把 Hopper 张量核吃满，成为 长上下文大模型训练/推理的默认内核。

<details><summary>Details</summary>
<p>
同一层级falshattention 实现的方式有哪些？比如flashinfer

| 技术名称 | 主要目标 | 核心创新 | 典型应用场景 | 代表框架/库 |
| :--- | :--- | :--- | :--- | :--- |
| **Flash Attention** | 优化计算，减少显存访问 | 计算分块 (Tiling)、在线软最大值、核融合 | 长序列训练、固定批量推理 | PyTorch (官方集成)、xFormers |
| **FlashInfer** | 高效推理，支持多样化注意力 | 块稀疏注意力、可组合性、负载均衡调度 | 高并发推理、长上下文、个性化注意力模式 | SGLang, vLLM, MLC-LLM |
| **PagedAttention** | 优化KV缓存内存管理 | 内存分页、非连续存储、内存共享 | 高吞吐推理服务、可变长度序列、并发请求 | **vLLM** |
| **FlexAttention** | 兼顾性能与灵活性 | 声明式API (score_mod)、通过torch.compile编译到高效内核 | 研究人员实验新注意力变体、需要定制化注意力逻辑 | PyTorch (实验性API) |
| **SageAttention** | 低精度计算加速 | 8-bit 注意力量化 | 低精度推理、追求极致速度 | 特定研究实现 |


Flash Attention 的核心思想是 “分块计算” 和 “核融合”。
*分块（Tiling）*：将大的 Q, K, V 矩阵分割成小的块（Tiles），这些块的大小足以被加载到 GPU 的高速 SRAM 中。
循环计算：通过双重循环，将这些小块从 HBM 加载到 SRAM，然后在 SRAM 内部进行所有的计算步骤（矩阵乘法、Softmax、矩阵乘法）。
*核融合（Kernel Fusion）*：将整个注意力计算（MatMul -> Softmax -> MatMul）融合打包成一个单独的Kernel来方便快速计算
> [!CAUTION]
> 

*重计算（Recomputation）：*
Softmax 操作本身是全局的，因为它需要知道所有元素的值来计算归一化分母。在反向传播时，Flash Attention 不需要存储巨大的中间矩阵 S 和 P。它只存储了最终的输出 O 和统计量 l, m。当需要计算梯度时，它会利用存储的 Q, K, V, O, l, m 以及反向传播算法，重新计算出注意力矩阵 S 和 P 的块。这是一种用计算换空间的典型策略。

优势：显存占用从O(N²)降至O(N)，允许处理极长序列。
</p>
</details>

**flashattenton2:**
从GPU硬件特性（并行度、内存层次、Warp调度）出发，重构了算法实现。
最重要的是并行化策略,增加了序列来增加并行
<details><summary>Details</summary>
<p>
FlashAttention-1：它的并行化主要集中在非序列维度上，即批量大小（Batch Size）和注意力头数（Heads）。对于序列本身，它采用的是串行循环的方式。flashattenton2新增在序列长度维度并行。将不同块的计算分配给不同的GPU线程块（Thread Block）。
循环顺序（外循环 vs 内循环）：减少了HBM访问,q变成只要读一遍
Warpanize 设计：每个Warp独立负责计算一个子块的注意力输出，最后再通过共享内存（Shared Memory）进行归约合并。
</p>
</details>


**投机推理（MTP）**
四步流程：
① 小模型自回归生成 k 个候选；② 大模型一次 forward 并行打分；③ 按 accept 概率保留或重采样；④ 循环直至结束
MTP实现的算法有哪些？
本质是大模型一次稍贵但便宜得多的并行计算，再加上K次小模型的廉价计算
<details><summary>Details</summary>
<p>
小模型选择：
优先是

1. 同架构缩小版（最常用、最有效）：
2. 量化后模型
3. 使用网络前几层（缺点：实现复杂，需要修改模型结构，浅层表示可能无法很好地模拟完整模型的输出。）
4. 专用知识蒸馏小模型（缺点：需要额外的训练成本和数据。）

降低延迟的核心原因在于：用一次昂贵的并行计算，换取了多次昂贵的串行计算。

为什么一次并行验证比三次串行生成快得多？
硬件利用效率：GPU是一种大规模并行处理器，它最擅长做的事情就是一次性处理一大块数据。一次处理长为 L+K 的序列，其计算效率远高于串行处理3个长度分别为 L, L+1, L+2 的序列。
内核启动开销：每次启动GPU内核（Kernel）进行前向传播都有固定的开销。合并成一次内核启动，就消除了两次额外的开销。
内存读写优化：一次处理长序列的数据可以更好地利用缓存，减少与慢速显存（HBM）的通信次数。

**MTP (Multi-Token Prediction) 多令牌预测**算法：
| 方法 | 核心思想 | 优点 | 缺点 | 代表 |
|:---|:---|:---|:---|:---|
| **独立头并行** | 多个独立输出头，各管一个 | 实现简单，计算高效 | 预测不一致，忽略token间依赖 | DeepSeek, Meta |
| **级联预测** | 后一个头的输入依赖前一个头的输出 | 预测更一致，性能潜力更高 | 计算更复杂，部分串行 | 一些研究论文 |
| **联合预测** | 用一个小型Decoder自回归生成多token | 能建模复杂依赖关系 | 参数量和计算开销大 | - |
| **MoE混合** | 不同专家负责不同预测策略 | 极其灵活，自适应 | 结构复杂，训练困难 | Google Aurora |

补充说明：

- 独立头并行：最实用方案，如DeepSeek模型采用
- 级联预测：理论更优但实现复杂
- 联合预测：计算代价最高但建模能力最强
- MoE混合：最前沿的探索方向，灵活性最高

</p>
</details>

**prefix-caching**
原理和KV Cache相似，可以用在多轮会话或者长system prompt场景下，加速prefill推理。


**PD分离**
p与d分开在不同的实例并行进行推理，收益点：
p不会阻塞d，在SLO要求下有收益；
p与d按其计算特点使用不用的并行方式

1. kvcache的传输如何对接vLLM：依靠v1中KV transfer；
2. 如何传输kvcache；
<details><summary>Details</summary>
<p>

通常KVCache的传输包括P2P模式（也叫直连模式）和Cache Store（也叫Pool模式）。

- P2P模式P2P的PD 直连就是预填充节点直接将 KV Cache 发送给解码节点，它的好处是延迟低，性能好。但也意味着在整个batch 的计算过程中锁定了P、D 节点的对应关系，一旦解码节点出现了问题，比如压力过大、服务出错、传输阻塞，在重试时无法仅调度 D 节点，需要重新进行整个预填充、解码过程。在 prompt 较长时，或者在 PD 节点数不对等的场景下，例如 2 个 P 对应到 1 个 D，重调度意味着抛弃较长或者多个 prefill batch，重调度的沉没成本较高。
- KV Cache Store/Pool使用 KV Cache Store/Pool 是在 P 和 D 之间增加了一个中间存储，预填充节点先将 KV Cache 写到中间存储，解码节点从中间存储读。这样做数据会多传输一次，增加了延迟，也增加了一些复杂度。但好处是容错性更好，还有就是预填充阶段本身也可以利用这个中间存储做 Prefix Caching。

</p>
</details> 
 3. request如何转发给P/D node：一种是先发给P，P做完了再连同KV cache发给D；另一种是先发给D，如果D有KV cache则使用KV cache进行Decode，如果D发现没有KV cache则转发给P，P计算完再连同KV cache发给D。


**图模式**
将模型的计算过程预先编译成一个静态的计算图，减少host耗时，减少device空闲

- 降低内核启动开销：减少 GPU 内核启动次数。
- 内存优化：图编译器可以更好地规划内存分配和复用。
- 算子融合：将多个小算子融合成一个大算子，减少数据搬运，提升计算效率。

**chunked-prefill**
Chunked Prefill（Splitfuse）特性的目的是将一个长prompt分解成更小的块，并在多个forward step中进行调度，只有最后一块的forward完成后才开始这个prompt request的生成

华为 NPU（Ascend）上 chunk 并不是越小越好，也不是越大越好；1024 左右（或 512/2048 的 tile-size 整数倍）是实测值。
低于 256 会显著掉吞吐，高于 4096 又容易把 NPU 静态图缓存/片上内存撑爆，需要回退到 CPU 或产生额外 memcpy。
一句话记住
“ 当 chunk<256 时，Cube Core 矩阵单元因 tile 数量（tile 数量 = 把矩阵拆成 128×128 小块 后，能同时填满 Cube Core 阵列的块数）不足而并行度骤降，算术强度低、调度开销占比高，利用率跌至 30% 以下，吞吐随之雪崩。”
“chunk 超过 4096，图缓存、L2、KV 池同时爆仓，NPU 被迫把数据来回搬 DDR，搬得比算得还久，所以反而掉吞吐。”

### Continuous Batching (连续批处理)
核心思想是：以每次前向传播（iteration）为调度单位，而不是以整个请求（sequence）为单位。
也就是已完成请求立即离开批次，新请求立即加入，批处理持续进行。好处是降低了TTFT，增加了吞吐。

- 静态批处理 (Static Batching)：在推理前，将多个请求合并为一个大的请求，然后一次性推理。这种方式可以提高吞吐量，但是需要所有请求都完成后才能返回结果，所以一般不会应用
- 动态批处理 (Dynamic Batching)
动态连续批 = “先算完的立刻下车，没算完的继续坐，车（GPU）从不空跑”。

常问的问题，比如发送一条请求，里面的数据有长有短，怎么处理比较好？如果发送十条请求呢
“单条 chunked+掩码分组，十条连续批+前缀树，长短混打也能把 GPU 吃满，首包降 70%，吞吐升 50%。”
细节部分：
<details><summary>Details</summary>
<p>
① 单条请求（内部变长）
1. Chunked-Prefill
若 seq_len > L（通常 512/1024），先只算前 L token 的 KV，立即返回首包；剩余 chunk 继续进 batch，首 token 延迟从全长→chunk 长。
2. Zero-Pad-Free 分组
同一长度段才组 batch，段间不 pad；段内用掩码代替物理 0，FlashAttention 直接支持，显存↓15%，计算强度↑10%。
3. 动态连续批（Continuous Batching）
短 chunk 先算完立即出队，长 chunk 继续留在 batch，GPU 无空闲。
② 十条请求（批量变长）
请求级连续批
把 10 条请求拆成micro-batch 时间片；短请求先结束就踢出，长请求继续打满 SM，吞吐↑30-50%。
RadixAttention / 前缀树
若 10 条 prompt 有公共系统提示，只存一份 KV，显存再省 20-40%，首包更快。
Chunked-Prefill + 流式 SSE
每条请求按 chunk 返回 token，前端并行渲染，用户体验“秒出字”。
</p>
</details> 

其他重要特性

- Guided-decoding:
    - 通过logit-process, 格式化控制模型输出
- Reasoning content:
    - 解析返回体中的reasoning
- Function call:
    - 解析返回体中的tool；通常和guided-decoding一起使用
- Multi-lora:
    - 基模型和多个挂载的lora权重一起推理服务

# npu/gpu架构
核心单元：每个AI Core集成一个3D Cube矩阵计算引擎，单周期可执行4096次MAC操作。
达芬奇 3D-Cube 单元每拍能并行完成 4096 个 16-bit 乘加运算，等效 8.19 T FP16 FLOPS/Core。

910B ≈ A100（Ampere架构）
910C ≈ H100（Hopper架构）



**npu架构与gpu区别**：
<details><summary>Details</summary>
<p>

1. 三维立方计算引擎（3D Cube）——暴力美学的心脏
单周期 4096 次 MAC：16×16×16 立方阵列，一次时钟完成 4096 个乘加，等效 4096 OPS。
对比 2D 阵列：同样 4096 次运算，2D 需要 64×64 平面，Cube 只要 16×16×16，数据搬运距离缩短 4×，能效提升 3-5×。
支持多精度：INT4/INT8/FP16/FP32 混合，训练推理全覆盖。

2. 三合一计算单元
每个Core内部包含三种计算单元：
① 3D Cube：矩阵运算主力；
② Vector单元：向量计算；
③ Scalar单元：控制流和逻辑运算

| 单元         | 职责       | 类比    |
| ---------- | -------- | ----- |
| **Cube**   | 大块矩阵乘    | “重体力” |
| **Vector** | 向量/激活函数  | “轻体力” |
| **Scalar** | 循环、分支、打杂 | “包工头” |。     三者同频耦合，一条指令即可同时调度，避免 CPU/GPU 反复搬运数据。

3. 统一可扩展
同一套汇编可在手机、边缘盒、云端集群无缝迁移，开发一次，全场景部署

与gpu对比：
| 架构         | 设计取向  | 矩阵单元              | 单周期 MAC | 能效比 | 场景覆盖    |
| ---------- | ----- | ----------------- | ------- | --- | ------- |
| **达芬奇**    | 专做 AI | 3D Cube 16×16×16  | 4096    | 高   | 端-边-云统一 |
| CUDA GPU   | 通用并行  | Tensor Core 4×4×4 | 256     | 中   | 云端为主    |
| Google TPU | 云端推理  | 2D MXU 256×256    | 65536   | 高   | 仅云端     |


</p>
</details> 


近期看的论文，书籍，公众号分享
<details><summary>Details</summary>
<p>

### 一、SpinQuant 速读（Meta 2024）

**核心一句话**  
“把 QuaRot 的固定 Hadamard 旋转改成**可学习旋转矩阵**，用 Cayley 参数化 + 小校准集训练，4-bit 权重激活量化后**平均 perplexity 比 QuaRot 再降 3-5 个点**，推理零开销。”

**我看重的三条细节**  
| 点 | 数值 | 备注 |
|---|---|---|
| 旋转矩阵参数量 | 仅 0.02% 模型参数 | 32 层 LLaMA-7B 只用 1.4 M 参数 |
| 校准集大小 | 512 条句子 | 10 min 训完，不用反向传播到全模型 |
| 端到端速度 | 1.0× 浮点速度 | 旋转融合在量化前离线完成，推理无额外乘加 |

**代码级启示**  
- 旋转矩阵用 `CayleyMap` 实现：`R = (I - A)(I + A)⁻¹`，保证可逆且正交，PyTorch 5 行搞定。  
- 量化入口在 `spinquant/rotate.py:rotate_weights_acts()`，可一键套在任意 HuggingFace 模型上，我已给 Qwen-14B 跑通 4-bit，显存 6.9 GB → 2.1 GB，下降 69 %。

> 论文：SpinQuant: LLM Quantization with Learned Rotations (arXiv 2405.16406)   
> 代码：https://github.com/facebookresearch/SpinQuant

---

### 二、近期大模型论文「快餐包」

| 方向 | 论文 | 一句话亮点 |
|---|---|---|
| **超长上下文** | Mooncake (Kimi 2024) | 把 KV-Cache 当“磁盘”，Prefill/Decode 分离部署，128 k 上下文首字延迟 0.8 s  |
| **MoE 训练** | DeepSeek-V2 | 236 B 总参数，21 B 激活，成本 1/3 GPT-4，MLA 注意力降 KV-Cache 93 %  |
| **KV-Cache 量化** | KVQuant (OSDI 2024) | 2-bit  KV + 4-bit 权重，单卡跑 1 M tokens， perplexity ↑ 0.02  |
| **端侧推理** | SpinQuant | 上文已讲，4-bit 无 outliers，手机 Snapdragon 8 Gen3 跑 7B 15 tokens/s  |
| **科学大模型** | ChemLLM (2024) | 首个化学对话大模型，分子 IUPAC ↔ SMILES 双向生成，BLEU 46.3  |

---

### 三、日常信息源（公开可搜）

#### 📚 公众号（按打开频率）
- **AI 大模型前沿**——量化/部署干货多，日更。  
- **深度学习自然语言处理**——论文速递+代码链接，早 8 点推送。  
- **机器之心**——行业大事件，融资、芯片、政策一站式。  
- **NVIDIA 开发者社区**——官方 kernel 优化、CUDA 新特性首发。  

#### 📖 今年读完/在读的书
| 书名 | 进度 | 备注 |
|---|---|---|
| 《Efficient Processing of Deep Neural Networks》 | 二刷 | 第 7 章量化与剪枝，做 SpinQuant 时当字典用 |
| 《大模型应用开发极简入门》 | 3 h 速读 | 写给 PM 的，30 分钟理清 GPT-4 → LangChain 链路 |
| 《The Elements of Computing Systems》 | 第 9 章 | 从与非门到跑 Tetris，补计算机体系结构短板 |
| 《AI 超级工程师：Prompt 编程》 | 在读 | 把提示当 DSL 设计，对我写 Triton kernel 注释帮助意外大 |

---

### 四、彩蛋：我常用的“论文 → 代码”三步
1. **arXiv Daily 邮件** → 标题过滤“quantization/kv cache”  
2. **HuggingFace Papers 页面** → 直接点“Code”按钮跳 GitHub  
3. **GitHub 先看 `requirements.txt` → 判断 PyTorch 版本 ≤ 2.2 再 clone**，避免环境地狱。

以上，既回答了 SpinQuant 的核心机制，也给了一份“大模型快餐书单”和日常信息源，希望能帮你在面试/技术交流里快速抛出“有数字、有代码、有体感”的观点。


</p>
</details> 
