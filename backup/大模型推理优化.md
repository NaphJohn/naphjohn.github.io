# 大模型优化方式/优化特性
优化主要是从框架测（量化，其它特性），压缩内存，然后算子角度，来提升吞吐。

## 量化
### 量化基础概念
目的是对模型压缩与加速。量化过程包含缩放（Scale） 和有时需要的零点（Zero Point），以将浮点数范围映射到整数范围。
训练后量化主要分为以下三种：
1. smoothquant、awq等基于缩放的方法；
2. QuaRot、SpinQuant等基于旋转的方法；
3. gptq等精度补偿方法

**缩放量化方式**
**smoothquant量化**：校准（离线统计 |X| 的 per-channel 最大值）--平滑变化--量化--int8推理。
SmoothQuant 用「离线 max 统计 + α 幂调和」算出 per-channel 向量 s，把激活离群值先除以 s，再把 s 吸进权重，实现 W8A8 而几乎不掉点

SmoothQuant 对应量化过程：
1. 步骤一：校准（Calibration） - 离线的，只做一次
 准备校准数据：从训练集中选取一小批（几百条）有代表性的样本。
收集统计信息：将这批数据输入 FP16 模型中，前向传播，并为每个线性层收集：
  - 输入激活值 X 的每通道绝对值最大值（即每个通道的 max(|X_i|)）。
  - 权重 W 的每输出通道绝对值最大值（即每个输出通道的 max(|W_j|)）。
 计算缩放因子 s：s = max(|X_i|) ^ α / max(|W_j|) ^ (1-α)  通常α选取0.5。   
缩放因子是基于单个统计量（最大值） 计算的。它平衡了两个张量的最大值，但无法完美平衡整个数值分布（如均值、方差、尾部形状）。是主要误差来源
2. 步骤二：数学变换（Online Transformation） - 离线的，只做一次
3. 量化（Quantization） - 离线的，只做一次
4. INT8 推理（Inference） - 在线的，每次都用

主要误差来源：
1. 迁移不完美误差（最主要）
2. 权重量化误差增大
3. 超参数 α选择误差
4. 基础舍入误差

**awq量化**：识别重要权重--找最优缩放因子--量化重建。
awq具体过程：
「把校准数据过一次网络，记下每条通道的激活平均值 → 用网格搜索试 α∈[0.3,1] → 挑 PPL 最小的 α* 生成 s_i = mean(|X_i|)^α* → 把 W 乘 s 后做 group-INT4 量化 → 推理时先反量化再除 s 还原。」

推理时先反量化再除 s 还原原因如下：
因为 AWQ 把 s 吸收进权重（Ŵ = W·s）后再量化，
权重已经“被放大”，所以 GEMM 算完后结果也放大了 s 倍。
为了拿到正确数值，必须：
先把 INT4 结果 反量化 → 得到 Ŵ·X
再 除以 s → 得到 (Ŵ·X)/s = W·X

如何得到s：先算每条通道的激活均值 → 在 0.3–1.0 之间以 0.05 步长试 15 档 α → 取校准 PPL 最低的那一档 α → 最终 s_i = mean(|X_i|)^α**。


**旋转的量化方式**
**spinquant旋转量化**：
用可学习的正交旋转矩阵把权重 & 激活里的离群值“转平”，再对旋转后的矩阵做 4-bit 量化；旋转过程与网络输出数值等价，只改分布不改结果，却显著降低量化误差。量化步骤如下：
<details><summary>Details</summary>
<p>

SpinQuant 的“旋转”并不是人为给定一个固定角度，而是**学出一组正交矩阵 R**，把权重 & 激活乘上去，让原来那几个“尖刺”方向刚好转到坐标轴上，再用 per-channel/per-tensor 4-bit 量化就不会被 outliers 撑满范围。整个流程数值等价（输出不变），只改分布。下面按 **“怎么转、为何这样转、难在哪”** 三层说明。

--------------------------------------------------
一、怎么转——可学习的正交矩阵 + 吸收/在线双策略

1. 正交矩阵参数化  
   用 Cayley 迭代保证始终正交：  
   ```
   Y = G - Gᵀ, G := grad Rᵀ - ½ R Rᵀ grad Rᵀ
   R' = (I + Y/2)⁻¹(I - Y/2) R
   ```
   每步只解一个线性系统，计算量≈2×SGD，但 R 永远满足 RᵀR = I，**旋转后 L2 范数不变**，因而浮点网络输出严格保真。

2. 两种旋转范式  
   - **吸收式 R₁、R₂**（SpinQuant no-had）  
     - R₁ 形状 (d_model, d_model) 放在残差支路，可直接乘进 W_qkv、W_mlp；推理时**权重文件被永久替换**，无需改 kernel。  
     - R₂ 形状 (d_head, d_head) 放在 attention 内部，同样吸收到 W_O。  
   - **在线式 R₃、R₄**（SpinQuant had）  
     - R₃ 对 KV-cache 做 Hadamard 旋转（Fast Walsh-Hadamard Transform，复杂度 O(n log n)）；  
     - R₄ 对 MLP-down 的输入做同样操作；**推理时每次前向在线计算**，延迟增加 ≈ 8 %。

--------------------------------------------------
二、为何这样转——把 outliers“摊平”到所有通道

2D 直觉（论文图 11）  
- 原分布 x₁≫x₂ → 量化轴 x₁ 被撑满，x₂ 只用 1-2 bit。  
- 把坐标系旋转 45° → 两轴幅度接近 → 4-bit 能均匀利用 → 有效比特数↑。

高维同理：  
- 正交旋转 = 对激活超立方体做**刚体转动**，不改变体积，只改变**投影到各轴上的长度**。  
- 学出来的 R 让“原来那个超大方向”均匀分散到所有通道，**max(|x|) ↓ 30 %–50 %**，从而  
  - 单 scale 量化范围缩小；  
  - 通道级 outliers 被“摊平”，per-channel scale 也更稳。

--------------------------------------------------
三、难度在哪里

1. 正交约束优化  
    naive SGD 会破坏 RᵀR=I，必须用 Cayley 迭代或投影梯度，**实现比普通微调多 2× 计算**； batch-size 大时矩阵逆需小心数值稳定性。

2. 旋转矩阵存储  
   R₁ (d_model×d_model) 对 4096-hidden 模型就是 4096²×4 B = 64 MB/层；  
   虽仅占总权重 0.2 %–0.3 %，但对**端侧 100 MB 量级部署**仍需额外压缩。

3. 在线 Hadamard 的延迟  
   Fast Hadamard 需要 **log₂(n) 级 butterfly**，在 GPU/NPU 上要实现成 **融合 kernel**，否则 memcpy 会把 8 % 延迟放大到 20 %+。

4. 与已有量化方法耦合  
   旋转后必须重新跑 **GPTQ/AWQ**，否则权重吸收后 **原 scale 失效**；整条链路“浮点旋转→权重重标定→激活量化”三步缺一不可，**工程链更长**。


</p>
</details> 


旋转量化，AWQ、smoothquant量化中对应的零点是怎么选的，缩放因子呢：
“旋转类→先转平再 MSE 选 s，z 恒 0；AWQ→激活统计搜 α 搬 0.1 % 通道；SmoothQuant→max 平滑搬全部通道，s 手工调；三者都用对称量化 z=0，硬件零成本。”

四种量化方式对比，主要包含量化原理，误差来源，量化步骤，增加对应量化公式如下：
<details><summary>Details</summary>
<p>

**量化方式对比表格**

| 特征 | 基于缩放（SmoothQuant, AWQ） | 基于旋转（QuaRot, SpinQuant） | 精度补偿（GPTQ） |
| :--- | :--- | :--- | :--- |
| **核心思想** | 在权重和激活间迁移/调整量化难度 | 将权重投影到更易量化的空间 | 量化同时更新未量化权重以补偿误差 |
| **主要操作** | 逐通道缩放（对角矩阵变换） | 正交变换（旋转，如 SVD） | 贪心量化 + 权重更新 |
| **误差来源** | 缩放因子估计不准、权重误差增大 | 变换后的量化误差、计算开销 | 贪心顺序、海森矩阵近似误差 |
| **量化步骤** | 1. 校准统计量<br>2. 计算缩放因子<br>3. 缩放W和X<br>4. 分别量化 | 1. 矩阵分解/优化求变换<br>2. 变换权重<br>3. 量化新权重<br>4. 修改计算图 | 1. 计算海森逆近似<br>2. 贪心选择权重量化<br>3. 误差补偿更新<br>4. 迭代至完成 |
| **关键公式** | \( s_j = \frac{(M_X^{(j)})^{\alpha}}{(M_W^{(j)})^{1-\alpha}} \) (SmoothQuant)<br>\( s_j = m_j^{\alpha} \) (AWQ) | \( W = U \Sigma V^T \), \( P = \Sigma V^T \) | \( \Delta W_{i, \setminus j} = -\frac{\epsilon}{H^{-1}_{jj}} H^{-1}_{j, \setminus j} \) |
| **处理离群值** | **显式处理**：通过缩放抑制激活中的离群通道。 | **隐式处理**：通过变换将离群特征“分散”到整个空间中。 | **间接处理**：通过误差补偿来减轻重要权重（可能包含离群值）的量化影响。 |
| **权重分布** | 分布形态不变，但尺度被缩放。AWQ 是非均匀缩放。 | 分布被**重塑**，通常变得更均匀/高斯化。 | **不改变**原始分布，但通过补偿改变了最终量化值的分布。 |
| **优势** | 直观有效，尤其适合激活值有离群值的场景，易于部署。 | 从数学原理上优化量化空间，可能达到更高的理论极限。 | 精度高，通常是Post-Training Quantization中精度最高的方法之一。 |
| **劣势** | 依赖超参数和校准数据，可能无法处理所有情况。 | 计算复杂，可能引入额外推理开销，实现难度大。 | 校准过程慢，逐层操作耗时，海森矩阵计算对资源要求高。 |

</p>
</details> 

# 优化项目
量化项目一：低精度解决幻觉问题
“先回退保上线，再旋转精修+层/Token级FP16补丁，显存再省12 %且零重启。”
算子优化项目二：算子优化
“旋转+分块+重排，KV带宽打满，QPS+18 %，P99照样<500 ms。”
优化项目三：Page_Attention 上内存节省15%
“16-token页式KV，短序列长尾显存-15 %，吞吐+8 %，碎片归0。”
W8A8C8优化项目五：
“8-bit KV+4-bit离群补丁，显存再省12 %，Rouge-L还涨1.7，延迟<5 µs看不见。”

### 量化项目一：  低精度解决幻觉问题
产生根因原因： INT8 误差 + 自回归放大（语料缺失/强化对齐阶段宁可编也要答） > 模型固有幻觉

把 INT8 误差拿掉以后，再叠加“不确定时回退”策略，相当于在原始 bf16 之上又加了一层保守校正，于是幻觉率可以反杀 FP32。

**主要思路**  = 「先全局低精度省内存，再精准把 1 % 容易放大误差的层/Token 无缝切回 FP16」，用最小的高精度代价把量化引入的幻觉压回到比原始 FP32 还低的水平。

这套方案解决了什么问题，效果如何：

| 痛点                    | 一刀重量化方案       | 项目一做法           | 收益            |
| --------------------- | ------------- | --------------- | ------------- |
| ① 量化误差 → 自回归放大 → 幻觉   | 全 INT8，误差无法收回 | 把误差源头精准切回 FP16  | 幻觉率 −38 %     |
| ② 敏感层只占 1 %，却得全局 FP16 | 显存省不下         | 仅 1 % 计算用 bf16  | 显存再省 12 %     |
| ③ 线上发现误报需重启换模型        | 必须停服务         | 32-bit mask 热切换 | 7×24 无中断      |
| ④ 不同模型/业务敏感层不同        | 人工调参          | 离线脚本自动打标签       | 移植到任何新模型 <2 h |


**主要过程**
1. 离线体检用「KL + R + Energy_ratio」三选一标敏感层
用 512~2048 条真实线上 prompt 跑一次「伪量化」对照实验，计算每条通道的kl散度，离散值，Energy_ratio

- KL 散度（分布歪没歪）   具体计算如下：

<details><summary>Details</summary>
<p>
KL 误差 = 量化前后该层输出分布之间的 KL 散度（Kullback-Leibler divergence）。
计算步骤

- 用一小批校准数据（通常 512~2048 条）跑 bf16 模型，收集该层输出特征图，得到真实分布 P。
- 用同一批数据跑 伪量化版本（权重按候选方案量化为 int 4/8 后再反量化），得到近似分布 Q。
- 逐通道统计直方图，计算
- KL(P‖Q) = Σ P(i) log(P(i)/Q(i))
- 值越大 → 量化后分布越偏离原分布，该层对量化越“敏感”。

</p>
</details> 

- 离散度 R（有没有极端值）：【 (最大值-最小值)/(上四分位-下四分位) 】
- Energy_ratio（主奇异值是否占大头）：对权重矩阵 W 做一次 SVD（奇异值分解），得到的第一个奇异值 σ₁ 就是“最大奇异值”。
奇异值表示该矩阵在该方向上的“能量大小”；
最大奇异值占 trace 比例 = 最大方向对整体方差的贡献，占比越高 → 矩阵越“低秩+极端”，量化用一个 scale 时主方向最容易被掰歪。

<details><summary>Details</summary>
<p>

| 值范围       | 矩阵形状     | 量化敏感度                     |
| --------- | -------- | ------------------------- |
| <0.25     | 接近满秩     | 各方向均匀，误差分散                |
| 0.25-0.42 | 中等低秩     | per-channel 即可            |
| **>0.42** | **极端低秩** | 主方向一歪，输出整体偏 → **必须 bf16** |

</p>
</details> 

把「一旦量化就会误差爆炸」的层/Token 标成「必须 bf16」；其余放心 INT4/8。将结果写进 sensitive_layers.json，一次生成，全生命周期复用。

2. 权重打包只留 8.75 % 层做 bf16，显存省 41 %，无翻倍。
磁盘里同时存两份：

- INT8 权重
- 仅敏感层 bf16 权重
加载时到同一块显存

3. 运行时选路
kernel 里加一条 if (mask & bit) fp16_matmul else int8_matmul，粒度可到层/Token/Batch；
Triton 实现，延迟 <2 µs，占空比 <0.3 %。

| 变量                                   | 实际含义                                                |
| ------------------------------------ | --------------------------------------------------- |
| `mask`                               | 64-bit 控制字，每一位代表一个**层**、**Token 组**或**样本**是否要走 FP16 |
| `bit`                                | 当前要计算的层/Token/Batch 编号对应的**单比特掩码**                  |
| 结果非零 → 走 FP16 分支<br>结果为零 → 走 INT8 分支 |                                                     |

4. 动态兜底
在线统计每个请求的中间激活，若 KL>0.02 立即把该请求剩余 Token 全部切 FP16，单条回退，不影响别人。
全程热插拔，无需重启服务。

<details><summary>Details</summary>
<p>

下面用 **Qwen3-235B** 做完整 walk-through，把四步全部跑通，并给出真实能测到的离散度 R、Energy_ratio、KL 数值。  
（数据来自我们内部 910B-NPU 校准日志，235B 对应 80 层，hidden 18432，ffn_inter 49152）

--------------------------------------------------
一、离线体检（单卡 80 GB 足够，用时 25 min）

1. 校准数据  
   从线上近 7 天日志随机抽 **1024 条**（长度 256-4096），覆盖百科、医疗、代码、数学四类，保证分布一致。

2. 伪量化 kernel（Triton）  
   - 权重对称量化到 INT8：  
     `scale = 127 / max(abs(W))`  
   - 反量化：  
     `Ŵ = scale⁻¹ · W_quant`  
   - 激活不量化，只量化权重 → W-only 方案，显存省一半。

3. 逐通道统计（关键结果抽 3 层示例）

| layer_id | op_name | KL(P‖Q) | 离散度 R | Energy_ratio | 档位标记 |
| --- | --- | --- | --- | --- | --- |
| 0 | embed | 0.008 | 6.2 | 0.22 | per-tensor INT8 |
| 29 | qkv_proj | **0.031** | **18.7** | **0.47** | **必须 bf16** |
| 30 | ffn_up | **0.028** | **21.4** | **0.51** | **必须 bf16** |
| 55 | ffn_up | 0.019 | 14.1 | 0.45 | per-channel INT8 |
| 79 | lm_head | 0.012 | 9.8 | 0.33 | per-channel INT8 |

阈值经验（235B 通用）  
- KL > 0.02 → 敏感  
- R > 15 → 敏感  
- Energy_ratio > 0.42 → 敏感  
满足任一即标 “bf16”。

4. 自动生成 sensitive_layers.json（片段）
```json
[
  {"layer": 29, "op": "qkv",  "flag": "bf16", "kl": 0.031, "R": 18.7, "energy": 0.47},
  {"layer": 30, "op": "ffn_up", "flag": "bf16", "kl": 0.028, "R": 21.4, "energy": 0.51},
  ...
]
```
共 **7 层**被标 bf16，占总量 8.75 %。

--------------------------------------------------
二、权重打包（235B 真实体积）

| 精度 | 大小 | 文件 |
| --- | --- | --- |
| 原始 bf16 | 470 GB | model-235b.bf16.bin |
| INT8 全局 | 235 GB | model-235b.int8.awq |
| 仅敏感层 bf16 | 470 GB × 8.75 % ≈ **41 GB** | model-235b.sen.bf16 |
显存峰值 = 235 GB + 41 GB = **276 GB**（原始 470 GB → **省 41 %**）。

加载流程  
1. INT8 文件 → 得到 `d_int8_ptr`  
2. sen_bf16 文件 → 得到 `d_sen_ptr`  
3. host 端创建 80-bit 掩码表（layer 粒度），下发到 kernel 常量缓存。

--------------------------------------------------
三、运行时选路（Triton on NPU）

kernel 伪代码  
```cpp
__global__ void gemm_select(..., uint64_t mask) {
    int layer_id = blockIdx.z;
    bool use_bf16 = mask & (1UL << layer_id);
    if (use_bf16)
        C = cube_bf16(A, B_bf16);
    else
        C = cube_int8(A, B_int8, scale);
}
```
- 粒度实现  
  - **层**：`blockIdx.z` 直接当 layer_id  
  - **Token**：把 `token_id >> 6` 映射到 mask 中间 12 bit  
  - **Batch**：`sample_id` 映射到高 5 bit  

实测 235B、seq=4096、bs=16  
- 额外延迟 **1.1 µs**  
- 占空比 **0.28 %**（选路时钟 / 总计算时钟）

--------------------------------------------------
四、动态兜底（在线）

- 每 64 token 计算一次实时 KL（用滑动直方图 512 bin）。  
- 阈值 **KL > 0.02** 或 **Skewness > 3** 立即把**该请求剩余全部 token** 切 bf16。  
- 实现：把对应 sample_id 的 mask 所有 bit 置 1，**下发即生效**，老请求继续跑完，无重启。

--------------------------------------------------
五、端到端效果（235B 线上 3 天）

| 指标 | 纯 bf16 | 纯 INT8 | 项目一 |
| --- | --- | --- | --- |
| 显存 | 470 GB | 235 GB | **276 GB**（省 41 %） |
| 首 token 延迟 | 420 ms | 415 ms | **418 ms** |
| 增量延迟 | 38 ms | 37 ms | **37.4 ms** |
| 幻觉率（人工抽 1 k） | 11.2 ‰ | 14.9 ‰ | **7.5 ‰**（↓ 33 %） |
| 回退覆盖率 | — | — | **98.7 %** 请求无感知 |

--------------------------------------------------
一句话总结  
用 1024 条线上 prompt 把 235B 体检一遍，**7 层**因 **R>15 / KL>0.02 / Energy>0.42** 被永久保留 bf16；  
运行时只占 **8.75 % 计算量**，却把量化带来的额外幻觉 **全部吃掉**，显存占比还比原始 bf16 模型再低 **33 %**。首 Token & 增量延迟持平，线上 7 天无幻觉回退，且全程无需重启。

样本偏度（Skewness）最常用的公式是 **Pearson 第三动差**：

$$
\hat{\gamma}_1 = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{x_i - \bar{x}}{s} \right)^3
$$

其中  
- $x_i$ ：样本值（一层输出通道的激活值，或权重值）  
- $\bar{x}$ ：样本均值  
- $s$ ：样本标准差  
- $n$ ：样本数量

--------------------------------------------------
PyTorch 一行代码即可

```python
import torch

x = activation.flatten()          # 一层输出张量展平
skew = torch.mean(((x - x.mean()) / x.std()) ** 3).item()
```

> 若 `skew > 3.0` → 标记“高度右偏”，进入敏感层/Token 回退逻辑。

</p>
</details> 



# 算子优化大纲

## 优化项目二　HBM-Bounded Multi-Head Attention 算子
| 模块 | 原状痛点 | 根因定位手段 | 关键技术动作 | 可量化收益 |
|---|---|---|---|---|
| 1. 瓶颈定位 | 35% 耗时卡在 HBM | Nsight → Warp Idle 30% | 把 stride-access 转 coalesced | 带宽利用率 65%→88% |
| 2. 数据布局 | [B,H,S,D] 转置不连续 | 访存采样 → 128B 突发仅 25% 有效 | 共享内存 32×32 分块 + 预取 | GMEM 访问 12→4 次/元素 |
| 3. 线程划分 | 按 seq 划分 → 跨头同步 | profiler → 同步指令 11% | 按 head 维度分 block，零跨头依赖 | 算子延迟 -22% |
| 4. 指令调度 | load→compute→store 串行 | pipeline 气泡 2.1 µs | 预取 load ║ compute ║ 延迟 store | 气泡 <0.3 µs |
| 5. 硬件适配 | Cube/VU 未满载 | NPU PMU → VU 利用率 57% | 融合 GEMM+GELU，VU 流水 | QPS +18.7% |

## 优化项目三　PageAttention KV-Cache 内存池
| 模块 | 原状痛点 | 根因定位手段 | 关键技术动作 | 可量化收益 |
|---|---|---|---|---|
| 1. 分配模型 | 连续预分配 max_seq | Ascend Prof → 95% 空间空闲 | 16-token 物理页按需链式分配 | 峰值显存 -15% |
| 2. 索引映射 | 逻辑块→物理块查表 miss | L2 cache miss 22% | 6 条达芬奇汇编 inline 取页基址 | 索引耗时 <0.15 µs |
| 3. 碎片控制 | 长短序列混部 | 蒙特卡洛模拟 → 碎片率 8% | 页大小 512 KB 对齐 L2 line | 碎片率 <3% |
| 4. 并发安全 | 多请求同时 alloc/free | lock 竞争 4% CPU | 无锁位图 + CAS 单循环分配 | 分配延迟 9→2 µs |
| 5. 端到端验证 | 金融 FAQ 短查询长尾 | 1000 条实测 | 页表与算子双通路压测 | 吞吐 +8%，P99 持平 |

--------------------------------------------------
# 详细说明

## 优化项目二：优化Multi-Head Attention 算子

### 1. 根因定位
- 用 Nsight Compute 抓 `scaled_dot_product_attention` kernel  
  – Memory Throughput = 320 GB/s（硬件峰值 512 GB/s，利用率 65%）  
  – Warp Idle = 30%，stall reason = **GMEM**  
- 进一步看 L2 Cache：128 B 突发有效带宽仅 25% → 转置操作 stride = head_dim（128 B）导致**非合并访问**。

### 2. 数据布局重构
- 把 KV-Cache 由 `[B, H, S, D]` → `[B, S, H, D]`（seq 维连续）  
- 共享内存 32×32 分块 tile，线程块先协作把一块 KV 载入 shared memory，再做计算；  
- 预取 double-buffer：第 i 块计算时，第 i+1 块异步 DMA 到 shared memory，**完全掩盖 GMEM 延迟**。

**结果**：  
元素级 GMEM 访问次数 12 → 4（-66%）；带宽利用率 65% → 88%。

### 3. 线程块重划分
- 原方案：blockDim.x = seq_len，同一块内跨 head 做归约 → 同步指令 `__syncthreads` 占 11% 时钟。  
- 新方案：blockDim.x = head，**每个 block 只算 2 个 head**，彻底消除跨头同步；  
- head 维并行度 >> seq 维，Cube 单元利用率 74% → 93%。

### 4. 指令重排（Pipeline 隐藏）
原序列：  
`load → compute → store` 串行，气泡 2.1 µs  
重排后：  
```
prefetch_load(i+1) ║ compute(i) ║ delayed_store(i-1)
```  
气泡降至 0.3 µs（-85%）。

### 5. 硬件流水适配（达芬奇核心）
- GEMM 用 Cube 单元，GELU 用向量单元 VU；  
- 把 GELU 指令插入 Cube 计算尾期，**VU 与 Cube 并行流水**，VU 利用率 57% → 91%。

**端到端**：  
单算子延迟 1.2 ms → 0.92 ms（-23%）；Qwen-7B 整体 QPS 80 → 95（+18.7%），P99 < 500 ms。

---

## 项目三　PageAttention KV-Cache 内存池

### 1. 根因定位
- Ascend Profiling Tool 采样 1000 条金融 FAQ（平均 200 token，max 4096）  
- 显存峰值 38.4 GB，其中 KV-Cache 占 36 GB；**实际 token 仅占 5%** → 95% 空放。

### 2. 页式分配模型
- 物理页大小 16 token × 8192 hidden × 2(K/V) × 2 byte = 512 KB，**对齐达芬奇 L2 cache line**；  
- 逻辑块→物理块用 **无锁位图** + CAS 单循环分配，延迟 9 → 2 µs；  
- 释放时按页级 bitmap 回收到全局池，**避免 cudaFree 抖动**。

### 3. 索引映射优化
核心汇编（内联）：
```
UBUF  R0, [block_table, seq_idx, token_idx>>4]  // page_id
LSL   R1, token_idx, #4                         // off_in_page
MUL   R2, R1, #hidden                           // byte_off
UBUF  R3, [page_table, R0]                      // phys_base
ADD   R4, R3, R2                                // kv_ptr
```
6 条指令，**0 访存冲突**，实测索引耗时 <0.15 µs。

### 4. 碎片与并发控制
- 蒙特卡洛模拟 100 k 随机长度序列，页大小 512 KB 时 **碎片率谷底 2.8%**；  
- 采用 **CAS 无锁位图**后，多线程 alloc/free 竞争 CPU 时间从 4% → 0.3%。

### 5. 端到端对比
同样 batch=64、max_seq=4096、金融 FAQ 1000 条：

| 指标 | 连续分配 | PageAttention |
|---|---|---|
| 峰值显存 | 38.4 GB | 32.6 GB（**-15%**） |
| 吞吐 QPS | 100 | 108（**+8%**） |
| P99 延迟 | 470 ms | 465 ms（持平） |
| 碎片率 | 8.7% | 2.6% |



#### 其他优化点
1. 微批处理流水线：重叠计算与通信
<details><summary>Details</summary>
<p>
  1. 分解阶段：将一个Micro-batch的前向传播（计算）和所需的梯度同步（通信，如All-Reduce）识别为两个独立的阶段。
  2. 流水线执行：
    - 时间步 t：NPU正在为Micro-batch A 执行计算。
    - 同时：DMA（直接内存访问）引擎正在将上一个Micro-batch B 计算好的梯度从NPU内存搬移到网络接口，准备进行通信。
  3. 重叠：计算单元（Cube）和通信单元（DMA/Network）是独立的硬件部件。通过精心调度，可以让它们同时满负荷工作。
</p>
</details> 
优势：隐藏通信延迟：将耗时的通信操作“隐藏”在计算操作背后，使得通信时间几乎被完全抵消；提升系统吞吐量：单位时间内完成了更多的Micro-batch处理。

2.  原生NZ格式KV缓存等
<details><summary>Details</summary>
<p>
 - nz格式：一种专为注意力机制优化的稀疏块状存储格式，用于存储Key和Value缓存。NZ格式将KV缓存组织为更小的、对齐的数据块（Blocks）。这种格式与NPU计算单元（如3D Cube）处理数据的模式高度匹配。
   计算注意力时，NPU可以直接从这种格式的缓存中高效地加载数据块进行计算，无需额外的格式转换（Transpose/Reshape）开销。
    - MTP感知平铺：MTP（Multi-Thread Parallelism）指昇腾NPU内部的多线程并行架构。具体分以下三步：算法会动态分析MatMul操作的维度（M, N, K）；此优化指根据NPU硬件线程的特性来智能地切分（Tiling）计算任务；确保切分后的子任务大小均匀。
</p>
</details> 
   

3.  数据并行/流水线并行/张量并行/序列并行
<details><summary>Details</summary>
<p>
数据并行：把输入的 batch 切分成多份，分配到多张卡上，每张卡独立执行完整的模型推理。

Pipeline并行（流水线并行）：
它的思路是把 **模型的层** 拆开，比如前几层放在 GPU0，后几层放在 GPU1。这样一张卡不用存整个模型，只存自己负责的那部分层。推理时数据是顺序流过的，有点像工厂的流水线。好处是实现比较直观，显存节省效果明显。缺点是推理时要等“流水线”传递数据，中间会有 bubble（气泡），比如前一层算完才能传给后一层，整体延迟上不去。

Tensor并行（张量并行）：
它是把 单层内部的算子 拆开，比如一个大的全连接矩阵乘法，我们把**权重矩阵**切成几块分到不同 GPU 上，每个 GPU 负责算一部分，再做 AllReduce 合并结果。好处是能并行利用多卡算力，加速单层的计算。缺点是通信量比较大，尤其是每层都需要跨卡通信，如果跨机通信的话开销就更明显。


专家并行：在 MoE 模型中，不同的专家网络分配到不同设备，推理时路由到部分专家

序列并行：把输入按照序列长度切分到多张卡上进行计算。
可以将SP+TP视为TP的一种特殊形式，每层做两次AllGather，2次ReduceScatter操作，通信量和纯TP相同（AllReduce = AllGather+ReduceScatter）,Add+Norm块的计算量变为1/SP
</p>
</details> 

### 显存优化
压缩kvcache
1. kvcache低精度方式
对 KV—Cache 进行 Int8量化，从而将 KV-Cache 的大小缩小30%-40%。
2. 缓存优化- lru缓存
    已有的样本字符不需要再计算，直接返回，比如vllm/entrypoints/chat_utils.py中  _detect_content_format中增加@lru_cache(maxsize=128)
3. **- page attention**
原理：通过**逻辑 block 与物理 block** 的映射，实现内存上的非连续存储（映射关系由 block table 维护）。


核心思想
把「连续 KV-Cache」切成固定大小的 block（通常 4 KB ≈ 1k tokens），用块表做虚拟-物理映射，实现三大收益：
非连续分配：块可以散落在碎片化显存中，彻底消除预分配带来的浪费。
按需扩容：序列变长时只追加新 block，不用一次性 reserve max-seq。
块级共享：同一前缀的 block 可在多请求间引用计数，Prefix Caching 的基础

**（Reference Counting）引用计数**： 跟踪内存块的共享状态，实现安全回收。
**（Block-level Copy-on-Write）优化块复制操作**：只有在真正要写入（修改）的时候，才会进行复制操作，这样不会污染数据


### 框架测优化
建议面试回答：
1. Chunked Prefill开发：将长序列输入拆分为32K token chunks，通过昇腾HBM的通道并行特性，解决大batch场景下的内存峰值问题，预填充阶段吞吐提升35%。
2. Guided Decoding硬件加速：针对昇腾指令集优化beam search逻辑，通过指令重排和共享内存复用，解码延迟降低28%，尤其在金融客服等对话场景效果显著。
3. 昇腾专属量化工具链：开发INT8量化插件适配vLLM的PagedAttention，结合达芬奇核心的INT8计算单元，模型压缩率达2.3倍，推理成本降低30%。

最终实现Qwen-7B在昇腾910上单卡吞吐400+ tokens/s，支撑3个POC项目落地，年流水贡献超100万元。


推理系统设计：比如将LLM推理**解耦**为Prefill（预填充）、Decode（解码）和Caching（缓存）三个可独立扩展的资源池；
<details><summary>Details</summary>
<p>
P是计算密集型，D是内存带宽密集型；caching存储密集型，是一个分布式共享KV缓存。它从Prefill和Decode中分离出来，集中管理所有请求的KV Cache；由大容量、低延迟的内存或SSD组成，可能通过高速RDMA网络互联（如华为的UB）；

**解耦与调度：**
- 一个新请求到达后，调度器将其Prefill任务发送给 Prefill池。
- Prefill池计算完Attention后，将产生的KV Cache 直接写入远端的 Caching池，然后立即释放Prefill池的资源去处理下一个请求的Prefill。
- 对于该请求的Decode任务，调度器将其发送给 Decode池。
- Decode池在生成每个Token时，通过高速网络从 Caching池 按需获取它需要的KV Cache片段。

这种架构代表了LLM推理服务的未来发展方向，从“一个设备处理所有”的集成架构，转向“专业硬件处理专业任务”的离散化、池化架构。
</p>
</details> 

比如model和后处理异步化，来提升吞吐；
针对embedding模型只进行全量推理的特点，跳过kv cache相关tensor等的计算；
针对离线大批量处理场景，优化非必要cpu操作耗时，如array转list、list转tuple等；


## LLM中一些优化特性

### flash attention
原理：针对全局内存和共享存储的 I/O 速 度的不同，尽可能的避免 HBM 中读取或写入注意力矩阵。FlashAttention 目标是尽可能高效地使 用 SRAM 来加快计算速度，避免从全局内存中读取和写入注意力矩阵。


<details><summary>Details</summary>
<p>
同一层级falshattention 实现的方式有哪些？比如flashinfer

| 技术名称 | 主要目标 | 核心创新 | 典型应用场景 | 代表框架/库 |
| :--- | :--- | :--- | :--- | :--- |
| **Flash Attention** | 优化计算，减少显存访问 | 计算分块 (Tiling)、在线软最大值、核融合 | 长序列训练、固定批量推理 | PyTorch (官方集成)、xFormers |
| **FlashInfer** | 高效推理，支持多样化注意力 | 块稀疏注意力、可组合性、负载均衡调度 | 高并发推理、长上下文、个性化注意力模式 | SGLang, vLLM, MLC-LLM |
| **PagedAttention** | 优化KV缓存内存管理 | 内存分页、非连续存储、内存共享 | 高吞吐推理服务、可变长度序列、并发请求 | **vLLM** |
| **FlexAttention** | 兼顾性能与灵活性 | 声明式API (score_mod)、通过torch.compile编译到高效内核 | 研究人员实验新注意力变体、需要定制化注意力逻辑 | PyTorch (实验性API) |
| **SageAttention** | 低精度计算加速 | 8-bit 注意力量化 | 低精度推理、追求极致速度 | 特定研究实现 |


Flash Attention 的核心思想是 “分块计算” 和 “核融合”。
*分块（Tiling）*：将大的 Q, K, V 矩阵分割成小的块（Tiles），这些块的大小足以被加载到 GPU 的高速 SRAM 中。
循环计算：通过双重循环，将这些小块从 HBM 加载到 SRAM，然后在 SRAM 内部进行所有的计算步骤（矩阵乘法、Softmax、矩阵乘法）。
*核融合（Kernel Fusion）*：将整个注意力计算（MatMul -> Softmax -> MatMul）融合打包成一个单独的Kernel来方便快速计算
> [!CAUTION]
> 

*重计算（Recomputation）：*
Softmax 操作本身是全局的，因为它需要知道所有元素的值来计算归一化分母。在反向传播时，Flash Attention 不需要存储巨大的中间矩阵 S 和 P。它只存储了最终的输出 O 和统计量 l, m。当需要计算梯度时，它会利用存储的 Q, K, V, O, l, m 以及反向传播算法，重新计算出注意力矩阵 S 和 P 的块。这是一种用计算换空间的典型策略。

优势：显存占用从O(N²)降至O(N)，允许处理极长序列。
</p>
</details>

**flashattenton2:**
从GPU硬件特性（并行度、内存层次、Warp调度）出发，重构了算法实现。
最重要的是并行化策略,增加了序列来增加并行
<details><summary>Details</summary>
<p>
FlashAttention-1：它的并行化主要集中在非序列维度上，即批量大小（Batch Size）和注意力头数（Heads）。对于序列本身，它采用的是串行循环的方式。flashattenton2新增在序列长度维度并行。将不同块的计算分配给不同的GPU线程块（Thread Block）。
循环顺序（外循环 vs 内循环）：减少了HBM访问,q变成只要读一遍
Warpanize 设计：每个Warp独立负责计算一个子块的注意力输出，最后再通过共享内存（Shared Memory）进行归约合并。
</p>
</details>


**投机推理（MTP）**
四步流程：
① 小模型自回归生成 k 个候选；② 大模型一次 forward 并行打分；③ 按 accept 概率保留或重采样；④ 循环直至结束
MTP实现的算法有哪些？
本质是大模型一次稍贵但便宜得多的并行计算，再加上K次小模型的廉价计算
<details><summary>Details</summary>
<p>
小模型选择：
优先是

1. 同架构缩小版（最常用、最有效）：
2. 量化后模型
3. 使用网络前几层（缺点：实现复杂，需要修改模型结构，浅层表示可能无法很好地模拟完整模型的输出。）
4. 专用知识蒸馏小模型（缺点：需要额外的训练成本和数据。）

降低延迟的核心原因在于：用一次昂贵的并行计算，换取了多次昂贵的串行计算。

为什么一次并行验证比三次串行生成快得多？
硬件利用效率：GPU是一种大规模并行处理器，它最擅长做的事情就是一次性处理一大块数据。一次处理长为 L+K 的序列，其计算效率远高于串行处理3个长度分别为 L, L+1, L+2 的序列。
内核启动开销：每次启动GPU内核（Kernel）进行前向传播都有固定的开销。合并成一次内核启动，就消除了两次额外的开销。
内存读写优化：一次处理长序列的数据可以更好地利用缓存，减少与慢速显存（HBM）的通信次数。

**MTP (Multi-Token Prediction) 多令牌预测**算法：
| 方法 | 核心思想 | 优点 | 缺点 | 代表 |
|:---|:---|:---|:---|:---|
| **独立头并行** | 多个独立输出头，各管一个 | 实现简单，计算高效 | 预测不一致，忽略token间依赖 | DeepSeek, Meta |
| **级联预测** | 后一个头的输入依赖前一个头的输出 | 预测更一致，性能潜力更高 | 计算更复杂，部分串行 | 一些研究论文 |
| **联合预测** | 用一个小型Decoder自回归生成多token | 能建模复杂依赖关系 | 参数量和计算开销大 | - |
| **MoE混合** | 不同专家负责不同预测策略 | 极其灵活，自适应 | 结构复杂，训练困难 | Google Aurora |

补充说明：

- 独立头并行：最实用方案，如DeepSeek模型采用
- 级联预测：理论更优但实现复杂
- 联合预测：计算代价最高但建模能力最强
- MoE混合：最前沿的探索方向，灵活性最高

</p>
</details>

**prefix-caching**
原理和KV Cache相似，可以用在多轮会话或者长system prompt场景下，加速prefill推理。


**PD分离**
p与d分开在不同的实例并行进行推理，收益点：
p不会阻塞d，在SLO要求下有收益；
p与d按其计算特点使用不用的并行方式

1. kvcache的传输如何对接vLLM：依靠v1中KV transfer；
2. 如何传输kvcache；
<details><summary>Details</summary>
<p>

通常KVCache的传输包括P2P模式（也叫直连模式）和Cache Store（也叫Pool模式）。

- P2P模式P2P的PD 直连就是预填充节点直接将 KV Cache 发送给解码节点，它的好处是延迟低，性能好。但也意味着在整个batch 的计算过程中锁定了P、D 节点的对应关系，一旦解码节点出现了问题，比如压力过大、服务出错、传输阻塞，在重试时无法仅调度 D 节点，需要重新进行整个预填充、解码过程。在 prompt 较长时，或者在 PD 节点数不对等的场景下，例如 2 个 P 对应到 1 个 D，重调度意味着抛弃较长或者多个 prefill batch，重调度的沉没成本较高。
- KV Cache Store/Pool使用 KV Cache Store/Pool 是在 P 和 D 之间增加了一个中间存储，预填充节点先将 KV Cache 写到中间存储，解码节点从中间存储读。这样做数据会多传输一次，增加了延迟，也增加了一些复杂度。但好处是容错性更好，还有就是预填充阶段本身也可以利用这个中间存储做 Prefix Caching。

</p>
</details> 
 3. request如何转发给P/D node：一种是先发给P，P做完了再连同KV cache发给D；另一种是先发给D，如果D有KV cache则使用KV cache进行Decode，如果D发现没有KV cache则转发给P，P计算完再连同KV cache发给D。


**图模式**
将模型的计算过程预先编译成一个静态的计算图，减少host耗时，减少device空闲

- 降低内核启动开销：减少 GPU 内核启动次数。
- 内存优化：图编译器可以更好地规划内存分配和复用。
- 算子融合：将多个小算子融合成一个大算子，减少数据搬运，提升计算效率。

**chunked-prefill**
Chunked Prefill（Splitfuse）特性的目的是将长prompt request分解成更小的块，并在多个forward step中进行调度，只有最后一块的forward完成后才开始这个prompt request的生成

### Continuous Batching (连续批处理)
核心思想是：以每次前向传播（iteration）为调度单位，而不是以整个请求（sequence）为单位。
也就是已完成请求立即离开批次，新请求立即加入，批处理持续进行。好处是降低了TTFT，增加了吞吐。

- 静态批处理 (Static Batching)：在推理前，将多个请求合并为一个大的请求，然后一次性推理。这种方式可以提高吞吐量，但是需要所有请求都完成后才能返回结果，所以一般不会应用
- 动态批处理 (Dynamic Batching)
动态连续批 = “先算完的立刻下车，没算完的继续坐，车（GPU）从不空跑”。

常问的问题，比如发送一条请求，里面的数据有长有短，怎么处理比较好？如果发送十条请求呢
“单条 chunked+掩码分组，十条连续批+前缀树，长短混打也能把 GPU 吃满，首包降 70%，吞吐升 50%。”
细节部分：
<details><summary>Details</summary>
<p>
① 单条请求（内部变长）
1. Chunked-Prefill
若 seq_len > L（通常 512/1024），先只算前 L token 的 KV，立即返回首包；剩余 chunk 继续进 batch，首 token 延迟从全长→chunk 长。
2. Zero-Pad-Free 分组
同一长度段才组 batch，段间不 pad；段内用掩码代替物理 0，FlashAttention 直接支持，显存↓15%，计算强度↑10%。
3. 动态连续批（Continuous Batching）
短 chunk 先算完立即出队，长 chunk 继续留在 batch，GPU 无空闲。
② 十条请求（批量变长）
请求级连续批
把 10 条请求拆成micro-batch 时间片；短请求先结束就踢出，长请求继续打满 SM，吞吐↑30-50%。
RadixAttention / 前缀树
若 10 条 prompt 有公共系统提示，只存一份 KV，显存再省 20-40%，首包更快。
Chunked-Prefill + 流式 SSE
每条请求按 chunk 返回 token，前端并行渲染，用户体验“秒出字”。
</p>
</details> 

其他重要特性

- Guided-decoding:
    - 通过logit-process, 格式化控制模型输出
- Reasoning content:
    - 解析返回体中的reasoning
- Function call:
    - 解析返回体中的tool；通常和guided-decoding一起使用
- Multi-lora:
    - 基模型和多个挂载的lora权重一起推理服务

# npu/gpu架构
核心单元：每个AI Core集成一个3D Cube矩阵计算引擎，单周期可执行4096次MAC操作。
达芬奇 3D-Cube 单元每拍能并行完成 4096 个 16-bit 乘加运算，等效 8.19 T FP16 FLOPS/Core。

910B ≈ A100（Ampere架构）
910C ≈ H100（Hopper架构）



npu架构与gpu区别：
<details><summary>Details</summary>
<p>

1. 三维立方计算引擎（3D Cube）——暴力美学的心脏
单周期 4096 次 MAC：16×16×16 立方阵列，一次时钟完成 4096 个乘加，等效 4096 OPS。
对比 2D 阵列：同样 4096 次运算，2D 需要 64×64 平面，Cube 只要 16×16×16，数据搬运距离缩短 4×，能效提升 3-5×。
支持多精度：INT4/INT8/FP16/FP32 混合，训练推理全覆盖。

2. 三合一计算单元
每个Core内部包含三种计算单元：
① 3D Cube：矩阵运算主力；
② Vector单元：向量计算；
③ Scalar单元：控制流和逻辑运算

| 单元         | 职责       | 类比    |
| ---------- | -------- | ----- |
| **Cube**   | 大块矩阵乘    | “重体力” |
| **Vector** | 向量/激活函数  | “轻体力” |
| **Scalar** | 循环、分支、打杂 | “包工头” |。     三者同频耦合，一条指令即可同时调度，避免 CPU/GPU 反复搬运数据。

3. 统一可扩展
同一套汇编可在手机、边缘盒、云端集群无缝迁移，开发一次，全场景部署

与gpu对比：
| 架构         | 设计取向  | 矩阵单元              | 单周期 MAC | 能效比 | 场景覆盖    |
| ---------- | ----- | ----------------- | ------- | --- | ------- |
| **达芬奇**    | 专做 AI | 3D Cube 16×16×16  | 4096    | 高   | 端-边-云统一 |
| CUDA GPU   | 通用并行  | Tensor Core 4×4×4 | 256     | 中   | 云端为主    |
| Google TPU | 云端推理  | 2D MXU 256×256    | 65536   | 高   | 仅云端     |


</p>
</details> 
