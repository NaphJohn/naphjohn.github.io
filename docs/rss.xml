<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>kaikai的博客</title><link>https://naphjohn.github.io</link><description>记录深度学习，大模型中技术感悟</description><copyright>kaikai的博客</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://avatars.githubusercontent.com/u/46772304?v=4</url><title>avatar</title><link>https://naphjohn.github.io</link></image><lastBuildDate>Fri, 05 Dec 2025 02:14:03 +0000</lastBuildDate><managingEditor>kaikai的博客</managingEditor><ttl>60</ttl><webMaster>kaikai的博客</webMaster><item><title>论文精读</title><link>https://naphjohn.github.io/post/lun-wen-jing-du.html</link><description>1. SepLLM通过标点符号优化kv cache
2. 。</description><guid isPermaLink="true">https://naphjohn.github.io/post/lun-wen-jing-du.html</guid><pubDate>Thu, 02 Oct 2025 01:53:00 +0000</pubDate></item><item><title>推理部分</title><link>https://naphjohn.github.io/post/tui-li-bu-fen.html</link><description>sample异步优化、add_model_outputs、prepare &amp; scheduler
scheduler、prepare、sample、add_model_outputs 4个模块耗时占比&gt;30%;
sample已优化一版，其他模块需要再分解
1.前后处理中添加断点，打印时间。</description><guid isPermaLink="true">https://naphjohn.github.io/post/tui-li-bu-fen.html</guid><pubDate>Tue, 30 Sep 2025 08:38:33 +0000</pubDate></item><item><title>Deepseek创新</title><link>https://naphjohn.github.io/post/Deepseek-chuang-xin.html</link><description>最新deepseek创新：
DeepSeek 在 V3.2-Exp 里把 DSA（DeepSeek Sparse Attention）做成**“一套可插拔的细粒度动态稀疏框架”**，核心优化可以概括为 4 句话、8 个关键词，对应 4 组实测收益——全部在 128 K 长度下验证，且**零 checkpoint 转换、即插即用**。</description><guid isPermaLink="true">https://naphjohn.github.io/post/Deepseek-chuang-xin.html</guid><pubDate>Tue, 30 Sep 2025 02:05:10 +0000</pubDate></item><item><title>cude &amp; TileLang算子编写</title><link>https://naphjohn.github.io/post/cude%20%26%20TileLang-suan-zi-bian-xie.html</link><description>1. 算子命名
2. rag内容read


gpu的hopper架构指的是：
Hopper = 4N 工艺 + 800 亿晶体管 + FP8 Transformer 引擎 + 900 GB/s NVLink，把大模型训练/推理的矩阵乘、通信、内存带宽全部拉满，相对 Ampere 实现 3-9 倍 AI 提速

编写工具：

| 工具           | 语言           | 硬件                  | 成熟度   | 典型场景                   | 2025 市占  |
| ------------ | ------------ | ------------------- | ----- | ---------------------- | -------- |
| **CUDA**     | C/C++        | NVIDIA only         | ★★★★★ | 手写 peak kernel、驱动库     | 70%      |
| **CUTLASS**  | C++ template | NVIDIA only         | ★★★★☆ | cuBLAS 底层、厂商库          | 60%      |
| **Triton**   | Python       | NVIDIA + AMD ROCm   | ★★★★☆ | LLM 推理/训练 fused kernel | 40%      |
| **TileLang** | Python       | NVIDIA + AMD + 沐曦\* | ★★★☆☆ | 研究原型→产品 kernel         | 5% 但增速最快 |

1. TileLang = Python 前端 + TVM 编译器 + Tile 级原语，让你用 1/3 代码量写出匹配手写汇编的算子，Hopper/Ada/AMD 通用，是 2025 年最值得关注的高性能 AI Kernel DSL
https://github.com/tile-ai/tilelang



# 量化
        ┌-------------------- 一层 Transformer --------------------┐
        |                                                        |
        |  1)  Pre-Norm                                        |
        |  2)  Attention (Q/K/V)                               |
        |  3)  Post-Attention Norm                             |
        |  4)  MoE-FFN Block                                   |
        |        ┌-----------------------------┐                |
        |        │  a)  Gate  (Linear)         │◄-- FP16 保留  |
        |        │  b)  Softmax + Top-K        │                |
        |        │  c)  Router / Load-Balance  │                |
        |        │  d)  Expert-0  (Linear×2)   │◄-- W8A8 量化  |
        |        │  e)  Expert-1  (Linear×2)   │                |
        |        │  ...                        │                |
        |        │  f)  Weighted Add (Top-K)   │                |
        |        └-----------------------------┘                |
        |                                                        |
        └--------------------------------------------------------┘

  输入校准集
      │
      ▼
[Anti-Outlier 预处理]
      │  1) 跑前向 → 记录 99.9 % 分位
      │  2) 生成 clamp_thr
      ▼
  激活 tensor ──► clamp(x, -thr, thr) ──► 计算 scale ──► 量化
      │                                              ▲
      ▼                                              │
  权重 tensor ──► per-channel scale ──► 直接 int8 ──┘
      │
      ▼
  逐层校准 (MSE)  ←-- 50 条 calib prompt
      │
      ▼
  导出混合策略：
      ├── gate 层 (Linear)          → **FP16**  （不量化）
      ├── expert-0/1/... down/up    → **W8A8 dynamic**
      └──其余投影层                 → **W8A8 static**

gate 只占 MoE 层里“一根红线”大小的 Linear，但它是决定 token 走哪条专家的投票器；保持 FP16 即可让雪崩风险归零，而内存代价可忽略不计。</description><guid isPermaLink="true">https://naphjohn.github.io/post/cude%20%26%20TileLang-suan-zi-bian-xie.html</guid><pubDate>Mon, 29 Sep 2025 11:22:34 +0000</pubDate></item><item><title>大模型的前世今生</title><link>https://naphjohn.github.io/post/da-mo-xing-de-qian-shi-jin-sheng.html</link><description># Encoder-only模型

bert模型：
BERT（Bidirectional Encoder Representations from Transformers）= “只有 Transformer 编码器” 的预训练语言模型。</description><guid isPermaLink="true">https://naphjohn.github.io/post/da-mo-xing-de-qian-shi-jin-sheng.html</guid><pubDate>Mon, 22 Sep 2025 07:11:37 +0000</pubDate></item><item><title>C++进阶</title><link>https://naphjohn.github.io/post/C%2B%2B-jin-jie.html</link><description># C++简介
C++ 是一种静态类型的、编译式的、通用的、大小写敏感的、不规则的编程语言，支持过程化编程、面向对象编程和泛型编程。</description><guid isPermaLink="true">https://naphjohn.github.io/post/C%2B%2B-jin-jie.html</guid><pubDate>Mon, 22 Sep 2025 03:38:55 +0000</pubDate></item><item><title>简历对应问题</title><link>https://naphjohn.github.io/post/jian-li-dui-ying-wen-ti.html</link><description>## 昇腾 NPU 深度优化


## 大模型推理通用方法论
#### • 客户要求 99.9% 的请求 P99 延迟 &lt; 200 ms，但模型参数量翻倍，你会先做哪三项优化？
客户要求 99.9% 的请求 P99 延迟 &lt; 200 ms，但模型参数量翻倍，你会先做哪三项优化？
A：

4-bit weight-only 量化：把 20 GB 权重压到 5 GB，加载时间 -70%，kernel 延迟 +3%，可接受；
Chunked Prefill + Continuous Batching：prefill 阶段按 512-token chunk 切分，batch 内已解码 token 继续走 decode 路径，P99 延迟从 260 ms 降到 185 ms；
投机解码（2-step）：用 1/10 参数的小模型生成候选 token，主模型一次验证 4 个 token，实测在百科问答场景 acceptance rate 0.72，整体延迟再降 22%。</description><guid isPermaLink="true">https://naphjohn.github.io/post/jian-li-dui-ying-wen-ti.html</guid><pubDate>Mon, 15 Sep 2025 15:49:42 +0000</pubDate></item><item><title>SGlang推理引擎</title><link>https://naphjohn.github.io/post/SGlang-tui-li-yin-qing.html</link><description>## SGlang框架
1. **前端语言**：
   - 提供原生成API接口，可以直接与本地模型交互和调用。</description><guid isPermaLink="true">https://naphjohn.github.io/post/SGlang-tui-li-yin-qing.html</guid><pubDate>Tue, 09 Sep 2025 12:10:55 +0000</pubDate></item><item><title>算子测精度差异</title><link>https://naphjohn.github.io/post/suan-zi-ce-jing-du-cha-yi.html</link><description>## 不同硬件精度不同底层原因（如NPU VS GPU）
1. 浮点数计算的非确定性 现代处理器为追求极致性能，广泛采用并行计算与融合运算（FMA），导致浮点运算顺序不固定 由于浮点数运算不满足严格的结合律 (a+b)+c ≠ a+(b+c)，不同的计算顺序会导致微小的舍入误差累积，最终造成结果差异。</description><guid isPermaLink="true">https://naphjohn.github.io/post/suan-zi-ce-jing-du-cha-yi.html</guid><pubDate>Tue, 09 Sep 2025 07:05:38 +0000</pubDate></item><item><title>深度学习基础</title><link>https://naphjohn.github.io/post/shen-du-xue-xi-ji-chu.html</link><description># CNN

**关键组件：**
  - 卷积层： 提取局部特征，使用滤波器（Filter）滑动计算。</description><guid isPermaLink="true">https://naphjohn.github.io/post/shen-du-xue-xi-ji-chu.html</guid><pubDate>Tue, 09 Sep 2025 03:24:30 +0000</pubDate></item><item><title>多模态训练推理</title><link>https://naphjohn.github.io/post/duo-mo-tai-xun-lian-tui-li.html</link><description>## Diffusion Models 扩散模型
核心思想是逐步的数据生成过程。</description><guid isPermaLink="true">https://naphjohn.github.io/post/duo-mo-tai-xun-lian-tui-li.html</guid><pubDate>Tue, 09 Sep 2025 01:39:45 +0000</pubDate></item><item><title>linux</title><link>https://naphjohn.github.io/post/linux.html</link><description>### gpu上建立容器
```powershell
nvidia-docker run -it \   （这边也可以使用docker run -it）
docker run -it -u root \
--gpus all  \
-p 1034:1034 -p 2735:2735 \
-e NVIDIA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
--ulimit memlock=-1 \
--name whk_vllm_091_0728 \
-v /data/:/data/ \
-v /home/model/:/home/model/ \
ee0767a44255 bash

#挂载命令
mkdir -p /nfs-data ; mount -t nfs -o vers=3,timeo=600,nolock 10.170.23.193:/ /nfs-data
``` 

### 初始化设置
```python
vim ~/.bashrc

export HISTSIZE=1000
export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export https_proxy=http://10.155.192.138:8080
``` 

### 修改pip源
```powershell
vim ~/.pip/pip.conf
[global]
index-url = http://7.223.199.227/pypi/simple
trusted-host = 7.223.199.227
timeout = 120

#pip install torch==2.5.1  --default-timeout=1000 -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
``` 

### 多机启动
```powershell
ray：# 指定通信网卡，使用ifconfig查看，找到和主机IP一致的网卡名
export GLOO_SOCKET_IFNAME=enp67s0f5
export TP_SOCKET_IFNAME=enp67s0f5
export RAY_EXPERIMENTAL_NOSET_ASCEND_RT_VISIBLE_DEVICES=1
export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

export MM_ALL_REDUCE_OP_THRESHOLD=1000000
export HCCL_OP_EXPANSION_MODE='AIV'
export NUMEXPR_MAX_THREADS=192

# 将其中一个节点设为头节点
ray start --head --num-gpus=8
# 在其他节点执行
ray start --address='7.216.55.58:6379' --num-gpus=8
``` 


### 远程链接容器
```powershell
#配置ssh
#第一步config文件
vi /etc/ssh/sshd_config
PermitRootLogin yes
PasswordAuthentication yes
#第二步建立/run/sshd
mkdir /run/sshd

#第三步确认没有sshd时候设置passwd
passwd #这个时候别有sshd
#第四步开启sshd
/usr/sbin/sshd
ssh 7.242.105.173 -p 8035 #来确认是否链接成功

#解决上面报错，生成对应ssh
ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key -P '' -q
ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key -P '' -q
ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key -P '' -q

#不需要ssh命令
git config --global http.sslVerify false

#支持自动迁移代码
 from torch_npu.contrib import transfer_to_npu
``` 

### 打patch包
```powershell
cp -r vllm patch/vllm
cd patch
git init
git add .
git commit  -m 'init'
修改代码
git add .
git commit -m 'xxxx'
git format-patch -1
修改patch名称
``` 

### 查看cann包
```powershell
#查看cann包版本
cat /usr/local/Ascend/ascend-toolkit/latest/version.cfg
#去除所有进程
ps -ef | grep python| grep -v grep | awk '{print $2}' | xargs kill -9
#pytorch中查看日志的两行命令
export ASCEND_GLOBAL_LOG_LEVEL=1
export ASCEND_SLOG_PRINT_TO_STDOUT=1
``` 

### 可视化数据
```powershell
# 使用 pip3 (推荐)
pip3 install visidata

# Ubuntu/Debian
sudo apt-get install visidata

# macOS (使用Homebrew)
brew install visidata


vd + csv文件
``` 

### Linux命令

```shell
# 查看可执行文件的依赖库
ldd /usr/bin/python3
ldd /bin/ls

# 查看动态库的依赖
ldd /usr/lib/x86_64-linux-gnu/libc.so.6

# 批量查看目录下所有程序的依赖
find /usr/bin -type f -executable -exec ldd {} \; 2&gt;/dev/null

# 按名称查找库文件
find /usr -name 'libc.so.6' -type f 2&gt;/dev/null
find / -name '*.so' -type f 2&gt;/dev/null

# 在标准库目录中查找
find /usr/lib /usr/lib64 -name 'libpthread*' -type f

#objdump - 目标文件分析 
# 查看动态库依赖
objdump -p /usr/bin/python3 | grep NEEDED

# 查看共享库的符号表
objdump -T /lib/x86_64-linux-gnu/libc.so.6

# 查看动态段信息
objdump -x /path/to/binary | grep -E '(NEEDED|SONAME)'

# 查看动态库依赖
readelf -d /usr/bin/bash | grep 'Shared library'

# 查看所有动态段信息
readelf -d /path/to/program

# 查看符号表
readelf -s /usr/lib/libm.so.6 | grep sqrt

# 查看运行中程序加载的库
lsof -p &lt;pid&gt; | grep '\.so'

# 查看哪个进程使用了特定库
lsof /usr/lib/libc.so.6
``` 

| 场景     | 命令示例                                                      |               |
| ------ | --------------------------------------------------------- | ------------- |
| GPU 状态 | nvidia-smi / nvidia-smi dmon -s pucvmet                   |               |
| 显存占用   | gpustat -cpP1 或 nvtop                                     |               |
| 看模型大小  | du -h /data/models/Qwen2-7B\*                             |               |
| 压测接口   | wrk -t4 -c100 -d30s -s post.lua &lt;http://ip:8000/generate&gt; |               |
| 实时日志   | tail -f /var/log/messages                                 | grep llm\_srv |
| 端口冲突   | lsof -i:8000                                              |               |
| 批量杀进程  | pkill -f uvicorn                                          |               |
| 防火墙    | ufw allow 8000/tcp                                        |               |
| 永久挂载   | echo '/dev/sdb1 /data ext4 defaults 0 0' &gt;&gt; /etc/fstab    |               |

。</description><guid isPermaLink="true">https://naphjohn.github.io/post/linux.html</guid><pubDate>Mon, 08 Sep 2025 07:22:10 +0000</pubDate></item><item><title>RAG和Agent</title><link>https://naphjohn.github.io/post/RAG-he-Agent.html</link><description># RAG 检索增强生成（Retrieval-Augmented Generation，RAG）
RAG的基本结构有哪些呢？
- 向量化模块：用来将文档片段向量化。</description><guid isPermaLink="true">https://naphjohn.github.io/post/RAG-he-Agent.html</guid><pubDate>Mon, 08 Sep 2025 02:54:13 +0000</pubDate></item><item><title>Code</title><link>https://naphjohn.github.io/post/Code.html</link><description># ACM模式

```python
#输入第一行整数，第二行列表
import sys

n = int(sys.stdin.readline())          # 第一行整数
nums = list(map(int, sys.stdin.readline().split()))  # 第二行列表


split() 按空白切分成字符串列表
strip()去掉字符串头尾空白（空格、Tab、\n）
read()一次性读完全部内容（直到 EOF）
.readline() | 只读一行（直到遇到 \n 或 EOF）

#输入第一行整数n，后面是n行列表
import sys
n = int(sys.stdin.readline())
lists = [list(map(int, sys.stdin.readline().split())) for _ in range(n)]
``` 

## 数组
数组是存放在连续内存空间上的相同类型数据的集合。</description><guid isPermaLink="true">https://naphjohn.github.io/post/Code.html</guid><pubDate>Sat, 06 Sep 2025 13:59:33 +0000</pubDate></item><item><title>大模型推理基础</title><link>https://naphjohn.github.io/post/da-mo-xing-tui-li-ji-chu.html</link><description># LLM的推理/transformer架构/vllm

## LLM 推理的推理过程
### QKV
**将QKV三条独立线拆开原因**,核心原因是 ‘参数专属 + 梯度解耦’,原因如下：
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;
把 Q、K、V 拆成 三条独立线性 而不是一个权重乘三次，核心原因是 ‘参数专属 + 梯度解耦’：
一份权重乘三遍 → Q/K/V 被迫共享同一参数空间，梯度累加在同一矩阵，导致：
更新方向被平均，query 要高频、key 要低秩、value 要高熵 的冲突需求互相拉扯；
秩被压缩，多头子空间重叠度↑，注意力退化趋同。</description><guid isPermaLink="true">https://naphjohn.github.io/post/da-mo-xing-tui-li-ji-chu.html</guid><pubDate>Sat, 06 Sep 2025 13:42:06 +0000</pubDate></item><item><title>大模型推理优化</title><link>https://naphjohn.github.io/post/da-mo-xing-tui-li-you-hua.html</link><description>## 量化
### 量化原理以及基本概念
量化过程包含缩放（Scale） 和有时需要的零点（Zero Point），以将浮点数范围映射到低比特整数范围。</description><guid isPermaLink="true">https://naphjohn.github.io/post/da-mo-xing-tui-li-you-hua.html</guid><pubDate>Sat, 06 Sep 2025 13:36:25 +0000</pubDate></item><item><title>大模型训练</title><link>https://naphjohn.github.io/post/da-mo-xing-xun-lian.html</link><description># 训练主要流程

##SFT（Supervised Fine-Tuning，监督微调）
监督微调是“大模型出厂”后的第一道工序：
把已经在海量无标注文本里预训练好的基座模型（如 LLaMA、Qwen）拿来；
用少量人工写好的“指令-答案”对（通常几万～几十万条）继续训练；
训练目标跟预训练一样仍是“预测下一个 token”，但只让模型在答案部分算 loss，迫使它学会“听指令、按要求回答”，而不是继续无脑续写。</description><guid isPermaLink="true">https://naphjohn.github.io/post/da-mo-xing-xun-lian.html</guid><pubDate>Sat, 06 Sep 2025 13:21:04 +0000</pubDate></item></channel></rss>