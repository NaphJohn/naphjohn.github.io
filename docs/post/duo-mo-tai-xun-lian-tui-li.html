<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://avatars.githubusercontent.com/u/46772304?v=4"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="## Diffusion Models 扩散模型
核心思想是逐步的数据生成过程。">
<meta property="og:title" content="多模态训练推理">
<meta property="og:description" content="## Diffusion Models 扩散模型
核心思想是逐步的数据生成过程。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://naphjohn.github.io/post/duo-mo-tai-xun-lian-tui-li.html">
<meta property="og:image" content="https://avatars.githubusercontent.com/u/46772304?v=4">
<title>多模态训练推理</title>



</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}

</style>




<body>
    <div id="header">
<h1 class="postTitle">多模态训练推理</h1>
<div class="title-right">
    <a href="https://naphjohn.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/NaphJohn/naphjohn.github.io/issues/8" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><h2>Diffusion Models 扩散模型</h2>
<p>核心思想是逐步的数据生成过程。<br>
前向过程（Forward Process / Diffusion Process）： 逐步向数据中添加噪声，直到数据完全变成纯高斯噪声。<br>
反向过程（Reverse Process）： 学习如何从纯噪声中一步步地去除噪声，最终恢复出原始数据。<br>
<strong>1. 前向过程（加噪）</strong><br>
这是一个固定的（非学习的）、线性的过程。它被定义为一个马尔可夫链（Markov Chain），每一步都向数据 $\mathbf{x}_t$ 中添加一小步高斯噪声。</p>
<p>公式： $\mathbf{x}t = \sqrt{\alpha_t} \mathbf{x}{t-1} + \sqrt{1 - \alpha_t} \epsilon_t$， 其中 $\epsilon_t \sim \mathcal{N}(0, \mathbf{I})$</p>
<ul>
<li>
$\mathbf{x}_0$ 是原始图像。</li>
<li>
$t$ 从 1 到 T（总步数，通常是1000步）。</li>
<li>
$\alpha_t$ 是预先定义好的噪声调度（Noise Schedule），控制每一步添加的噪声量。它随着 $t$ 增大而减小，意味着越往后加的噪声相对越多。</li>
</ul>
<p>目的： 经过足够多的步数 $T$ 后，$\mathbf{x}_T$ 就完全变成了一个各向同性的高斯噪声（Isotropic Gaussian Noise），不再包含任何原始数据的信息。</p>
<p><strong>2. 反向过程（去噪）</strong><br>
这是一个学习的过程，是模型的核心。我们需要训练一个神经网络来学习如何逆转前向过程。</p>
<p>目标： 给定第 $t$ 步的带噪图像 $\mathbf{x}t$ 和时间步 $t$，神经网络需要预测出添加到 $\mathbf{x}{t-1}$ 上的噪声 $\epsilon$，或者直接预测出去噪后的图像 $\mathbf{x}_0$。目前主流是预测噪声。<br>
<strong>神经网络（U-Net）</strong>： 通常使用 U-Net 架构，并加入注意力机制（Attention） 和时间步嵌入（Timestep Embedding）。</p>
<ul>
<li>U-Net： 具有编码器-解码器结构，带有跳跃连接，非常适合捕捉图像的多尺度特征并进行像素级的预测。</li>
<li>时间步嵌入： 将当前的时间步 $t$ 编码成一个向量，并注入到U-Net的每一层中，让网络知道当前处于去噪的哪个阶段（是早期粗粒度去噪还是后期细粒度修复）。</li>
<li>注意力机制： 帮助模型处理图像不同部分之间的长程依赖关系，对于生成全局一致的图像至关重要。</li>
<li>
</li></ul>
<p><strong>训练过程：</strong></p>
<ol>
<li>从训练集中随机取一张图片 $\mathbf{x}_0$。</li>
<li>随机采样一个时间步 $t$ （从 1 到 T）。</li>
<li>采样一个随机噪声 $\epsilon \sim \mathcal{N}(0, \mathbf{I})$。</li>
<li>将噪声按前向过程公式加到图片上，得到 $\mathbf{x}_t$。</li>
<li>将 $\mathbf{x}t$ 和 $t$ 输入神经网络，让网络预测添加的噪声 $\epsilon\theta(\mathbf{x}_t, t)$。</li>
<li>计算预测噪声和真实噪声之间的<strong>均方误差（MSE Loss）</strong>： $L = ||\epsilon - \epsilon_\theta(\mathbf{x}_t, t)||^2$。</li>
</ol>
<p><strong>采样/推理过程（生成新图像）：</strong></p>
<ol>
<li>从纯高斯噪声 $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$ 开始。</li>
<li>从 $t = T$ 一步步循环到 $t = 1$：
<ul>
<li>将当前的 $\mathbf{x}t$ 和 $t$ 输入训练好的网络，得到预测的噪声 $\epsilon\theta(\mathbf{x}_t, t)$。</li>
<li>使用公式计算出 $\mathbf{x}_{t-1}$（这个过程会额外加入一点随机噪声，除非是最后一步）。</li>
</ul>
</li>
<li>最终得到生成的高清图像 $\mathbf{x}_0$。</li>
</ol>
<h3>与vae/GAN区别</h3>
<p>Diffusion Model 相对于 VAE 的最大优势在于生成质量。它牺牲了生成速度。</p>
<h2>DDPM 与 DDIM 的区别</h2>
<p>DDPM (Denoising Diffusion Probabilistic Models)：<br>
采样随机而且慢。反向过程的每一步都遵循马尔可夫性（当前状态只依赖于前一步），并且会加入随机噪声。<br>
DDIM (Denoising Diffusion Implicit Models)：<br>
采样确定而且快，支持跳步采样。揭示了扩散模型的反向过程其实是一个_微分方程求解器_。它建立了扩散模型与神经ODE（Neural ODE） 的联系</p>
<h2>Classifier Guidance 与 Classifier-Free Guidance</h2>
<h3>Classifier Guidance（分类器引导）</h3>
<p><strong>原理</strong>：</p>
<ol>
<li>需要训练一个额外的分类器。这个分类器接收带噪图像 x_t 和时间步 t，预测其条件 y（如类别标签）的概率。</li>
<li>在反向去噪过程中，不仅使用扩散模型预测的噪声 ε_θ，还计算这个分类器关于输入 x_t 的梯度 ∇_{x_t} log p(y | x_t)。</li>
<li>用这个梯度来“引导”下一步的生成方向，使其朝着最大化分类器置信度（即更符合条件 y）的方向移动。</li>
</ol>
<p><strong>公式（简化）</strong>： ε_guided = ε_θ(x_t, t) - s * σ_t * ∇_{x_t} log p(y | x_t)<br>
s 是引导尺度（guidance scale），控制引导的强度。s 越大，生成结果与条件 y 的对齐越好，但可能会降低多样性。</p>
<h3>Classifier-Free Guidance（无分类器引</h3>
<p><strong>原理：</strong></p>
<ol>
<li>彻底省去了分类器。它通过在训练时随机地丢弃条件（例如，有 10% 的概率将文本条件置空），同时训练一个条件扩散模型 ε_θ(x_t, t, y) 和一个无条件扩散模型 ε_θ(x_t, t, ∅)。</li>
<li>在推理采样时，将条件预测和无条件预测进行向量组合，指向更符合条件的方向。</li>
</ol>
<ul>
<li>公式： ε_guided = ε_θ(x_t, t, ∅) + s * [ε_θ(x_t, t, y) - ε_θ(x_t, t, ∅)]
<ul>
<li>ε_θ(x_t, t, y)：有条件噪声预测（我们想要的）。</li>
<li>ε_θ(x_t, t, ∅)：无条件噪声预测（随意的，不关心条件的）。</li>
<li>[ε_θ(x_t, t, y) - ε_θ(x_t, t, ∅)]：这个向量指向了“使得生成图像更符合条件 y”的方向。</li>
<li>s 同样是引导尺度，控制方向上的步长。<br>
总结：Classifier-Free Guidance 通过巧妙的训练方式和采样时的向量运算，实现了更强大、更简单的控制引导，是目前文生图等技术的基石。</li>
</ul>
</li>
</ul>
<h2>Stable Diffusion</h2>
<p>原理<br>
LDM 的架构包含三个核心组件：</p>
<ol>
<li>自编码器 (VAE)：</li>
</ol>
<ul>
<li>注意：这个VAE是预先训练好的，在扩散训练和推理过程中是冻结（freeze）的。</li>
</ul>
<ol start="2">
<li>扩散模型 (U-Net)：</li>
</ol>
<ul>
<li>核心创新：扩散过程（加噪/去噪）不是在原始图像 x 上进行，而是在VAE编码后的潜在空间 z 上进行的。</li>
<li>U-Net 的结构也进行了调整，以适应在潜在空间 z 上操作。</li>
</ul>
<p>3.条件机制 (Conditioning Mechanism)：</p>
<ul>
<li>通常使用 CLIP 文本编码器，将输入文本提示词（Prompt）编码成向量表示。</li>
<li>通过 Cross-Attention（交叉注意力） 机制注入到 U-Net 的中间层，实现精准的文本控制生成。</li>
</ul>
<p>损失函数：<br>
L_SD = E[|| ε - ε_θ(z_t, t, c) ||^2 ]<br>
整个损失函数只做一件事：衡量预测噪声与真实噪声的差距。</p>
<p><strong>工作流程：</strong><br>
总结流程：<br>
<strong>随机噪声（潜空间） → U-Net多步去噪（受文本引导） → 干净的潜表示 → VAE解码器 → 最终高清图片</strong></p>
<p><em>训练：</em></p>
<ol>
<li>图像 x 通过编码器 E 得到潜在表示 z = E(x)。</li>
<li>对 z 进行常规的DDPM扩散过程（加噪）。</li>
<li>U-Net 学习在潜在空间中去噪，并且通过交叉注意力接受文本条件。</li>
</ol>
<p><em>推理（生成）</em>：</p>
<ol>
<li>在潜在空间中随机采样一个噪声 z_T。</li>
<li>用U-Net和DDIM等采样器，在文本条件的引导下，逐步去噪得到 z_0。</li>
<li>将去噪后的潜在表示 z_0 送入VAE解码器 D，得到高清图像 x = D(z_0)。</li>
</ol>
<p>为什么更高效？</p>
<ol>
<li>计算复杂度大幅降低：</li>
</ol>
<ul>
<li>计算复杂度与数据维度呈指数关系。在 64x64x4 的潜在空间中进行扩散，相比在 512x512x3 的像素空间中进行，计算量和内存占用减少了数十倍。</li>
<li>这是性能提升的最主要原因。</li>
</ul>
<ol start="2">
<li>语义集中：</li>
</ol>
<ul>
<li>潜在空间 z 是原始图像经过VAE编码压缩后的“精华”，它过滤掉了图像中的高频细节（纹理、噪声）等难以学习又耗费算力的信息，保留了高级的语义和概念信息。</li>
<li>在这个更抽象、更紧凑的空间里进行扩散，模型可以更专注于语义内容的学习和生成，效率更高，效果也更好。</li>
</ul>
<ol start="3">
<li>更适合与其它模态对齐：<br>
文本、深度图等控制条件本身也是抽象表示。在潜在空间中进行融合（通过Cross-Attention）比在像素空间更自然、更高效。</li>
</ol>
<p>结论：Stable Diffusion (LDM) 通过将扩散过程从像素空间迁移到计算成本低得多的潜在空间，在不显著牺牲质量的前提下，极大地降低了计算需求。<br>
SD与VAE区别：<br>
一句话：SD 的 VAE 只是“压缩-解压”工具，图像生成质量与多样性由潜空间扩散模型负责，而非传统 VAE 的采样随机性</p>
<h1>SD生成模型对应lora微调训练</h1>
<p>Stable Diffusion（简称SD）是一种基于扩散过程的图像生成模型，应用于文生图场景，能够帮助我们生成图像。<br>
本文档主要介绍如何利用训练框架PyTorch_npu+华为自研Ascend Snt9B硬件，完成SDXL模型的LoRA微调训练。</p>
<h1>常见问题</h1>
<p>问题：执行训练脚本报SSL证书校验错误。<br>
图1 SSL证书校验错误<br>
解决方法：<br>
在入口py文件中加入os.environ['CURL_CA_BUNDLE'] = ''  ，重新执行即可。<br>
训练入口py文件为 diffusers0.21.0/examples/text_to_image/train_text_to_image_lora_sdxl.py。</p>
<h1>多模态模型</h1>
<h2>qewn2.5 VS qwen3-vl</h2>
<p>Qwen2.5-VL 是“强基线”——Dense+128 K+事件级视频；<br>
Qwen3-VL 是“全栈升级”——MoE/DeepStack/Interleaved-MRoPE/256 K→1 M/Thinking 全套上新，长文档、小时级视频、像素级定位、逻辑推理全面跃迁</p>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th>维度</th>
<th>Qwen2.5-VL</th>
<th>Qwen3-VL</th>
</tr>
</thead>
<tbody>
<tr>
<td>底座 LLM</td>
<td>Qwen2.5 Dense</td>
<td>Qwen3 Dense / MoE</td>
</tr>
<tr>
<td>视觉编码器</td>
<td>ViT-bigG 14×14</td>
<td>更深 ViT 16×16，<strong>DeepStack</strong> 多层融合</td>
</tr>
<tr>
<td>位置编码</td>
<td>M-RoPE（t, h, w）</td>
<td><strong>Interleaved-MRoPE</strong>（t/h/w 频率交错）</td>
</tr>
<tr>
<td>上下文</td>
<td>128 K</td>
<td><strong>原生 256 K，可扩 1 M</strong></td>
</tr>
<tr>
<td>推理分支</td>
<td>❌</td>
<td>✅ Thinking 版（…）</td>
</tr>
<tr>
<td>最大参数</td>
<td>72 B Dense</td>
<td>235 B MoE（激活 22 B）</td>
</tr>
<tr>
<td>OCR 语种</td>
<td>约 20 种</td>
<td><strong>32 种</strong> + 倾斜/模糊鲁棒</td>
</tr>
<tr>
<td>视频定位</td>
<td>事件片段级</td>
<td><strong>秒级时间戳</strong>对齐</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p>二、原理差异速览<br>
DeepStack 融合（Qwen3-VL 独有）<br>
不再只用 ViT 最后一层，而是把 第 8、16、24 层特征同时抽出，经 PatchMerger 对齐后分别注入 LLM 的 1、2、3 层，形成“视觉早融合”，提升细粒度与空间推理 。<br>
Interleaved-MRoPE<br>
把时间、高度、宽度三个维度的旋转频率交错混合，使模型对长视频时序关系更鲁棒，且支持可变帧率输入 。<br>
Text-Timestamp Alignment<br>
视频帧与"&lt;0.8 s&gt;"这类显式时间戳文本绑定，LLM 可直接用自然语言做秒级索引，而 2.5-VL 仅能用粗粒度事件标签 。<br>
Thinking 分支<br>
同 Qwen3 LLM，在 ViL 阶段保留推理头，输出 … 中间思考，再给出最终答案，视觉问答 + 逻辑推理显著上升</p>
<p>输入→输出完整流程（以 Qwen3-VL 为例）</p>
<ol>
<li>输入预处理<br>
图像：动态分辨率 + 16×16 patch → ViT 多层特征（8/16/24）<br>
视频：每秒采样 1 帧，每帧打时间戳文本 → 与图像同方式编码<br>
文本：正常 tokenizer，但位置 id 由 Interleaved-MRoPE 统一计算 (t, h, w) 。</li>
<li>视觉早融合（DeepStack）<br>
多层 ViT 特征 → PatchMerger → 视觉 token 序列<br>
视觉 token 与文本 token 交错拼接，形成统一序列。</li>
<li>LLM 前向<br>
底座：Qwen3-MoE（30 B-A3B 例）<br>
每 token 激活 3 B 参数，Thinking 版会先输出推理链 …，再输出最终文本 。</li>
<li>后处理<br>
支持 JSON 带坐标（BBox/Points）或 秒级时间戳（视频）<br>
可返回  内容供上层折叠显示。</li>
</ol></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://naphjohn.github.io">kaikai的博客</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","NaphJohn/naphjohn.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}



</script>
<script>MathJax = {tex: {inlineMath: [["$", "$"]]}};</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</html>
