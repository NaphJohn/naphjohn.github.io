<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://avatars.githubusercontent.com/u/46772304?v=4"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="## Diffusion Models 扩散模型
核心思想是逐步的数据生成过程。">
<meta property="og:title" content="多模态训练推理">
<meta property="og:description" content="## Diffusion Models 扩散模型
核心思想是逐步的数据生成过程。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://naphjohn.github.io/post/duo-mo-tai-xun-lian-tui-li.html">
<meta property="og:image" content="https://avatars.githubusercontent.com/u/46772304?v=4">
<title>多模态训练推理</title>



</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
.copy-feedback {
    display: none;
    position: absolute;
    top: 10px;
    right: 50px;
    color: var(--color-fg-on-emphasis);
    background-color: var(--color-fg-muted);
    border-radius: 3px;
    padding: 5px 8px;
    font-size: 12px;
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">多模态训练推理</h1>
<div class="title-right">
    <a href="https://naphjohn.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/NaphJohn/naphjohn.github.io/issues/8" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><h2>Diffusion Models 扩散模型</h2>
<p>核心思想是逐步的数据生成过程。<br>
前向过程（Forward Process / Diffusion Process）： 逐步向数据中添加噪声，直到数据完全变成纯高斯噪声。<br>
反向过程（Reverse Process）： 学习如何从纯噪声中一步步地去除噪声，最终恢复出原始数据。<br>
<strong>1. 前向过程（加噪）</strong><br>
这是一个固定的（非学习的）、线性的过程。它被定义为一个马尔可夫链（Markov Chain），每一步都向数据 $\mathbf{x}_t$ 中添加一小步高斯噪声。</p>
<p>公式： $\mathbf{x}t = \sqrt{\alpha_t} \mathbf{x}{t-1} + \sqrt{1 - \alpha_t} \epsilon_t$， 其中 $\epsilon_t \sim \mathcal{N}(0, \mathbf{I})$</p>
<ul>
<li>
$\mathbf{x}_0$ 是原始图像。</li>
<li>
$t$ 从 1 到 T（总步数，通常是1000步）。</li>
<li>
$\alpha_t$ 是预先定义好的噪声调度（Noise Schedule），控制每一步添加的噪声量。它随着 $t$ 增大而减小，意味着越往后加的噪声相对越多。</li>
</ul>
<p>目的： 经过足够多的步数 $T$ 后，$\mathbf{x}_T$ 就完全变成了一个各向同性的高斯噪声（Isotropic Gaussian Noise），不再包含任何原始数据的信息。</p>
<p><strong>2. 反向过程（去噪）</strong><br>
这是一个学习的过程，是模型的核心。我们需要训练一个神经网络来学习如何逆转前向过程。</p>
<p>目标： 给定第 $t$ 步的带噪图像 $\mathbf{x}t$ 和时间步 $t$，神经网络需要预测出添加到 $\mathbf{x}{t-1}$ 上的噪声 $\epsilon$，或者直接预测出去噪后的图像 $\mathbf{x}_0$。目前主流是预测噪声。<br>
<strong>神经网络（U-Net）</strong>： 通常使用 U-Net 架构，并加入注意力机制（Attention） 和时间步嵌入（Timestep Embedding）。</p>
<ul>
<li>U-Net： 具有编码器-解码器结构，带有跳跃连接，非常适合捕捉图像的多尺度特征并进行像素级的预测。</li>
<li>时间步嵌入： 将当前的时间步 $t$ 编码成一个向量，并注入到U-Net的每一层中，让网络知道当前处于去噪的哪个阶段（是早期粗粒度去噪还是后期细粒度修复）。</li>
<li>注意力机制： 帮助模型处理图像不同部分之间的长程依赖关系，对于生成全局一致的图像至关重要。</li>
<li>
</li></ul>
<p><strong>训练过程：</strong></p>
<ol>
<li>从训练集中随机取一张图片 $\mathbf{x}_0$。</li>
<li>随机采样一个时间步 $t$ （从 1 到 T）。</li>
<li>采样一个随机噪声 $\epsilon \sim \mathcal{N}(0, \mathbf{I})$。</li>
<li>将噪声按前向过程公式加到图片上，得到 $\mathbf{x}_t$。</li>
<li>将 $\mathbf{x}t$ 和 $t$ 输入神经网络，让网络预测添加的噪声 $\epsilon\theta(\mathbf{x}_t, t)$。</li>
<li>计算预测噪声和真实噪声之间的<strong>均方误差（MSE Loss）</strong>： $L = ||\epsilon - \epsilon_\theta(\mathbf{x}_t, t)||^2$。</li>
</ol>
<p><strong>采样/推理过程（生成新图像）：</strong></p>
<ol>
<li>从纯高斯噪声 $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$ 开始。</li>
<li>从 $t = T$ 一步步循环到 $t = 1$：
<ul>
<li>将当前的 $\mathbf{x}t$ 和 $t$ 输入训练好的网络，得到预测的噪声 $\epsilon\theta(\mathbf{x}_t, t)$。</li>
<li>使用公式计算出 $\mathbf{x}_{t-1}$（这个过程会额外加入一点随机噪声，除非是最后一步）。</li>
</ul>
</li>
<li>最终得到生成的高清图像 $\mathbf{x}_0$。</li>
</ol>
<h3>与vae/GAN区别</h3>
<p>Diffusion Model 相对于 VAE 的最大优势在于生成质量。它牺牲了生成速度。</p>
<h2>DDPM 与 DDIM 的区别</h2>
<p>DDPM (Denoising Diffusion Probabilistic Models)：<br>
采样随机而且慢。反向过程的每一步都遵循马尔可夫性（当前状态只依赖于前一步），并且会加入随机噪声。<br>
DDIM (Denoising Diffusion Implicit Models)：<br>
采样确定而且快，支持跳步采样。揭示了扩散模型的反向过程其实是一个_微分方程求解器_。它建立了扩散模型与神经ODE（Neural ODE） 的联系</p>
<h2>Classifier Guidance 与 Classifier-Free Guidance</h2>
<h3>Classifier Guidance（分类器引导）</h3>
<p><strong>原理</strong>：</p>
<ol>
<li>需要训练一个额外的分类器。这个分类器接收带噪图像 x_t 和时间步 t，预测其条件 y（如类别标签）的概率。</li>
<li>在反向去噪过程中，不仅使用扩散模型预测的噪声 ε_θ，还计算这个分类器关于输入 x_t 的梯度 ∇_{x_t} log p(y | x_t)。</li>
<li>用这个梯度来“引导”下一步的生成方向，使其朝着最大化分类器置信度（即更符合条件 y）的方向移动。</li>
</ol>
<p><strong>公式（简化）</strong>： ε_guided = ε_θ(x_t, t) - s * σ_t * ∇_{x_t} log p(y | x_t)<br>
s 是引导尺度（guidance scale），控制引导的强度。s 越大，生成结果与条件 y 的对齐越好，但可能会降低多样性。</p>
<h3>Classifier-Free Guidance（无分类器引</h3>
<p><strong>原理：</strong></p>
<ol>
<li>彻底省去了分类器。它通过在训练时随机地丢弃条件（例如，有 10% 的概率将文本条件置空），同时训练一个条件扩散模型 ε_θ(x_t, t, y) 和一个无条件扩散模型 ε_θ(x_t, t, ∅)。</li>
<li>在推理采样时，将条件预测和无条件预测进行向量组合，指向更符合条件的方向。</li>
</ol>
<ul>
<li>公式： ε_guided = ε_θ(x_t, t, ∅) + s * [ε_θ(x_t, t, y) - ε_θ(x_t, t, ∅)]
<ul>
<li>ε_θ(x_t, t, y)：有条件噪声预测（我们想要的）。</li>
<li>ε_θ(x_t, t, ∅)：无条件噪声预测（随意的，不关心条件的）。</li>
<li>[ε_θ(x_t, t, y) - ε_θ(x_t, t, ∅)]：这个向量指向了“使得生成图像更符合条件 y”的方向。</li>
<li>s 同样是引导尺度，控制方向上的步长。<br>
总结：Classifier-Free Guidance 通过巧妙的训练方式和采样时的向量运算，实现了更强大、更简单的控制引导，是目前文生图等技术的基石。</li>
</ul>
</li>
</ul>
<h2>Stable Diffusion</h2>
<p>原理<br>
LDM 的架构包含三个核心组件：</p>
<ol>
<li>自编码器 (VAE)：</li>
</ol>
<ul>
<li>注意：这个VAE是预先训练好的，在扩散训练和推理过程中是冻结（freeze）的。</li>
</ul>
<ol start="2">
<li>扩散模型 (U-Net)：</li>
</ol>
<ul>
<li>核心创新：扩散过程（加噪/去噪）不是在原始图像 x 上进行，而是在VAE编码后的潜在空间 z 上进行的。</li>
<li>U-Net 的结构也进行了调整，以适应在潜在空间 z 上操作。</li>
</ul>
<p>3.条件机制 (Conditioning Mechanism)：</p>
<ul>
<li>通常使用 CLIP 文本编码器，将输入文本提示词（Prompt）编码成向量表示。</li>
<li>通过 Cross-Attention（交叉注意力） 机制注入到 U-Net 的中间层，实现精准的文本控制生成。</li>
</ul>
<p>损失函数：<br>
L_SD = E[|| ε - ε_θ(z_t, t, c) ||^2 ]<br>
整个损失函数只做一件事：衡量预测噪声与真实噪声的差距。</p>
<p><strong>工作流程：</strong><br>
总结流程：<br>
<strong>随机噪声（潜空间） → U-Net多步去噪（受文本引导） → 干净的潜表示 → VAE解码器 → 最终高清图片</strong></p>
<p><em>训练：</em></p>
<ol>
<li>图像 x 通过编码器 E 得到潜在表示 z = E(x)。</li>
<li>对 z 进行常规的DDPM扩散过程（加噪）。</li>
<li>U-Net 学习在潜在空间中去噪，并且通过交叉注意力接受文本条件。</li>
</ol>
<p><em>推理（生成）</em>：</p>
<ol>
<li>在潜在空间中随机采样一个噪声 z_T。</li>
<li>用U-Net和DDIM等采样器，在文本条件的引导下，逐步去噪得到 z_0。</li>
<li>将去噪后的潜在表示 z_0 送入VAE解码器 D，得到高清图像 x = D(z_0)。</li>
</ol>
<p>为什么更高效？</p>
<ol>
<li>计算复杂度大幅降低：</li>
</ol>
<ul>
<li>计算复杂度与数据维度呈指数关系。在 64x64x4 的潜在空间中进行扩散，相比在 512x512x3 的像素空间中进行，计算量和内存占用减少了数十倍。</li>
<li>这是性能提升的最主要原因。</li>
</ul>
<ol start="2">
<li>语义集中：</li>
</ol>
<ul>
<li>潜在空间 z 是原始图像经过VAE编码压缩后的“精华”，它过滤掉了图像中的高频细节（纹理、噪声）等难以学习又耗费算力的信息，保留了高级的语义和概念信息。</li>
<li>在这个更抽象、更紧凑的空间里进行扩散，模型可以更专注于语义内容的学习和生成，效率更高，效果也更好。</li>
</ul>
<ol start="3">
<li>更适合与其它模态对齐：<br>
文本、深度图等控制条件本身也是抽象表示。在潜在空间中进行融合（通过Cross-Attention）比在像素空间更自然、更高效。</li>
</ol>
<p>结论：Stable Diffusion (LDM) 通过将扩散过程从像素空间迁移到计算成本低得多的潜在空间，在不显著牺牲质量的前提下，极大地降低了计算需求。<br>
SD与VAE区别：<br>
一句话：SD 的 VAE 只是“压缩-解压”工具，图像生成质量与多样性由潜空间扩散模型负责，而非传统 VAE 的采样随机性</p>
<p>Stable Diffusion 1.5 → SD 2 → SD 3 三代官方基础模型的「横向差异速览表」，1.5/2 用 UNet+CLIP，SD3 把 UNet 换成 MM-DiT，文本三编码器。</p>
<details><summary>Details</summary>
<p>
</p><markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th>维度</th>
<th>SD 1.5</th>
<th>SD 2.0 / 2.1</th>
<th>SD 3 / 3.5</th>
</tr>
</thead>
<tbody>
<tr>
<td>发布时间</td>
<td>2022-10</td>
<td>2022-11 / 2023-01</td>
<td>2024-06</td>
</tr>
<tr>
<td>参数规模</td>
<td>≈ 0.98 B</td>
<td>≈ 0.98 B</td>
<td>2 B–8 B（多档）</td>
</tr>
<tr>
<td>文本编码器</td>
<td>CLIP ViT-L/14</td>
<td>OpenCLIP-ViT/H</td>
<td>全新 3×Transformer（T5 + 两个 CLIP）</td>
</tr>
<tr>
<td>最大原生分辨率</td>
<td>512×512</td>
<td>768×768</td>
<td>1024×1024–2048×2048</td>
</tr>
<tr>
<td>显存门槛</td>
<td>4 GB</td>
<td>6 GB</td>
<td>8 GB+（官方建议 12 GB）</td>
</tr>
<tr>
<td>训练数据</td>
<td>LAION-2B</td>
<td>LAION-5B（NSFW 过滤）</td>
<td>LAION-5B + 内部高美学子集</td>
</tr>
<tr>
<td>负面提示依赖</td>
<td>中</td>
<td>高（必须）</td>
<td>低（理解力↑）</td>
</tr>
<tr>
<td>文字渲染能力</td>
<td>差</td>
<td>差</td>
<td>可嵌入短词</td>
</tr>
<tr>
<td>社区生态</td>
<td>最丰富（Lora/插件）</td>
<td>中等</td>
<td>逐步完善中</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p></p>
</details> 
<h1>SD生成模型对应lora微调训练</h1>
<p>Stable Diffusion（简称SD）是一种基于扩散过程的图像生成模型，应用于文生图场景，能够帮助我们生成图像。<br>
本文档主要介绍如何利用训练框架PyTorch_npu+华为自研Ascend Snt9B硬件，完成SDXL模型的LoRA微调训练。</p>
<h1>常见问题</h1>
<p>问题：执行训练脚本报SSL证书校验错误。<br>
图1 SSL证书校验错误<br>
解决方法：<br>
在入口py文件中加入os.environ['CURL_CA_BUNDLE'] = ''  ，重新执行即可。<br>
训练入口py文件为 diffusers0.21.0/examples/text_to_image/train_text_to_image_lora_sdxl.py。</p>
<h1>Opensora模型（文本到视频生成）</h1>
<p>实际项目：opensora基于modelarts 适配npu；<br>
文本 → T5 编码 → STDiT 逐帧去噪 → 时空 VAE 解码 → 视频文件（mp4, 25 fps）<br>
「先压后去噪」<br>
VAE 把视频压成隐码 → DiT 在隐码里玩 Diffusion → 推理用最终权重解码回视频<br>
「OpenSora = 时空 VAE + STDiT + T5；WebVid-2M 学；VAE loss ↘ 0.8e-3，DiT loss ↘ 0.05 停；输出 4-16 s 720p 视频。」<br>
opensora模型与SD模型架构比较</p>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th>模块</th>
<th>SD 1.5/2</th>
<th>SD 3</th>
<th>OpenSora</th>
</tr>
</thead>
<tbody>
<tr>
<td>主干去噪</td>
<td>UNet</td>
<td>MMDiT（双 Transformer）</td>
<td>STDiT（时空 DiT）</td>
</tr>
<tr>
<td>文本编码</td>
<td>CLIP/OpenCLIP</td>
<td>CLIP-L/G + T5-XXL</td>
<td>CLIP（PixArt 权重）</td>
</tr>
<tr>
<td>时空建模</td>
<td>无（单帧）</td>
<td>无（单帧）</td>
<td><strong>时间注意力层</strong>插在空间 DiT 块之间</td>
</tr>
<tr>
<td>隐空间压缩</td>
<td>8×8 VAE（仅空间）</td>
<td>8×8 VAE（空间）</td>
<td><strong>8×8×4</strong> 视频 VAE（空间 8×，时间 4×）</td>
</tr>
<tr>
<td>训练策略</td>
<td>静态图 + 随机噪声</td>
<td>静态图 + 随机噪声</td>
<td>视频 + 时间步采样 + Rectified-Flow</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p><strong>全流程</strong>如下表格</p>
<details><summary>Details</summary>
<p>
</p><markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th>环节</th>
<th>输入</th>
<th>输出</th>
<th>权重作用</th>
<th>数据集</th>
<th>收敛/loss 含义</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>VAE 训练</strong></td>
<td>WebVid-2M 原始视频帧</td>
<td>时空隐码 <code class="notranslate">z</code> (B, C, T, H, W)</td>
<td>OpenSora-VAE-v1.2 <strong>初始化</strong><br>PixArt-SDXL-VAE 提供空间编码器结构</td>
<td>WebVid-2M 视频帧<br>→ webvid_meta_data.csv</td>
<td><strong>MSE + KL</strong> 阶段递减<br>stage3 loss ≈ 0.8×10⁻³ 停止</td>
</tr>
<tr>
<td><strong>DiT 训练</strong></td>
<td>隐码 <code class="notranslate">z</code> + T5 文本嵌入</td>
<td>去噪后隐码 <code class="notranslate">ẑ</code></td>
<td>STDiT-v3 <strong>初始化权重</strong><br>T5-XXL 提供文本编码</td>
<td>WebVid-2M 隐码 + 文本</td>
<td><strong>MSE (ε-预测)</strong><br>stage3 loss ≈ 0.05 停止</td>
</tr>
<tr>
<td><strong>推理</strong></td>
<td>文本 prompt + 噪声</td>
<td>4-16 s 720p/1080p 视频</td>
<td><strong>三阶段最终权重</strong><br>VAE-decoder 把隐码变像素</td>
<td>无（纯生成）</td>
<td>无 loss，人工/CLIP 打分</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<ol>
<li>权重清单 &amp; 作用<br>
| 文件 | 放置位置 | 用途 |<br>
|---|---|---|<br>
| OpenSora-VAE-v1.2 | weights/ | 时空压缩/解压初始化 |<br>
| PixArt-SDXL-VAE | weights/ | 空间编码器结构参考 |<br>
| STDiT-v3 | weights/ | DiT 主干初始化 |<br>
| T5-XXL | weights/ | 文本编码器（冻结） |<br>
| VGG16 | torch cache | perceptual loss 计算（仅 VAE 阶段）|</li>
<li>数据集 = 学什么<br>
WebVid-2M（约 2.3 M 段 5-30 s 视频）<br>
→ 处理后 webvid_meta_data.csv 含 id, text, 帧路径<br>
→ 实际只取 fmin ≥ 1（≥ 1 帧）且分辨率 ≥ 360 p 的子集</li>
<li>收敛标准 &amp; Loss<br>
VAE 三阶段：<br>
loss = MSE(像素重建) + β·KL(隐码)<br>
日志 loss.txt 每步打印；stage3 &lt; 0.0008 手动停<br>
DiT 三阶段：<br>
loss = MSE(噪声预测 ε)<br>
stage3 &lt; 0.05 且验证集 CLIP ≥ 0.33 视为收敛</li>
</ol>
<p></p>
</details> 
<h1>多模态模型</h1>
<h2>qewn2.5 VS qwen3-vl</h2>
<p>Qwen2.5-VL 是“强基线”——Dense+128 K+事件级视频；<br>
Qwen3-VL 是“全栈升级”——MoE/DeepStack/Interleaved-MRoPE/256 K→1 M/Thinking 全套上新，长文档、小时级视频、像素级定位、逻辑推理全面跃迁</p>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th>维度</th>
<th>Qwen2.5-VL</th>
<th>Qwen3-VL</th>
</tr>
</thead>
<tbody>
<tr>
<td>底座 LLM</td>
<td>Qwen2.5 Dense</td>
<td>Qwen3 Dense / MoE</td>
</tr>
<tr>
<td>视觉编码器</td>
<td>ViT-bigG 14×14</td>
<td>更深 ViT 16×16，<strong>DeepStack</strong> 多层融合</td>
</tr>
<tr>
<td>位置编码</td>
<td>M-RoPE（t, h, w）</td>
<td><strong>Interleaved-MRoPE</strong>（t/h/w 频率交错）</td>
</tr>
<tr>
<td>上下文</td>
<td>128 K</td>
<td><strong>原生 256 K，可扩 1 M</strong></td>
</tr>
<tr>
<td>推理分支</td>
<td>❌</td>
<td>✅ Thinking 版（…）</td>
</tr>
<tr>
<td>最大参数</td>
<td>72 B Dense</td>
<td>235 B MoE（激活 22 B）</td>
</tr>
<tr>
<td>OCR 语种</td>
<td>约 20 种</td>
<td><strong>32 种</strong> + 倾斜/模糊鲁棒</td>
</tr>
<tr>
<td>视频定位</td>
<td>事件片段级</td>
<td><strong>秒级时间戳</strong>对齐</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p>二、原理差异速览<br>
DeepStack 融合（Qwen3-VL 独有）<br>
不再只用 ViT 最后一层，而是把 第 8、16、24 层特征同时抽出，经 PatchMerger 对齐后分别注入 LLM 的 1、2、3 层，形成“视觉早融合”，提升细粒度与空间推理 。<br>
Interleaved-MRoPE<br>
把时间、高度、宽度三个维度的旋转频率交错混合，使模型对长视频时序关系更鲁棒，且支持可变帧率输入 。<br>
Text-Timestamp Alignment<br>
视频帧与"&lt;0.8 s&gt;"这类显式时间戳文本绑定，LLM 可直接用自然语言做秒级索引，而 2.5-VL 仅能用粗粒度事件标签 。<br>
Thinking 分支<br>
同 Qwen3 LLM，在 ViL 阶段保留推理头，输出 … 中间思考，再给出最终答案，视觉问答 + 逻辑推理显著上升</p>
<p>输入→输出完整流程（以 Qwen3-VL 为例）</p>
<ol>
<li>输入预处理<br>
图像：动态分辨率 + 16×16 patch → ViT 多层特征（8/16/24）<br>
视频：每秒采样 1 帧，每帧打时间戳文本 → 与图像同方式编码<br>
文本：正常 tokenizer，但位置 id 由 Interleaved-MRoPE 统一计算 (t, h, w) 。</li>
<li>视觉早融合（DeepStack）<br>
多层 ViT 特征 → PatchMerger → 视觉 token 序列<br>
视觉 token 与文本 token 交错拼接，形成统一序列。</li>
<li>LLM 前向<br>
底座：Qwen3-MoE（30 B-A3B 例）<br>
每 token 激活 3 B 参数，Thinking 版会先输出推理链 …，再输出最终文本 。</li>
<li>后处理<br>
支持 JSON 带坐标（BBox/Points）或 秒级时间戳（视频）<br>
可返回  内容供上层折叠显示。</li>
</ol>
<h1>ViT模型</h1>
<p>ViT = Patch Embedding + Transformer Encoder + cls_token（图像分类），就是「把图切成 patches → 当成一串 token → 做自注意力」，全局上下文一步到位，无需 CNN。</p>
<p>具体处理过程如下：</p>
<details><summary>Details</summary>
<p>
</p><ol>
<li>输入处理<br>
将 H×W×3 图片切成 固定大小 patches（如 16×16），得到 N = HW/16² 个 patch。<br>
每个 patch 展平 → 线性投影到 D 维向量（= token）。<br>
追加 可学习分类 token（cls_token）+ 位置编码（1-D 或 2-D sin/cos），形成长度 N+1 的序列。</li>
<li>模型结构<br>
纯 Transformer Encoder（L 层）：<br>
– 多头自注意力（MSA）<br>
– 前馈网络（MLP）<br>
– LayerNorm + 残差<br>
无卷积、无池化，全局感受野一层到位。</li>
<li>分类头<br>
取 cls_token 的最终表示 → 线性层 → softmax → 类别概率。</li>
<li>训练 &amp; 推理<br>
与 BERT 一样用 大规模监督预训练（ImageNet-21k → ILSVRC）。<br>
推理时一次前向即得全局特征，计算量 ∝ patch 数，适合 GPU 并行。</li>
</ol>
<p></p>
</details> 
<h1>wav2lip</h1>
<p>Wav2Lip是一种基于对抗生成网络的由语音驱动的人脸说话视频生成模型。主要应用于数字人场景。不仅可以基于静态图像来输出与目标语音匹配的唇形同步视频，还可以直接将动态的视频进行唇形转换，输出与输入语音匹配的视频，俗称“对口型”。该技术的主要作用就是在将音频与图片、音频与视频进行合成时，口型能够自然。<br>
Wav2Lip模型的<strong>输入为一段视频和一段语音</strong>，<strong>输出为一段唇音同步的视频</strong>。<br>
Wav2Lip的网络模型总体上分成三块：<strong>生成器、判别器和一个预训练好的唇音同步判别模型Pre-trained Lip-sync Expert。</strong><br>
生成器是基于encoder-decoder的网络结构，分别利用2个encoder（speech encoder和identity encoder）去对输入的语音和视频人脸进行编码，并将二者的编码结果进行拼接，送入到face decoder中进行解码得到输出的视频帧。<br>
判别器Visual Quality Discriminator对生成结果的质量进行规范，提高生成视频的清晰度。<br>
引入预训练的唇音同步判别模型Pre-trained Lip-sync Expert，作为衡量生成结果的唇音同步性的额外损失，可以更好地保证生成结果的唇音同步性。</p>
<p>Wav2Lip 用「梅尔频谱 + 参考脸」→ GAN 生成「唇部同步帧」，双判别器保障「对齐 + 清晰」，输入任意音频即可零样本产出嘴型完美对齐的新视频。<br>
<strong>梅尔频谱</strong>= “音频的指纹图”<br>
先把任意音频转成 80 条频率带的时间-能量图，横轴时间，纵轴频率，颜色=能量大小。<br>
好处：扔掉相位、采样率等冗余信息，只保留“人嘴该动”的节奏与音高线索，图小、算得快。</p>
<p>参考脸 = “一张静态证件照”<br>
从原视频里随机抽一帧（或给单张照片），裁出 96×96 的嘴周区域，告诉网络“这张脸长这样，唇色、齿形、皮肤纹理别给我改”。</p>
<p>唇部同步帧 = “GAN 新生成的嘴型图”<br>
生成器把梅尔谱的节奏 + 参考脸的外观拼起来，输出一帧帧 96×96 的“嘴”，再贴回原图，就得到“声音到哪儿，嘴张到哪儿”的新视频。<br>
双判别器负责 A. 嘴型对齐（同步判别器）+ B. 画面清晰（视觉判别器），让结果既跟拍又高清。</p>
<p><strong>原理总览</strong><br>
生成对抗网络（GAN） 架构：<br>
– 生成器：编码「语音特征」+「参考人脸」，解码为「唇部同步帧」；<br>
– 双判别器：<br>
① 唇同步判别器（基于改进 SyncNet）→ 判断音-唇是否同步；<br>
② 视觉质量判别器 → 判断画面真实度与清晰度。<br>
损失函数：同步损失 + 对抗损失 + 重建损失联合训练，迫使生成器产出「既对齐又逼真」的口播帧<br>
<strong>网络模块拆解</strong><br>
Identity Encoder（残差卷积）：提取参考帧的「身份+姿态」特征。<br>
Speech Encoder（2D CNN）：把梅尔频谱编码为「语音动画」特征。<br>
Face Decoder：拼接两类特征 → 上采样 → 生成 96×96 唇部帧。<br>
Lip-sync Expert（预训练 SyncNet）：额外监督，确保音-唇时间对齐 。<br>
<strong>训练 &amp; 推理流程</strong><br>
人脸检测 → 裁剪嘴部 96×96 区域。<br>
音频转梅尔频谱，滑窗 16 帧为一段。<br>
生成器逐段输出唇部帧；判别器实时反馈同步/真实度。<br>
推理时无需 3D 参数，任何语音 + 任意人脸即可零样本合成</p>
<details><summary>Details</summary>
<p>
</p><p>Wav2Lip 训练全流程速记（一页 A4 能看完）<br>
总体目标<br>
「让一段音频驱动一张脸，嘴型完美对上节拍，画面还不糊」——靠两套预训练权重（人脸检测预训练模型+专家唇形同步鉴别器） + 三步训练完成。LRS2数据集<br>
预训练权重（必下载）</p>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th>权重</th>
<th>下载后改名/路径</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>人脸检测 S3FD</td>
<td>face_detection/detection/sfd/s3fd.pth</td>
<td>裁出 96×96 嘴部 ROI</td>
</tr>
<tr>
<td>专家唇同步 SyncNet</td>
<td>checkpoints/lipsync_expert.pth</td>
<td>训练时当“节拍老师”，逼生成器对齐音频</td>
</tr>
<tr>
<td>数据流水线（一条命令）</td>
<td></td>
<td></td>
</tr>
<tr>
<td>bash</td>
<td></td>
<td></td>
</tr>
<tr>
<td>复制</td>
<td></td>
<td></td>
</tr>
<tr>
<td>使用的数据集是LRS2（Lip Reading Sentences 2）是牛津大学与 BBC 合作推出的英文唇语同步公开数据集，也是目前训练Wav2Lip、AV-ASR 等“对口型”模型最常用的基准数据。</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>原始视频 → 帧 + 16 kHz 音频</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td>python preprocess.py --data_root LRS2_partly --preprocessed_root lrs2_preprocessed/</td>
<td></td>
<td></td>
</tr>
<tr>
<td>产出结构</td>
<td></td>
<td></td>
</tr>
<tr>
<td>复制</td>
<td></td>
<td></td>
</tr>
<tr>
<td>lrs2_preprocessed/main/00001/xxx.jpg  # 每帧嘴图</td>
<td></td>
<td></td>
</tr>
<tr>
<td>lrs2_preprocessed/main/00001/audio.wav （16 kHz 单声道音频）</td>
<td></td>
<td></td>
</tr>
<tr>
<td>训练路线图</td>
<td></td>
<td></td>
</tr>
<tr>
<td>① （可选）重训专家鉴别器</td>
<td></td>
<td></td>
</tr>
<tr>
<td>bash</td>
<td></td>
<td></td>
</tr>
<tr>
<td>复制</td>
<td></td>
<td></td>
</tr>
<tr>
<td>python color_syncnet_train.py --data_root lrs2_preprocessed/main/ \</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<pre class="notranslate"><code class="notranslate">                          --checkpoint_dir savedmodel/syncnet_model/
</code></pre>
<p>目标 loss ≈ 0.25（官方已给权重可跳过）<br>
② 主模型训练<br>
bash<br>
复制<br>
python wav2lip_train.py --data_root lrs2_preprocessed/main/ <br>
--checkpoint_dir savedmodel <br>
--syncnet_checkpoint_path checkpoints/lipsync_expert.pth<br>
默认 每 3000 step 存一次 ckpt；首次 700 step 做评估，耐心等。<br>
关键超参速查（hparams.py）</p>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>面试一句话</th>
</tr>
</thead>
<tbody>
<tr>
<td>batch_size</td>
<td>80</td>
<td>显存够就拉满，嘴型更稳</td>
</tr>
<tr>
<td>syncnet_wt</td>
<td>0.03</td>
<td>同步损失权重，↑ 嘴型对齐 ↑ 但可能模糊</td>
</tr>
<tr>
<td>eval_interval</td>
<td>3000</td>
<td>隔多少 step 测一次同步/重建 loss</td>
</tr>
<tr>
<td>img_size</td>
<td>96</td>
<td>只改嘴部 96×96，省算力</td>
</tr>
<tr>
<td>fps</td>
<td>25</td>
<td>与数据集帧率一致，否则音画错位</td>
</tr>
<tr>
<td>收敛指标（背下来）</td>
<td></td>
<td></td>
</tr>
<tr>
<td>专家 SyncNet loss ≈ 0.25</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wav2Lip sync loss ≈ 0.2</td>
<td></td>
<td></td>
</tr>
<tr>
<td>低于这两个值，推理基本“对得上、不糊脸”。</td>
<td></td>
<td></td>
</tr>
<tr>
<td>面试金句</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“两步权重 + 三步训练，SyncNet 当节拍老师，GAN 保清晰，只改 96×96 嘴区，实现零样本唇同步。”</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p></p>
</details> 
<h1>Qwen2.5-VL</h1>
<p>Qwen2.5-VL 的视觉编码器 = 任意分辨率 ViT + 2D-RoPE + Window Attention + 轻量 Neck，先单独对齐图文特征，再与 LLM 端到端混训，推理时把图像 patch 当「超长文本 token」直接拼进输入，无需额外 CNN 或 Q-Former，这就是「添加」的全部过程。</p>
<p>一、视觉编码器结构（Qwen2.5-VL Vision Encoder）</p>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th>模块</th>
<th>设计要点</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>Patch Embedding</td>
<td>16×16 卷积，stride=16</td>
<td>把任意长宽图像打成 1D patch 序列</td>
</tr>
<tr>
<td>2D-RoPE</td>
<td>将旋转位置编码同时施加在 h、w 两个维度</td>
<td>保持 patch 间的二维空间相对位置</td>
</tr>
<tr>
<td>Window Attention</td>
<td>局部 7×7 window + 移位窗口</td>
<td>降低 O(n²) 计算，显存随分辨率线性增长</td>
</tr>
<tr>
<td>Global Query Token</td>
<td>每 4 层插入 1 个可学习 global query</td>
<td>远程 patch 也能直接交互，防止信息孤岛</td>
</tr>
<tr>
<td>Neck（MLP）</td>
<td>2 层 MLP 压缩通道 → LLM 词表维度</td>
<td>把视觉特征映射到文本 token 空间</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p>二、训练流程（3 阶段）</p>
<ol>
<li>Vision-Language Alignment（冻结 LLM）<br>
600 M 图文对 + 400 M OCR 对，只训 Neck 层 + Vision Encoder，让 patch 特征对齐词向量空间。</li>
<li>Vision-Language Instruction Tuning（LLM 解冻）<br>
200 M 高质量指令数据（图像描述、图表、定位、视频），全参数训练，学习「看-说」指令。</li>
<li>Mixed Instruction Tuning（文本不掉）<br>
再把纯文本 SFT 子集（约 20 M 样本）以 1:1 比例混训，确保文本基准不降级。</li>
</ol></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://naphjohn.github.io">kaikai的博客</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z', 'copy': 'M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z', 'check': 'M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","NaphJohn/naphjohn.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}

document.addEventListener('DOMContentLoaded', () => {
    const createClipboardHTML = (codeContent, additionalClasses = '') => `
        <pre class="notranslate"><code class="notranslate">${codeContent}</code></pre>
        <div class="clipboard-container position-absolute right-0 top-0 ${additionalClasses}">
            <clipboard-copy class="ClipboardButton btn m-2 p-0" role="button" style="display: inherit;">
                <svg height="16" width="16" class="octicon octicon-copy m-2"><path d="${IconList["copy"]}"></path></svg>
                <svg height="16" width="16" class="octicon octicon-check color-fg-success m-2 d-none"><path d="${IconList["check"]}"></path></svg>
            </clipboard-copy>
            <div class="copy-feedback">Copied!</div>
        </div>
    `;

    const handleCodeElements = (selector = '') => {
        document.querySelectorAll(selector).forEach(codeElement => {
            const codeContent = codeElement.innerHTML;
            const newStructure = document.createElement('div');
            newStructure.className = 'snippet-clipboard-content position-relative overflow-auto';
            newStructure.innerHTML = createClipboardHTML(codeContent);

            const parentElement = codeElement.parentElement;
            if (selector.includes('highlight')) {
                parentElement.insertBefore(newStructure, codeElement.nextSibling);
                parentElement.removeChild(codeElement);
            } else {
                parentElement.parentElement.replaceChild(newStructure, parentElement);
            }
        });
    };

    handleCodeElements('pre.notranslate > code.notranslate');
    handleCodeElements('div.highlight > pre.notranslate');

    let currentFeedback = null;
    document.querySelectorAll('clipboard-copy').forEach(copyButton => {
        copyButton.addEventListener('click', () => {
            const codeContent = copyButton.closest('.snippet-clipboard-content').innerText;
            const tempTextArea = document.createElement('textarea');
            tempTextArea.value = codeContent;
            document.body.appendChild(tempTextArea);
            tempTextArea.select();
            document.execCommand('copy');
            document.body.removeChild(tempTextArea);

            const copyIcon = copyButton.querySelector('.octicon-copy');
            const checkIcon = copyButton.querySelector('.octicon-check');
            const copyFeedback = copyButton.nextElementSibling;

            if (currentFeedback && currentFeedback !== copyFeedback) {currentFeedback.style.display = 'none';}
            currentFeedback = copyFeedback;

            copyIcon.classList.add('d-none');
            checkIcon.classList.remove('d-none');
            copyFeedback.style.display = 'block';
            copyButton.style.borderColor = 'var(--color-success-fg)';

            setTimeout(() => {
                copyIcon.classList.remove('d-none');
                checkIcon.classList.add('d-none');
                copyFeedback.style.display = 'none';
                copyButton.style.borderColor = '';
            }, 2000);
        });
    });
});

</script>
<script>MathJax = {tex: {inlineMath: [["$", "$"]]}};</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</html>
